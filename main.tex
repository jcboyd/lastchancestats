\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{caption}
% \usepackage{subcaption}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{amsmath, amsthm, amssymb}
\usepackage[ansinew]{inputenc}
\usepackage{bbm}
\usepackage{cancel}
\usepackage{mathdots}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
% \setlength\parindent{0pt}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\let\biconditional\leftrightarrow
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\p}{\text{p}}

\DeclareMathOperator*{\argmax}{\arg\!\max}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\title{Last Chance Stats}
\author{Joseph Boyd}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

\tableofcontents

%Statistics are so ubiquitous across the hard and soft sciences that initial undergraduate courses on the topic may attempt to make themselves one-size-fits-all, regardless of the diverse backgrounds of the students in the class. The result therefore may be a course devoid of the mathematical rigour and comprehension expected by a mathematician. This can lead to an irrational hatred of statistics. Last Chance Stats returns to first principles and develops the theory behind statistics step by step. Each section aims to present a new idea, passing through the most essential and interesting mathematics, while sidestepping burdensome secondary details, and building towards a culminating result. The sections are accessible as individual modules, but also aim to contribute to a cohesive picture of statistics in general. Last Chance Stats begins with fundamental results from number theory, and goes on to show the connections to statistics, as well as to answer the nagging questions that often go unanswered in statistics courses--questions such as, `where do probability functions come form?', `what is the error function? what is the gamma function? and how can they be evaluated?', `what theory goes into a hypothesis test?', `what are degrees of freedom?', `what is a p-score?', `why is sample variance weighted by $N-1$ and not $N$ samples?'.

\section{Warm Up}

This section presents an assortment of mathematical problems, each embodying an idea that is useful or instructive to the material that follows.

\subsection{Proof of Pythagorean Theorem}

Consider a square with sides of length $c$ rotated inside a larger square, such that the four corners of the smaller square meet a distinct edge of the larger at a point $a$ units along that edge. The sides of the larger square are therefore of length $a + b$ for some $a, b > 0$. The area of the four resulting right-angled triangles that fill the empty space are $\frac{1}{2}ab$ apiece. Thus, we have it that, $$(a + b)^2 = c^2 + 4 \cdot \frac{1}{2}ab,$$ expanding to, $$a^2 + 2ab + b^2 = c^2 + 2ab,$$ and finally, $$a^2 + b^2 = c^2.$$

\subsection{Irrationality of $\sqrt{2}$}

Suppose $\sqrt{2}$ is rational. Then there exist $a, b \in \mathbb{Z}$, with $a > b$ and $gcd(a, b) = 1$\footnote{That is, $\frac{a}{b}$ is an irreducible fraction} such that, $$\frac{a}{b} = \sqrt{2}.$$

Consequently, $$\frac{a^2}{b^2} = 2.$$ This implies that $a^2$ is even, and therefore that $a$ is even. That is, $$a = 2k,$$ for some $k \in \mathbb{Z}$. So, by substitution, $$\frac{4k^2}{b^2} = 2.$$ Rearranging, $$b^2 = 2k^2.$$ However, this now implies that $b^2$ is even, hence $b$ also, implying a common factor between $a$ and $b$, though by assumption they have no common factors, and clearly we can repeat the argument ad infinitum. This contradiction proves $\sqrt{2}$ is irrational\footnote{This is an example of a proof by contradiction.}.

\subsection{Euclid's Proof of the Infinitude of Primes}

Take any finite set of primes, $p_1, p_2, \dots, p_n$. Then, the number, $q = p_1 \times p_2 \times \cdots \times p_n + 1$, is either prime or not prime. If it is prime, we have generated a new prime not in our set. If it is not, it has a prime factor not in our set, as $q \equiv 1 \ (\text{mod}\ p_i)$ for $i = 1, 2, \dots, n$. It follows that there are infinitely many primes\footnote{This is an example of a constructive proof.}.

\subsection{The Potato Paradox}

A potato that is $99\%$ water weighs 100g. It is dried until it is only $98\%$ water. It now weighs only 50g. How can this be?

The mistake people typically make is to assume there has been a $1\%$ reduction in the volume of water, rather than a change to the water/non-water ratio. In fact, the ratio of water to non-water content has changed from $1:99$ to $2:98$ or $1:49$. Since the non-water content of the potato has not changed, the water must have lost $50\%$ of its volume, and so the potato now weighs half as much!

\subsection{The Monty Hall Problem}

This famous paradox, named after an American game show host, has the following problem statement:

\emph{A game show presents you with the choice of selecting one of three doors. Two of the doors contain a goat (undesirable), whilst the third contains a car. Upon your choice, the game show host, who knows the contents of each of the doors, opens one of the two remaining doors to reveal a goat. He then gives you the option of either sticking with your initial choice, or switching to the one remaining door. The question is, is it advantageous to switch doors?}

The answer is that it \emph{is} advantageous, increasing your winning chances from $1/3$ to $2/3$. The essential realisation is that the host (Monty Hall) has knowledge of the door contents and will always reveal a goat, whether you have chosen the car or not.

The solution can be understood as a decision theory problem. We designate two strategies: changing and not changing doors. In either case, there is a $2/3$ chance of initially having a goat. Because the other goat is eliminated independently by the host, changing wins whenever a goat was initially chosen, that is, with probability $2/3$. Not changing wins only if the car was initially chosen, that is, with probability $1/3$. Therefore changing will double your winning chances!

Another (admittedly more complicated) way to solve the problem is using Bayes' theorem. Here we designate $C_1, C_2$, and $C_3$ as the events the car is behind doors 1, 2, and 3 respectively; $X_1$ the event the player chooses door 1; and $H_3$ the event the host opens door 3. Note the numbering of the doors does not matter, for example `door 2' simply refers to `the other door'. First, $\text{Pr}(H_3|C_1,X_1) = 1/2$, $\text{Pr}(H_3|C_2,X_1) = 1$, and $\text{Pr}(H_3|C_3,X_1) = 0$, reflecting the knowledge that the host always opens a goat door (never the car). Then,

\begin{align}
\text{Pr}(C_2|X_1,H_3) &= \frac{\text{Pr}(H_3|C_2,X_1)\text{Pr}(C_2|X_1)}{\text{Pr}(H_3|X_1)} \notag \\
&= \frac{\text{Pr}(H_3|C_2,X_1)\text{Pr}(C_2|X_1)}{\text{Pr}(H_3|C_1,X_1)\text{Pr}(C_1|X_1) + \text{Pr}(H_3|C_2,X_1)\text{Pr}(C_2|X_1) + \text{Pr}(H_3|C_3,X_1)\text{Pr}(C_3|X_1)} \notag \\
&= \frac{1 \cdot 1/3}{1/2 \cdot 1/3 + 1 \cdot 1/3 + 0 \cdot 1/3} \notag \\
&= \frac{2}{3}, \notag
\end{align}

so the winning odds for changing doors is $2/3$, hence not changing only $1/3$.

\subsection{Finding Hamlet in the Digits of $\pi$}

It is supposed, though not known for certain, that $\pi$ is a normal number. This means that in all the infinite decimal digits of $\pi$, every sequence of $n$ digits appears as often as every other length $n$ sequence. So, 123 occurs as often as 321 or 666, but more often than 123456789, because it is a shorter sequence. In this way, the behaviour of the digits of $\pi$ is indistinguishable to what would arise from a uniform random distribution. Of course, the digits of $\pi$ are not random, as many formulas exist for generating them. The same rules of normality apply if $\pi$ is converted to a different base, for example, binary code--the arithmetic system of computers. The number $\pi$ in binary is,

\begin{align}
\pi &= 3.14... \notag \\
&= 1\cdot2^1 + 1\cdot2^0 + 0\cdot2^{-1} + 0\cdot2^{-2} + 1\cdot2^{-3} + 0\cdot2^{-4} + 0\cdot2^{-5} + 1\cdot2^{-6} + \cdots \notag \\
&= 11.001001... \notag
\end{align}

Note that to express $\pi$ in binary rather than decimal notation requires more digits, because it takes $\log_2(10) \approx 3.3$ binary digits (bits) to express each power of ten. Nevertheless, the same properties about the normality of $\pi$ hold in its binary form--$000$ is just as likely as $111$, and so on.

Now, to find Hamlet, we would need to start with a common base. Hamlet is written in Latin characters. One way to convert Latin characters to numbers is to use a character encoding standard, such as ASCII (American Standard Code for Information Interchange). People use ASCII indirectly every day--it is how our data comes to be represented by computers and communication systems. ASCII gives a representation of each character in 8 binary digits. For example, in ASCII,

$$\text{``pie''} = 01110000 \ 01101001 \ 01100101.$$

So, Hamlet, like any text, has an ASCII, and therefore binary representation. Without specifying exactly what it is we can determine its length. There are 186,391 characters in the 31,842 word Hamlet, including spaces and indentation. At 8 bits per character, this is 1,491,128 bits.

To develop a formula, we first consider a simpler problem: what is the expected number of digits we should pass over in a random binary sequence before we encounter a specific binary sub-sequence, say, 11? If we consider a binary tree covering all the possibilities, and let the expected length to find 11 be denoted by $\mathbb{E}_{11}$, then we may first see 0 with probability $\frac{1}{2}$, in which case we are back to where we started. That is, the first digit is a failure, because it is not the one we are looking for. The expected length from there is therefore $1 + \mathbb{E}_{11}$. We might alternatively see 1 in the first digit--a match--also with probability $\frac{1}{2}$. In this case, the next digit is 0 with probability $\frac{1}{2}$, which would mean a failure on the second digit, and again, we return to the starting point, with expected length now $2 + \mathbb{E}_{11}$. Otherwise, we could get a 1 in the second digit, and we have found the match straight away. In this case, the expected length is simply 2. Therefore we may write,

$$\mathbb{E}_{11} = \frac{1}{2}\times(1 + \mathbb{E}_{11}) + \frac{1}{4}\times (2 + \mathbb{E}_{11}) + \frac{1}{4}\times 2,$$

which we may solve to find $\mathbb{E}_{11} = 6$. That is, we expect to pass over $6$ binary digits (on average) before we see the particular sequence, 11. We see that in general, it is possible to fail on each of the $n$ digits in the sequence. Thus, we have the following expression for the general $n$-bit sequence,

$$\frac{1}{2^n}\mathbb{E}_{\{0, 1\}^n} = 1\times\frac{1}{2^1} + 2\times\frac{1}{2^2} + 3\times\frac{1}{2^3} + \cdots + 2n\times\frac{1}{2^{n}}.$$

If we denote this sum $S$ then clearly,

\begin{align}
S - \frac{1}{2}S &= 1\times\frac{1}{2^1} + 1\times\frac{1}{2^2} + 1\times\frac{1}{2^3} + \cdots + 1\times\frac{1}{2^{n}} = 1 - \frac{1}{2^{n}} \notag \\
&\implies S = 2 - \frac{1}{2^{n-1}}, \notag
\end{align}

hence, $\mathbb{E}_{\{0, 1\}^n} = 2^{n+1} - 2$. Applying this formula, we can expect to find Hamlet somewhere within the first $2^{1,491,129} - 2 \approx 3.6\times10^{448,874}$ digits of $\pi$.

\section{Differential Calculus}

This section presents the fundamental results of differential calculus: first the formal definition of a limit, then that of a derivative. In particular, we note the relationship between the binomial theorem and the differentiation of polynomials. The section culminates with a proof of the mean value theorem.

\subsection{Binomial Theorem}

The binomial theorem expresses a formula for expanding powers of a binomial\footnote{A binomial is the sum of two \emph{monomials}, that is, a polynomial with a single term.}. For example, $(x + y)^3 = x^3 + 3x^2y + 3xy^2 + y^3$. In general, the binomial expansion is,

$$(x + y)^n = \sum_{k = 0}^{n}{{n}\choose{k}}x^ky^{n-k},$$

where, $${{n}\choose{k}} = \frac{n!}{k!(n-k)!},$$ is known as the binomial coefficient. Note the binomial coefficient is symmetric, that is, ${{n}\choose{k}} = {{n}\choose{n-k}}$. This symmetry may be seen in Pascal's triangle:

\begin{center}
\begin{tabular}{ccccccccccccc}
&&&&&&1&&&&&& \\
&&&&&1&&1&&&&& \\
&&&&1&&2&&1&&&& \\
&&&1&&3&&3&&1&&& \\
&&1&&4&&6&&4&&1&& \\
&1&&5&&10&&10&&5&&1& \\
1&&6&&15&&20&&15&&6&&1 \\
\end{tabular}
\end{center}

Pascal's triangle, named after French mathematician Blaise Pascal (1623-1662) embodies a recurrence relation for the coefficients of a binomial expansion. The numbers of each row are generated by summing the two numbers diagonally above in the previous row. In terms of binomial expansion, this is equivalent to collecting like terms after expanding. To illustrate, consider the coefficients for the quadratic expansion, $x^2 + 2xy + y^2$,

\begin{center}
\begin{tabular}{ccccc}
1&&2&&1\\
\end{tabular}
\end{center}

Multiplying again by the binomial, $(x + y)$, gives the coefficients,

\begin{center}
\begin{tabular}{cccccccccccccccc}
1&&1&&2&&2&&1&&1 \\
\end{tabular}
\end{center}

which corresponds to $x^3 + x^2y + 2x^2y + 2xy^2 + xy^2 + y^3$, and clearly each of the interior pairs are like terms, so these are summed to arrive at,

\begin{center}
\begin{tabular}{cccccccccc}
1&&3&&3&&1 \\
\end{tabular}
\end{center}

In general, this relation is reflected in Pascal's rule,

$${{n}\choose{k}} = {{n-1}\choose{k-1}} + {{n-1}\choose{k}},$$
		
for $1 \leq k \leq n$. Isaac Newton (1642-1727) extended the notion of binomial expansion to real powers in 1665. This was an important stepping-stone towards the definition of a derivative, as we will see.

\subsection{($\epsilon$-$\delta$) Definition of a Limit}

The standard formal ($\epsilon$-$\delta$) definition of a limit is due to German mathematician, Karl Weierstrass (1815-1897). We say that, $$\lim_{x \to a} f(x) = L,$$

if for any value, $\epsilon > 0$, specifically where $\epsilon$ is arbitrarily small, we can find a value $\delta > 0$ such that,

$$0 < |x - a| < \delta \implies |f(x) - L| < \epsilon.$$

This somewhat off-putting notation captures the notion of a function approaching a limiting value continuously. It states that however close (within an infinitesimal $\epsilon$ units) we wish to get to the limiting value, $L$, we can always find a correspondingly tiny interval of radius $\delta$ around a point $a$ such that the function is closer. Note that this accounts also for the case where $f(x)$ is undefined at $x = a$.

An example is always useful for such subtle ideas. Therefore, let $f(x) = 2x^2 + 2$. Then clearly, $\lim_{x \to 0} f(x) = 2$. To satisfy the inequality, $|2x^2 + 2 - 4| < \epsilon$, we can select $\delta = \sqrt{\epsilon/2 + 1}$. Clearly this is always possible, no matter how small we take $\epsilon$ to be, and so the limit exists. A function is said to be \emph{continuous} at a point $a$ if the limit $L$ exists, and $f(a) = L$, which is not always the case. Suppose $g(x) = 1$ for $x <0$ and $g(x) = 2$ for $x \geq 0$. Such a function makes an instantaneous jump at $x = 0$ to a value distinct from the limit to which it was approaching. A function is continuous if it is continuous at all points in its domain. A function that is \emph{differentiable} at a point, $a$, implies continuity at $a$, but not vice-versa.

\subsection{Differentiating From First Principles}

The gradient of a linear function, $y = f(x)$, over an interval, $[x, \Delta x]$, is defined as $\Delta y / \Delta x$, that is, `rise over run'. The derivative of a function is simply the gradient taken at a point. Formally, we define a derivative as,

$$\frac{\mathop{d}}{\mathop{dx}} f(x) = \lim_{\Delta x \to 0} \frac{f(x  + \Delta x) - f(x)}{\Delta x},$$

and it is clear this is just `rise over run', albeit taken on an infinitesimal interval. It represents the gradient of a line tangent to the curve at $x$. To see how we can differentiate a polynomial, consider the general $n$th degree polynomial\footnote{Note that the powers of a polynomial are non-negative integers by definition.}, $f(x) = a_nx^n + a_{n-1}x^{n-1} + \cdots + a_1x + a_0$. Then, from the definition,

\begin{align}
\frac{\mathop{d}}{\mathop{dx}} f(x) &= \lim_{\Delta x \to 0} a_n\frac{(x + \Delta x)^n - x^n}{\Delta x} + \cdots + a_1\frac{(x + \Delta x) - x}{\Delta x} + a_0\frac{1 - 1}{\Delta x}\notag \\
&= \lim_{\Delta x \to 0} a_n\frac{{{n}\choose{0}}x^n + {{n}\choose{1}}x^{n-1}\Delta x + \cdots + {{n}\choose{n}}(\Delta x)^n - x^n}{\Delta x} + \cdots + a_1\frac{\Delta x}{\Delta x} + a_0\frac{0}{\Delta x} \notag \\
&= na_nx^{n-1} + (n-1)a_{n-1}x^{n-2} + \cdots + a_1, \notag
\end{align}

and it is clear that only the second term of each expansion survives cancelation, or annihilation by the infinitesimal $\Delta x$. From this we can infer that $\frac{\mathop{d}}{\mathop{dx}} a_kx^k = ka_kx^{k-1}$, the rule that every student of calculus is familiar with, though usually does not realise it to be a consequence of the binomial theorem! It is only when one tries to `plug and play' from first principles that these connections become apparent. As noted before, Newton extended the definition of the binomial theorem to real-valued powers.

\subsection{Chain Rule}

The chain rule for differentiation describes how to differentiate function compositions, $f \cdot u \triangleq f(u(x))$. Assuming $u(x)$ is differentiable at $a$, and $f(u)$ is differentiable at $u(a)$, the derivative,

\begin{align}
(f \cdot u)'(a) &= \lim_{x \to a}\frac{f(u(x)) - f(u(a))}{x - a} \notag \\
&= \lim_{x \to a}\frac{f(u(x)) - f(u(a))}{u(x) - u(a)} \cdot \frac{u(x) - u(a)}{x - a} \notag \\
&= f'(u(a))u'(a). \notag
\end{align}

An extension to this expression is needed for a rigorous proof, as a function may oscillate as it tends towards a limit, and $u(x)$ may equal $u(a)$ infinitely many times, resulting in a inexhaustible number of undefined points as $x \to a$, but we will leave it as a sketch.

In general, the chain rule may be applied repeatedly for any number of nested functions,

$$\frac{\mathop{d}}{\mathop{dx}} f_n(f_{n-1}(\cdots (f_1(x)))) = \frac{\mathop{df_n}}{\mathop{df_{n-1}}}\cdot \frac{\mathop{df_{n-1}}}{\mathop{df_{n-2}}} \cdot \cdots \cdot \frac{\mathop{df_2}}{\mathop{df_1}}\cdot \frac{\mathop{df_1}}{\mathop{dx}}.$$

The corresponding technique for anti-differentiation is integration by substitution.

\subsection{Product Rule}

The product rule for differentiation allows us to differentiate products of functions of $x$. Suppose $f(x) = u(x)v(x)$. Then, from first principles,

\begin{align}
f'(x) &= \lim_{\Delta x \to 0} \frac{f(x + \Delta x) - f(x)}{\Delta x} \notag \\
&= \lim_{\Delta x \to 0} \frac{u(x + \Delta x)v(x + \Delta x) - u(x)v(x)}{\Delta x} \notag \\
&= \lim_{\Delta x \to 0} \frac{u(x + \Delta x)v(x + \Delta x) - u(x)v(x + \Delta x) + u(x)v(x + \Delta x) - u(x)v(x)}{\Delta x} \notag \\
&= \lim_{\Delta x \to 0} \frac{u(x + \Delta x) - u(x)}{\Delta x}\cdot v(x + \Delta x) + \frac{v(x + \Delta x) - v(x)}{\Delta x}\cdot u(x)\notag \\
&= u'(x)v(x) + v'(x)u(x). \notag
\end{align}

The quotient rule can be derived by applying both the product rule and the chain rule. For $f(x) = u(x) / v(x)$, write, $f(x) = u(x)v(x)^{-1}$. Then, 

\begin{align}
\frac{\mathop{d}}{\mathop{dx}}f(x) &= u'(x)v(x)^{-1} - u(x)v'(x)v(x)^{-2} \notag \\
&= \frac{u'(x)v(x) - u(x)v'(x)}{v(x)^2}. \notag
\end{align}

The corresponding technique for anti-differentiation is integration by parts.

\subsection{Rolle's Theorem}

Rolle's theorem by Michel Rolle (1652-1719) asserts that for a differentiable function, $f(x)$, if $f(a) = f(b)$ for some $a$ and $b$, then there must be a stationary point on the interval $[a, b]$, unless $f = c$ for some constant, $c$. Plainly speaking, it says that if a curve returns to its starting point at the end of an interval, there must be at least one turning point somewhere in that interval. To prove this, first consider that there must be at least one extreme point $c$ on the interval $[a, b]$, be it maximum or minimum. For brevity, we assume that $c$ is a maximum. So, for all points after the interior point, $c$, $c + h$, such that $h > 0$, the function is upper-bound by $f(c)$. That is,

$$\frac{f(c + h) - f(c)}{h} \leq 0.$$

Taking the limit gives,

$$f'(c) = \lim_{h \to 0^+} \frac{f(c + h) - f(c)}{h} \leq 0.$$

Likewise, for $h < 0$ we have,

$$f'(c) = \lim_{h \to 0^-} \frac{f(c + h) - f(c)}{h} \geq 0,$$

from which it follows that the derivative of the function, $f'(x)$, is 0 at $c$.

\subsection{Mean Value Theorem}

The mean value theorem states that for a differentiable function defined on an interval $[a, b]$, at least one point on the curve must have gradient equal to that of the chord connecting the endpoints, $(a, f(a))$ and $(b, f(b))$. It is an intuitive result, as any curve which succeeds in climbing (or descending) from $f(a)$ to $f(b)$, must match the rise over run of that slope at least at one point, otherwise it will never make the ascent.

First define $g(x) = f(x) - rx$. This must be differentiable on $[a, b]$ since $f(x)$ is by construction. We choose $r$ such that $g(x)$ satisfies Rolle's theorem, that is, $g(a) = g(b)$, hence $f(a) - ra = f(b) - rb$, and $r = (f(a) - f(b))/(a - b)$, that is, the slope of the chord connecting the endpoints. Because of Rolle's theorem, for some $c \in [a, b], g'(c) = 0,$ hence $f'(c) = r$, and the proof is complete.

\section{Linear Algebra}

A system of linear equations is a set of $M$ equations in $N$ variables. Each variable may feature in one or more of the $M$ equations. The typical interest in these systems is to find a \emph{simultaneous solution}, that is, values for each of the variables satisfying all the equations at once. If $M < N$ (more variables than equations), the system is \emph{underdetermined}. In this case, there will not be enough information to identify a unique solution, and the solution will be expressed in terms of at least one of the variables, giving an infinite family of solutions. If any equations are mutually contradictory\footnote{For example, the equations $x + y + z = 1$ and $x + y + z = 2$ have no simultaneous solution.}, however, the system will have no solution. If $M > N$, the system is \emph{overdetermined}. Unless the excess equations are in fact linear combinations of others, the solution will necessarily have mutually contradictory equations. A system of linear equations may alternatively be written as a matrix equation,

$$\mathbf{A}\mathbf{x} = \mathbf{b},$$

where $\mathbf{A}$ is the matrix of coefficients, $\mathbf{x}$ is vector form of the simultaneous solution, and $\mathbf{b}$ is the vector of constants. The solution, $\mathbf{x}$, may therefore be found by inverting $\mathbf{A}$. A square matrix $\mathbf{A} \in \mathbb{R}^{N\times N}$ is invertible or non-singular if there is a matrix $\mathbf{A}^{-1} \in \mathbb{R}^{N\times N}$ such that $\mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$. Non-square matrices can have at most distinct left or right inverses. There are many matrix properties necessary and sufficient for invertibility. In particular, the columns of $\mathbf{A}$ are linearly independent and span\footnote{The span of a set of vectors is intuitively its expressiveness as a basis. $N$ linearly independent vectors form a basis for the vector space $\mathbb{R}^{N}$. On the other hand, if there are linear dependencies in the system, the span will be of lower dimension.} the vector space $\mathbb{R}^{N}$. This is equivalent to having full rank, which is in turn equivalent to having zero nullity\footnote{The rank of a matrix, $\mathbf{A}$ is the dimensionality of the vector space spanned with its columns as a basis. This is always equivalent to the span of its rows (row rank of $\mathbf{A}$). The nullity of a matrix is the dimensionality of its null space, that is, the set of all vectors solving $\mathbf{A}\mathbf{x} = \mathbf{0}$.}. These properties are connected in the rank-nullity theorem\footnote{The rank-nullity theorem states that for $N \times N$ matrix, $\mathbf{A}$, its rank and nullity sum to its number of columns, formally, $\text{rank}(\mathbf{A}) + \text{null}(\mathbf{A}) = N$}, and further in the fundamental theorem of linear algebra. If the values of a matrix are randomly sampled from a uniform distribution, the matrix has an infinitesimal chance of having linearly dependent columns, and hence of being singular. For this reason, singular matrices are less a practical concern in least squares regression than is \emph{multi-collinearity}, that is, highly correlated columns that cause ill-conditioning and numerical errors.

\subsection{Analytic Form of Inverse}

Another necessary and sufficient property of a non-singular matrix is a non-zero determinant. The determinant has an analytic form recursive in the \emph{minors} of the matrix, that is, each of the $N$ sub-matrices formed by removing a given row and each of the $N$ columns in turn. Thus, for $\mathbf{A} \in \mathbb{R}^{2 \times 2} \text{det}(\mathbf{A}) =
\begin{vmatrix}
a&b\\
c&d
\end{vmatrix} =
ad - bc
$. For $\mathbf{A} \in \mathbb{R}^{3 \times 3}$,

$$
\text{det}(\mathbf{A}) =
\begin{vmatrix}
a&b&c\\
d&e&f\\
g&h&i
\end{vmatrix} =
a\begin{vmatrix}
e&f\\
h&i
\end{vmatrix} -
b\begin{vmatrix}
d&f\\
g&i
\end{vmatrix} +
c\begin{vmatrix}
d&e\\
g&h
\end{vmatrix},
$$

and for $\mathbf{A} \in \mathbb{R}^{4 \times 4}$,

$$
\text{det}(\mathbf{A}) =
\begin{vmatrix}
a&b&c&d\\
e&f&g&h\\
i&j&k&l\\
m&n&o&p
\end{vmatrix} =
a\begin{vmatrix}
f&g&h\\
j&k&l\\
n&o&p
\end{vmatrix} -
b\begin{vmatrix}
e&g&h\\
i&k&l\\
m&o&p
\end{vmatrix} +
c\begin{vmatrix}
e&g&h\\
i&k&l\\
m&o&p
\end{vmatrix} -
d\begin{vmatrix}
e&g&h\\
i&k&l\\
m&o&p
\end{vmatrix}.
$$

A naive approach to computing a determinant would involve $\mathcal{O}(N!)$ operations. However, efficient algorithms exist, and if required, a determinant is computed as a byproduct of inversion. The determinant is not usually needed in practice, however. The analytic form of the inverse of matrix $\mathbf{A} \in \mathbb{R}^{N \times N}$ is,

$$\mathbf{A}^{-1} = \frac{1}{\text{det}(\mathbf{A})}\mathbf{C}^{T},$$

where $\mathbf{C} \in \mathbb{R}^{N \times N}$ is the \emph{cofactor} matrix of $\mathbf{A}$, whose elements are each of the $N^2$ minors of $\mathbf{A}$, that is, the determinants of each of the $(N - 1) \times (N - 1)$ sub-matrices of $\mathbf{A}$, and with alternating signs. The transpose of the cofactor matrix is denoted the \emph{adjugate} matrix. This form rarely has use outside of theory, as its computation is intractable like the analytic determinant.

\subsection{Gaussian Elimination}

Gaussian elimination is an ancient technique for solving systems of linear equations, but has in modern times been erroneously attributed to Gauss, who merely contributed to its notation. The technique consists of performing a series of \emph{elementary row operations} to transform the system into a form where its solution or solutions may be extracted directly. The row operations are: swap two rows; scale a row by a constant factor, and; add one row to another. With a finite sequence of these operations, a system may be reduced to \emph{row echelon form}, where the matrix has an upper-triangular form, allowing solutions to be found by back-substituting each variable. When the system is further reduced to \emph{reduced echelon form}, the technique is known as Gauss-Jordan elimination. It may be seen easily that these row operations correspond with multiplying the matrix by slight modifications of the identity matrix. The technique has a number of useful applications, in particular computing a matrix inverse. This can be found by applying the technique to the system,

$$\mathbf{A}\mathbf{X} = \mathbf{I},$$

and reducing $\mathbf{A}$ to $\mathbf{I}$ transforms $\mathbf{I}$ to $\mathbf{A}$ in a product of elementary row operations matrices. Gaussian elimination can also be used to inform about a matrix rank, as well as to efficiently compute a matrix determinant.

\subsection{Matrix Decompositions}

More efficient algorithms for solving systems of linear equations and inverting matrices come in the form of matrix decompositions. The most widely used, LU decomposition, consists of factorising the matrix into lower and upper triangular matrices. Inverting these triangular matrices may be done very efficiently. QR decomposition is another decomposition where $\mathbf{Q}$ is orthogonal and $\mathbf{R}$ is upper-triangular. In the special case of a Hermitian, positive-definite matrix, we can perform a Cholesky\footnote{Andr\'e-Louis Cholesky was a French mathematician (1875-1918).} decomposition faster than the previous methods. Note that a Hermitian matrix, named after French mathematician, Charles Hermite (1822-1901), is a matrix of complex numbers equal to its transpose in its real parts, and complex conjugate in imaginary parts. Thus, if our domain is the real numbers, a Hermitian matrix is the same as a symmetric matrix. In this case, we can write our matrix as,

$$\mathbf{A} = \mathbf{L}\mathbf{L}^T,$$

where $\mathbf{L}$ is a lower triangular matrix. Solutions can then be derived in the usual way using back-propagation. Symmetric positive-definite matrices arise in machine learning models, thus Cholesky decomposition and the related, iterative conjugate gradient method are of interest.

\section{Vector Calculus}

Vector calculus is an important tool for understanding multivariate probability distributions.

\subsection{Preliminary Results}

\subsubsection{Sums and Matrices}

It is useful to note how certain matrix expressions may be written as sums. There are many occasions in machine learning where linear models are written interchangeably using summation notation and with vectors. Consider the simplest case--the dot product,

$$\mathbf{a}\cdot\mathbf{b} = \mathbf{a}^T\mathbf{b} = a_1b_1 + a_2b_2 + \cdots + a_nb_n = \sum_i a_ib_i.$$

The geometric interpretation of a dot product is a projection of one vector onto the line defined by another. In machine learning, linear classifiers are log-linear with respect to their parameters. Geometrically, a model prediction amounts to projecting a data point onto a line (or hyperplane) defined by the parameters. This line runs perpendicular to a decision boundary, which determines the classification of the point. Now consider a matrix vector multiplication,

$$\mathbf{A}\mathbf{b} = \big[A_{:, 1}, A_{:, 2}, \dots, A_{:, N}\big]
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n \\
\end{bmatrix}
= b_1A_{:, 1} + b_2A_{:, 2} + \dots + b_NA_{:, N} = \sum_{i} b_iA_{:, i},$$

which represents a projection onto the hyperplane defined by the span of $\mathbf{A}$. Note that if instead we sum over rows rather than columns of $\mathbf{A}$, we have,

$$\sum_{i} b_i\mathbf{a}_i = \mathbf{A}^T\mathbf{b}.$$

Now consider a matrix product,

$$\mathbf{A}\mathbf{B} = \big[A_{:, 1}, A_{:, 2}, \dots, A_{:, N}\big]
\begin{bmatrix}
\mathbf{b}_1^T \\
\mathbf{b}_2^T \\
\vdots \\
\mathbf{b}_n^T \\
\end{bmatrix}
= \sum_{i} A_{:, i}\mathbf{b}_i^T.
$$

Again working backwards, should we have rows rather than columns, we have the rule,

$$\sum_{i} \mathbf{a}_i\mathbf{b}_i^T = \mathbf{A}^T\mathbf{B}.$$

\subsubsection{Transposition Chain Rule}

For matrices \textbf{A} and \textbf{B}, the elements of their product, \textbf{AB}, are formed by multiplying each row of \textbf{A} with each column of \textbf{B}. Thus, $(\textbf{AB})_{ij} = A(i, :) \cdot B(:, j)$, that is, the dot product of row $i$ of matrix \textbf{A} with column $j$ of matrix \textbf{B}. Clearly, $A^T(i, :) \cdot B(:, j)= B^T(j, :) \cdot A(:,i)$, from which the rule,

$$(\mathbf{AB})^T = \mathbf{B}^T\mathbf{A}^T.$$

It is further clear that due to associativity, $(\mathbf{ABC})^T = (\mathbf{A(BC)})^T = (\mathbf{BC})^T\mathbf{A}^T$ = $\mathbf{C}^T\mathbf{B}^T\mathbf{A}^T$. From this, the chain rule,

$$(\mathbf{A}_1\mathbf{A}_2\cdots \mathbf{A}_n)^T = \mathbf{A}_n^T\mathbf{A}_{n-1}^T\cdots\mathbf{A}_1^T.$$

\subsubsection{Inversion Chain Rule}

Given invertible square matrices \textbf{A} and \textbf{B}, it is clear that if we form the expression, $\mathbf{AB}\mathbf{B}^{-1}\mathbf{A}^{-1} = \mathbf{A}\mathbf{I}\mathbf{A}^{-1} = \mathbf{I}$, then $\mathbf{B}^{-1}\mathbf{A}^{-1}$ is the right inverse of $\mathbf{AB}$. It is further clear that this is also the left inverse. We have therefore,

$$(\mathbf{AB})^{-1} = \mathbf{B}^{-1}\mathbf{A}^{-1}.$$

Just as with transposition, we see that due to associativity $(\mathbf{ABC})^{-1} = (\mathbf{A(BC)})^{-1} = (\mathbf{BC})^{-1}\mathbf{A}^{-1}$ = $\mathbf{C}^{-1}\mathbf{B}^{-1}\mathbf{A}^{-1}$, and in general,

$$(\mathbf{A}_1\mathbf{A}_2\cdots \mathbf{A}_n)^{-1} = \mathbf{A}_n^{-1}\mathbf{A}_{n-1}^{-1}\cdots\mathbf{A}_1^{-1}.$$

\subsubsection{Trace Cyclic Permutation Property}

The trace of a matrix product is $\text{tr}(\mathbf{A}\mathbf{B}) = \sum_{j=1}^N\sum_{i = 1}^N a_{ij}b_{ji}$, that is, the sum of the diagonal elements of the product. First consider the product,

\begin{align}
\text{tr}(\mathbf{A}\mathbf{B}\mathbf{C}) &= \text{tr}\Bigg(\begin{bmatrix}
\sum_{j=1}^N c_{j1}\sum_{i=1}^Na_{1i}b_{ij} & \cdots & & \cdots \\
\cdots & \sum_{j=1}^N c_{j2}\sum_{i=1}^Na_{2i}b_{ij} & & \cdots \\
 &  & \ddots & \\
\cdots & \cdots &&  \sum_{j=1}^N c_{jN}\sum_{i=1}^Na_{Ni}b_{ij} \\
\end{bmatrix}\Bigg) \notag \\
&= \sum_{k=1}^N\sum_{j=1}^N\sum_{i=1}^Na_{ki}b_{ij}c_{jk}. \notag
\end{align}

Now consider the trace of the permutation of this matrix product,

\begin{align}
\text{tr}(\mathbf{C}\mathbf{A}\mathbf{B}) &= \sum_{k=1}^N\sum_{j=1}^N\sum_{i=1}^Na_{ji}b_{ik}c_{kj} \notag \\
&= \text{tr}(\mathbf{A}\mathbf{B}\mathbf{C}), \notag
\end{align}

where $j$ and $k$ are swapped. This result is called the trace cyclic permutation property, as it shows that $\text{tr}(\mathbf{A}\mathbf{B}\mathbf{C}) = \text{tr}(\text{ROR}(\mathbf{A}\mathbf{B}\mathbf{C}))$. A related result is known as the \emph{trace trick}. Note that $\mathbf{x}^T\mathbf{A}\mathbf{x} = \text{tr}(\mathbf{x}^T\mathbf{A}\mathbf{x})$, since the result of the product is a $1 \times 1$ matrix (i.e. a scalar). By the permutation property we have, $\mathbf{x}^T\mathbf{A}\mathbf{x} = \text{tr}(\mathbf{x}\mathbf{x}^T\mathbf{A}) = \text{tr}(\mathbf{A}\mathbf{x}\mathbf{x}^T)$.

\subsection{Vector Calculus}

The single most important principle of vector calculus is the definition of what it means to differentiate with respect to a vector. In general, the gradient of a multi-variate function is the vector of partial derivatives, that is,

$$\nabla\mathbf{f} \triangleq \frac{\partial f}{\partial(x_1, \dots, x_n)} = \bigg[\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}, \dots, \frac{\partial f}{\partial x_{n}}\bigg].$$

From our definition of gradient, it is easy to derive some of the fundamental formulae of vector calculus. For function $f : \mathbb{R}^{n} \to \mathbb{R}^{m}$, that is, a vector-valued function in $n$ variables, its derivative, known as the Jacobian $\mathbf{J}$, is,

$$\mathbf{J} \triangleq \frac{\partial (f_1, \dots, f_m)}{\partial(x_1, \dots, x_n)} = \begin{bmatrix}
\frac{\partial f_1}{\partial x_{1}}&\frac{\partial f_1}{\partial x_{2}}&\cdots&\frac{\partial f_1}{\partial x_{n}}\\
\frac{\partial f_2}{\partial x_{1}}&\frac{\partial f_2}{\partial x_{2}}&\cdots&\frac{\partial f_2}{\partial x_{n}}\\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_{1}}&\frac{\partial f_m}{\partial x_{2}}&\cdots&\frac{\partial f_m}{\partial x_{n}}\\
\end{bmatrix},$$

that is, the matrix of partial derivatives corresponding to each of the input and output pairs. In the case that $f$ is scalar-valued, the Jacobian becomes the row vector given above. The Jacobian features as the first-order term in the Taylor expansion of a multivariate function. For a \emph{scalar}-valued function, $f$, the Hessian matrix is the matrix of all second order partial derivatives, written,

$$\mathbf{H} \triangleq \nabla^2\mathbf{f} = \mathbf{J}(\nabla\mathbf{f}(\mathbf{x})) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_{1}^2}&\frac{\partial^2 f}{\partial x_{1}\partial x_{2}}&\cdots&\frac{\partial^2 f}{\partial x_{1}\partial x_{n}}\\
\frac{\partial^2 f}{\partial x_{2}\partial x_{1}}&\frac{\partial^2 f}{\partial x_{2}^2}&\cdots&\frac{\partial^2 f}{\partial x_{2}\partial x_{n}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_{n}\partial x_{1}}&\frac{\partial^2 f}{\partial x_{n}\partial x_{2}}&\cdots&\frac{\partial^2 f}{\partial x_{n}^2}\\
\end{bmatrix}.$$

Note that unlike the Jacobian, the Hessian is not defined for vector-valued functions.

\subsubsection{Differentiating a Dot Product}

Following our rule from above, we have,

\begin{align}
\frac{\partial(\mathbf{b}^T\mathbf{a})}{\partial\mathbf{a}} =
\begin{bmatrix}
\frac{\partial}{\partial a_1} \mathbf{b}^T\mathbf{a} \\
\frac{\partial}{\partial a_2} \mathbf{b}^T\mathbf{a} \\
\vdots \\
\frac{\partial}{\partial a_n} \mathbf{b}^T\mathbf{a} \\
\end{bmatrix}
= 
\begin{bmatrix}
b_1 \\
b_2 \\
\vdots \\
b_n \\
\end{bmatrix}
= \mathbf{b} \notag.
\end{align}

Of course, if we replace $\mathbf{b}$ with $\mathbf{a}$, we are differentiating the squared $l_2$ norm\footnote{The $l_2$ norm of a vector, $\mathbf{a}$, is $|\mathbf{a}|_2 = \sqrt{\sum_i a_i^2}$. The $l_1$ norm is $|\mathbf{a}|_1 = \sum_i |a_i|$.} of $\mathbf{a}$ $\frac{\partial(\mathbf{a}^T\mathbf{a})}{\partial\mathbf{a}} = 2\mathbf{a}$.

\subsubsection{Differentiating a Vector Quadratic}

Beginning with a single element we have, $\partial(\mathbf{a}^T\mathbf{A}\mathbf{a})/\partial a_1 = 2A_{i, i}a_i + \sum_{j \neq i} A_{ij}a_j + \sum_{j \neq i} A_{ji}a_j = \mathbf{a}^TA(i, :) + \mathbf{a}^TA(:, i)$. Thus,

\begin{align}
\frac{\partial(\mathbf{a}^T\mathbf{A}\mathbf{a})}{\partial\mathbf{a}} =
\begin{bmatrix}
\frac{\partial}{\partial a_1} \mathbf{a}^T\mathbf{A}\mathbf{a} \\
\frac{\partial}{\partial a_2} \mathbf{a}^T\mathbf{A}\mathbf{a} \\
\vdots \\
\frac{\partial}{\partial a_n} \mathbf{a}^T\mathbf{A}\mathbf{a} \\
\end{bmatrix}
=
\begin{bmatrix}
\mathbf{a}^TA(1, :) + \mathbf{a}^TA(:, 1) \\
\mathbf{a}^TA(2, :) + \mathbf{a}^TA(:, 2) \\
\vdots \\
\mathbf{a}^TA(n, :) + \mathbf{a}^TA(:, n) \\
\end{bmatrix}
&= \mathbf{a}^T\mathbf{A}^T + \mathbf{a}^T\mathbf{A} \notag \\
&= (\mathbf{A} + \mathbf{A}^T)\mathbf{a} \notag
\end{align}

\subsubsection{Trace Formula}

Considering firstly the partial derivative of a single element, it is clear that,

$$\frac{\partial}{\partial A_{ij}}\big(\text{tr}(\mathbf{B}\mathbf{A})\big) = \frac{\partial}{\partial A_{ij}}\big(\mathbf{B}(1, :)\mathbf{A}(:, 1) + \mathbf{B}(2, :)\mathbf{A}(:, 2) + \cdots + \mathbf{B}(n, :)\mathbf{A}(:, n)\big) = B_{ji}.$$ Thus, it is clear that in general, 

$$\frac{\partial}{\partial A} (\text{tr}(\mathbf{BA})) = \mathbf{B}^T.$$

\subsubsection{Log Formula}

Considering firstly the partial derivative of a single element, it is clear that,

$$\frac{\partial}{\partial A_{ij}}\log|\mathbf{A}| = \frac{1}{|\mathbf{A}|}C_{ij},$$

where $C_{ij}$ is row $i$, column $j$, of the cofactor matrix, $\mathbf{C}$, the matrix whose elements are each of the sub-determinants used to calculate the determinant of $\mathbf{A}$. Noting the general inverse formula, $\mathbf{A}^{-1} = \frac{1}{|\mathbf{A}|}\mathbf{C}^T$, we have,

$$\frac{\partial}{\partial \mathbf{A}}\log|\mathbf{A}| = \frac{1}{|\mathbf{A}|}\mathbf{C} = (\mathbf{A}^T)^{-1}.$$

\section{Series}

This section introduces series and infinite sums. Notably, it presents Taylor series, a technique with many uses in the material to come. The section culminates in deriving an infinite sum for $\pi$.

\subsection{Geometric Series}

A geometric series\footnote{A series is the sum of a sequence of terms.} is a series, $s$, of the form, $$s = a + ar + ar^2 + ar^3 + \cdots + ar^{n-1} = \sum_{k=0}^{n-1}ar^k,$$ for some choice of a, r, and n. Multiplying this sum by the factor $r$ and adding $a$ gives, $$rs + a = a + ar + arr + ar^2r + \cdots + ar^{n-1}r = s + ar^n.$$ Therefore, $$s = a\frac{1 - r^n}{1 - r},$$ $r \neq 1$. Thus we have a closed-form expression\footnote{A closed-form expression is one that may be evaluated in a finite number of operations.} for the value of a geometric series, which will converge provided $|r| < 1$.

\subsection{Taylor Series}

A Taylor series, named after English mathematician Brook Taylor (1685-1731), is a technique for writing an infinitely differentiable function\footnote{A function is infinitely differentiable at a point if all its derivatives exist}, $f(x)$, as an infinite order polynomial,

\begin{align}
f(x) &= \sum_{i = 0}^{\infty} \frac{f^{(n)}(a)}{n!}(x - a)^n \notag \\
&= f(a) + \frac{f'(a)}{1!}(x - a) + \frac{f''(a)}{2!}(x - a)^2 + \frac{f'''(a)}{3!}(x - a)^3 + \cdots \notag,
\end{align}

for some choice of $a$. A Taylor series is an example of a power series. When $a = 0$, we have what is called a Maclaurin series, named after Scottish mathematician Colin Maclaurin (1698 - 1746). The first $n$ terms of the series may then be used to compute an approximation to the function at a point. For example, a calculator (which is only capable of addition and multiplication) will use a Taylor series approximation for its trigonometric functions. As we will see, Taylor series allow us to manipulate functions that have no closed form, and are in fact the most important technique to our analysis.

\subsubsection{Taylor's Theorem}

Taylor series follow from Taylor's theorem, which quantifies the remainder term for a $k$th order Taylor polynomial of a function. The theorem states that given a function $f : \mathbb{R} \to \mathbb{R}$ is $k$ times differentiable at point $a \in \mathbb{R}$, then there exists a function $h_k : \mathbb{R} \to \mathbb{R}$ such that,

$$f(x) = f(a) + \frac{f'(a)}{1!}(x - a) + \frac{f''(a)}{2!}(x - a)^2 + \cdots  + \frac{f(k)(a)}{k!}(x - a)^k + h_k(x)(x - a)^k,$$

where $$\lim_{a \to 0}h_k(a) = 0.$$

To show this, define $h_k(x) = \frac{f(x) - P(x)}{(x - a)^k}$ for $x \neq a$ and $h_k(x) = 0$ for $x = 0$, where $P(x)$ is the order $k$ Taylor polynomial for $f(x)$ centred on $a$. First note that $f^{(j)} = P^{(k)}$ for $k = 1, \dots k$. Applying L'H\^{o}pital's rule repeatedly,

\begin{align}
\lim_{x \to a} \frac{f(x) - P(x)}{(x - a)^k} &= \lim_{x \to a} \frac{\frac{d^k}{dx^k}(f(x) - P(x))}{\frac{d^k}{dx^k}(x - a)^k} \notag \\
&= \frac{1}{k!}\lim_{x \to a} \frac{f^{(k-1)}(x) - P^{(k-1)}(x)}{x - a} \notag \\
&= \frac{1}{k!}\lim_{x \to a} \frac{f^{(k-1)}(x) - f^{(k-1)}(a)}{x - a} - \frac{P^{(k-1)}(x) - P^{(k-1)}(a)}{x - a} \notag \\
&= \frac{1}{k!}(f^{(k)}(a) - P^{(k)}(a)) \label{eq:taylor} \\
&= 0, \notag
\end{align}

where (\ref{eq:taylor}) comes from the definition of a limit. Thus, we have shown the error term converges to 0 as $x \to a$.

\subsubsection{Convergence}

Holomorphic functions are a key interest in complex analysis, the study of functions of complex variables. Holomorphic functions are functions that are complex differentiable at every point on their domain. A consequence of complex differentiability is that the function is both infinitely differentiable and equal to its own Taylor series on that domain. A major result from complex analysis is that all homomorphic functions are analytic. A function is analytic if and only if its Taylor series converges to the function in some neighbourhood of points around $x_0$ for every $x_0$ in the function's domain. That is, there is always a locally convergent power series for the function. The radius of convergence of a power series,

$$f(z) = \sum_{n=0}^{\infty}c_n(z - a)^n$$

defined around a point $a$ is $r$, defined to be,

$$r = \sup\Bigg\{|z - a| \ \Bigg| \ \sum_{n=0}^{\infty}c_n(z - a)^n \ \text{converges} \Bigg\},$$

beyond which the series diverges. An elementary function is a function in one variable expressible in terms of a finite number of arithmetic operators, exponentials, logarithms and roots. This includes trigonometric and hyperbolic functions, as these are expressible as complex exponential functions. Elementary functions are analytic at all but a finite number of points (singularities). Functions that are analytic everywhere (on all complex points) are said to be \emph{entire} functions, and have no singularities. These include polynomials (whose Taylor expansions are of course equal to themselves), trigonometric, hyperbolic, and exponential functions, and many others. Such functions have an infinite radius of convergence.

\subsubsection{Taylor Expansions of Sine and Cosine}

In the analysis to come, we will use the Taylor series for $\sin x$,

\begin{align}
\sin x &= \sin 0 + \frac{\cos 0}{1!}(x - 0) - \frac{\sin 0}{2!}(x - 0)^2 - \frac{\cos 0}{3!}(x - 0)^3 + \cdots \notag \\
&= x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots \notag \\
&= \sum_{n=0}^{\infty}\frac{(-1)^n}{(2n + 1)!}x^{2n+1}, \notag
\end{align}

as well as for $\cos x$,

$$
\cos(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots = \sum_{n=0}^{\infty}\frac{(-1)^n}{(2n)!}x^{2n}.
$$

These are entire functions, hence the Taylor series converge for all real numbers.

\subsubsection{Bernoulli Numbers}

The Bernoulli numbers are an infinite sequence of numbers that appear sufficiently often in number theory to merit a name (such as $\pi$ and $e$). They originate as the coefficients of the Bernoulli polynomials, but make various other appearances in number theory. They were first discovered in Europe\footnote{The Bernoulli numbers were independently discovered by the great Japanese mathematician Seki Takakazu around the same time. It is unknown which discovery came first.} by Swiss mathematician, Jakob Bernoulli (1655-1705), a member of the eminent Bernoulli family that produced several noted mathematicians. The first few Bernoulli numbers are,

$$B_0 = 1, B_1 = \pm \frac{1}{2}, B_2 = \frac{1}{6}, B_3 = 0, B_4 = -\frac{1}{30}, B_5 = 0, B_6 = \frac{1}{42}, B_7 = 0, B_8 = -\frac{1}{30}.$$

Curiously, the second Bernoulli number, $B_1$, may be plus or minus $\frac{1}{2}$. Therefore, there are strictly speaking \emph{two} sequences, the \emph{first} Bernoulli numbers (with $B_1 = -\frac{1}{2}$) and the \emph{second} Bernoulli numbers (with $B_1 = \frac{1}{2}$). The numbers may be defined by a recurrence relation. For example, the second Bernoulli numbers are related by,

$$B_n = 1 - \sum_{k=0}^{n-1}{{{n}\choose{k}}}\frac{B_k}{n - k + 1}, B_0 = 1.$$

The Bernoulli numbers appear within some Taylor expansions, such as that of the hyperbolic tangent function, $\tanh x$,

$$\tanh x = x - \frac{1}{3}x^3 + \frac{2}{15}x^5 - \frac{17}{315}x^7 + \cdots = \sum_{n=1}^{\infty}\frac{B_{2n}4^n(4^n-1)}{(2n)!}x^{2n-1}.$$

The Bernoulli numbers also appear in the Euler-Maclaurin formula, which specifies the error incurred by approximating integrals with sums, for example, the error term of the trapezoidal rule.

\subsection{$\pi$ as an Infinite Series}

\subsubsection{Pythagorean Trigonometric Identity}

From the Pythagorean theorem, we may derive an important trigonometric identity. Given a right-angle triangle, we denote the sides $a, b, c$ such that $\sin{x} = \frac{a}{c}$ and $\cos{x} = \frac{b}{c}$. Note that this implies that, $$\tan{x} = \frac{a}{b} = \frac{\sin{x}}{\cos{x}}.$$ Now, dividing the Pythagorean equation by $c^2$ gives,

$$\frac{a^2}{c^2} + \frac{b^2}{c^2} = \frac{c^2}{c^2},$$

hence,

$$\sin^2x + \cos^2x = 1.$$

From this it may be seen that the coordinates $(\cos x, \sin x)$ sketch a unit circle on the real plane.

\subsubsection{Differentiating Inverse Trigonometric Functions}

The function, $\arctan x$ (otherwise written $\tan^{-1}x$) is an inverse trigonometric function, the inverse mapping of the function $\tan{x}$. If we let $y = \arctan{x}$, then we have $\tan{y} = x$. Differentiating both sides gives,

$$\frac{d}{dx} \tan{y} = 1.$$

Now,

$$\frac{d}{dx} \tan{y} = \frac{dy}{dx}\frac{d}{dy}\tan{y} = \frac{dy}{dx} \frac{1}{\cos^2{y}},$$

so,

$$\frac{dy}{dx} = \cos^2(\arctan{x}).$$

Rearranging the Pythagorean trigonometric identity, we have $\cos^2 x = 1 - \sin^2x$, and squaring and rearranging our other identity, $\sin^2{x} = \cos^2{x}\tan^2{x}$. By substitution we obtain, $\cos^2x = \frac{1}{1 + \tan^2x}$. Thus, we have,

\begin{align}
\frac{d}{dx}\arctan{x} &= \frac{1}{1 + \tan^2(\arctan{x})} \notag \\
&= \frac{1}{1 + x^2} \notag
\end{align}

\subsubsection{An Infinite Series For $\pi$}

From the previous result, we have $$\arctan{x} = \int \frac{1}{1 + x^2} dx.$$ Replacing the integrand with its Maclaurin series gives,

\begin{align}
\arctan{x} &= \int \sum_{n=0}^{\infty} (-1)^nx^{2n} dx \notag \\
&= \sum_{n=0}^{\infty} \frac{(-1)^n}{2n + 1}x^{2n+1}. \notag
\end{align}

Now, we know that a right-angle triangle with two $45^{\circ}$ angles has opposite and adjacent sides of equal length. Hence, $\arctan({1}) = \pi/4$. Evaluating our infinite series at $x = 1$ gives,

$$\pi/4 = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \dots$$

Though pretty, this series is extremely slow to converge. Other series representations for $\pi$ exist, deriving from other trigonometric identities.

\section{Infinite Products}

Infinite products are defined for a sequence of (complex) numbers, $a_1, a_2, a_3, \dots$, as,

$$\prod_{n=1}^{\infty}a_n = a_1a_2a_3\dots$$

Just like infinite sums they may converge or diverge. The product converges if the limit of the partial product (the first $n$ factors) exists as $n \to \infty$. In this section we present some of the more stupendous mathematical results involving infinite products.

\subsection{Vi\`ete's Formula}

The first recorded infinite product was formulated by French mathematician Fran\c cois Vi\`ete (1540-1603), and gives an expression for $\pi$. The expression derives from an old method for approximating the area of a circle: suppose we have a circle with radius 1. Its area is $\pi$ units. Suppose then that we inscribe a square within this circle. The area of the square (4-sided polygon) is $(\sqrt{2})^2 = 2$. This gives a (very) rough approximation to the area of the circle. If we then inscribe a hexagon (8-sided polygon) in the circle, it is clear we get a better approximation. A hexadecahedron (16-sided polygon) would give a better approximation again, and so on. As the number of sides goes to infinity, the inscribed shapes `telescope' to the shape of the circle.

We can therefore derive an expression for the area of the circle, $\pi$, by taking the area of the square and scaling it by the ratio of the area of the octagon to the area of the square, and scaling this result by the ratio of hexadecahedron to octagon, icosidodecahedron to hexadecahedron, and so on. We choose shapes whose number of sides are powers of two because there is a useful relation between these, as we shall see. For brevity, we will denote the area of the $2^n$-sided polygon as $A_n$. Thus, $A_2$ is the area of the square, $A_3$ the octagon, and so on. Our technique for calculating the area of the circle is therefore described by,

$$
\pi = 2 \cdot \frac{A_3}{A_2} \cdot \frac{A_4}{A_3} \cdot \frac{A_5}{A_4} \cdot \cdots
$$

We will rearrange this as,

$$
\frac{2}{\pi} = \frac{A_2}{A_3} \cdot \frac{A_3}{A_4} \cdot \frac{A_4}{A_5} \cdot \cdots
$$

Now, it can be seen that a $2^n$-sided polygon is an arrangement of $2^n$ equal isosceles triangles, with arm length $1$ and angle $2\pi/2^n$. Applying the formula for the area of an isosceles triangle gives, $A_n = 2^{n}\cdot\frac{1}{2}\sin(\pi / 2^{n-1})$. Thus, we have,

$$
\frac{2}{\pi} = \frac{1}{2\sin(\pi/4)} \cdot \frac{\sin(\pi/4)}{2\sin(\pi/8)} \cdot \frac{\sin(\pi/8)}{2\sin(\pi/16)} \cdot \cdots
$$

Applying the trigonometric identity, $\sin{2x} = 2\sin x\cos x$, our expression simplifies to,

$$
\frac{2}{\pi} = \frac{\sqrt{2}}{2} \cdot \cos(\pi/8) \cdot \cos(\pi/16) \cdot \cos(\pi/32) \cdot \cdots
$$

A useful trigonometric identity can be derived from the angle sum identity, $\cos(\alpha + \beta) = \cos\alpha\cos\beta - \sin\alpha\sin\beta$. Namely that $\cos x = \cos^2(x/2) - \sin^2(x/2)$. Applying the pythagorean identity, $\cos x = 2\cos^2(x/2) - 1$. Finally, rearranging gives,

$$\cos \Big(\frac{x}{2}\Big) = \pm\sqrt{\frac{1 + \cos x}{2}}.$$

Exploiting this identity gives us a recurrence relationship from $\cos x$ to $\cos(x/2)$, finally yielding the startling,

$$
\frac{2}{\pi} = \frac{\sqrt{2}}{2} \cdot \frac{\sqrt{2 + \sqrt{2}}}{2} \cdot \frac{\sqrt{2 + \sqrt{2 + \sqrt{2}}}}{2} \cdot \cdots
$$

\subsection{The Basel Problem}
In 1734, Leonhard Euler (1707-1783), found the elegant solution to the sum,

$$\sum_{n=0}^{\infty}\frac{1}{n^2} = 1 + \frac{1}{4} + \frac{1}{9} + \cdots,$$

a problem posed 90 years earlier by Italian mathematician, Pietro Mengoli (1626-1686). Its solution had previously defied the attempts of the eminent Bernoulli dynasty of mathematicians. The problem is named after Euler's hometown, and first place of employment in Switzerland. The solution involves infinite products, and a mathematical leap of faith, that was only later confirmed rigorously. Incidentally, this sum is the value of $\zeta(2)$, where $\zeta(s)$ is the then undiscovered Riemann zeta function\footnote{The Riemann zeta function, defined over the complex numbers, formulated by Bernhard Riemann (1826-1866) has proven to be of great mathematical interest. The Riemann hypothesis, which conjectures the placement of zeta function roots has great implications for number theory, particularly the distribution of prime numbers. The hypothesis is the subject of one of the seven Millennium problems.}. Euler's solution starts by considering the function,

\begin{align}
\frac{\sin x}{x} &= \frac{ x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots}{x}  \notag \\
&=  1 - \frac{x^2}{3!} + \frac{x^4}{5!} - \cdots \label{eq:sinx}
\end{align}

Having written the infinite series expansion of this function, it is obvious that the limit, $\lim_{x\to 0} \sin x / x = 1$. Euler then found an infinite \emph{product} expansion for this function,

\begin{align}
\frac{\sin x}{x} &= \prod_{k=1}^{\infty}1 - \frac{x^2}{k^2\pi^2}, \notag \\
&= \Big(1 - \frac{x}{\pi}\Big)\Big(1 + \frac{x}{\pi}\Big)\Big(1 - \frac{x}{2\pi}\Big)\Big(1 + \frac{x}{2\pi}\Big)\Big(1 - \frac{x}{3\pi}\Big)\Big(1 + \frac{x}{3\pi}\Big)\cdots \notag
\end{align}

reasoning that the infinitely many roots ($\pm\pi, \pm2\pi, \pm3\pi, \dots $) of the left- and right-hand functions were the same, hence the functions themselves. This was not an entirely rigorous piece of reasoning, but it was later confirmed to be correct\footnote{See the Weierstrass factorisation theorem.}. Multiplying this product out shows that the coefficient for $x^2$ is,

$$
-\frac{1}{\pi^2}\Big(1 + \frac{1}{4} + \frac{1}{9} + \cdots \Big)= -\frac{1}{\pi^2}\sum_{n=0}^{\infty}\frac{1}{n^2}.
$$

But we know from (\ref{eq:sinx}) that the coefficient for $x^2$ is $-\frac{1}{3!}$, hence, equating the two gives,

$$
\sum_{n=1}^{\infty}\frac{1}{n^2} = \frac{\pi^2}{6}.
$$

\subsection{Wallis' Product}

Wallis's product is an infinite product expression for $\pi$ discovered by the English mathematician John Wallis (1616-1703). It can be derived starting from the same expression from the Basel problem,

$$
\frac{\sin x}{x} = \prod_{k=1}^{\infty}1 - \frac{x^2}{k^2\pi^2}.
$$

If we let $x = \pi/2$ we have,

\begin{align}
\frac{2}{\pi} &= \prod_{k=1}^{\infty}1 - \frac{1}{4k^2} \notag \\
\implies \frac{\pi}{2} &= \prod_{k=1}^{\infty} \frac{4k^2}{4k^2 - 1} \notag \\
&= \prod_{k=1}^{\infty} \frac{2k}{2k - 1} \cdot \frac{2k}{2k + 1} \notag \\
&= \frac{2}{1} \cdot \frac{2}{3} \cdot \frac{4}{3} \cdot \frac{4}{5} \cdot \frac{6}{5} \cdot \frac{6}{7} \cdots \notag
\end{align}

If we consider the partial product, we derive a related expression,

\begin{align}
p_k &= \prod_{n=1}^{k} \frac{2n}{2n - 1} \cdot \frac{2n}{2n + 1} \notag \\
&= \frac{1}{2k + 1} \prod_{n=1}^{k}\frac{(2n)^4}{[2n(2n - 1)]^2} \notag \\
&= \frac{1}{2k + 1} \cdot \frac{2^{4k}(k!)^4}{[(2k)!]^2} \to \frac{\pi}{2}, \notag
\end{align}

as $k \to \infty$.

\subsection{The Method of Eratosthenes}

The method of Eratosthenes is a prime number sieve algorithm discovered by Eratosthenes of Cyrene (c. 276 BC - 195/194 BC). It is central to the derivation of the Euler product formula. It is a method for identifying all primes up to some natural number, $N$. An example of the algorithm is given here. Let $N = 15$, then we have the list of candidate primes,

$$2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15$$

2 must be prime as there are no smaller numbers in the list to divide it. We can then eliminate all multiples of 2, as they are composite by definition,

$$2, 3, \cancel{4}, 5, \cancel{6}, 7, \cancel{8}, 9, \cancel{10}, 11, \cancel{12}, 13, \cancel{14}, 15$$

3 was not eliminated by this step, and as the new smallest candidate, it must be prime. Likewise, we may eliminate all multiples of 3,

$$2, 3, \cancel{4}, 5, \cancel{6}, 7, \cancel{8}, \cancel{9}, \cancel{10}, 11, \cancel{12}, 13, \cancel{14}, \cancel{15}$$

Continuing in this way (though by now all non-primes have been eliminated), we find 2, 3, 5, 7, 11, and 13 are the primes up to 15.

\subsection{The Euler Product Formula}

The Riemann zeta function analytically continues the infinite sum,

\begin{align}\zeta(s) = \sum_{i=1}^{\infty}\frac{1}{n^s} = 1 + \frac{1}{2^s} + \frac{1}{3^s} + \frac{1}{4^s} + \frac{1}{5^s} + \cdots,\label{eq:zeta}
\end{align}

for complex numbers, $s$, with $\text{Re}(s) > 1$. The case of $s = 2$ is the form of the Basel problem. Consider,

\begin{align}\frac{1}{2^s}\zeta(s) = \frac{1}{2^s} + \frac{1}{4^s} + \frac{1}{6^s} + \frac{1}{8^s} + \frac{1}{10^s} + \cdots \label{eq:halfzeta}
\end{align}

Subtracting (\ref{eq:halfzeta}) from (\ref{eq:zeta}) removes all multiples of 2 leaving,

\begin{align}\Big(1 - \frac{1}{2^s}\Big)\zeta(s) = 1 + \frac{1}{3^s} + \frac{1}{5^s} + \frac{1}{7^s} + \frac{1}{9^s} + \cdots \label{eq:thirdzeta}
\end{align}

Multiplying (\ref{eq:thirdzeta}) by $1/3^s$ and subtracting it from itself gives,

$$\Big(1 - \frac{1}{3^s}\Big)\Big(1 - \frac{1}{2^s}\Big)\zeta(s) = 1 + \frac{1}{5^s} + \frac{1}{7^s} + \frac{1}{9^s} + \frac{1}{11^s} + \cdots$$

If we continue in this fashion, eliminating the primes and all their remaining multiples, we are following the method of Eratosthenes, leaving,

$$
\cdots\Big(1 - \frac{1}{11^s}\Big)\Big(1 - \frac{1}{7^s}\Big)\Big(1 - \frac{1}{5^s}\Big)\Big(1 - \frac{1}{3^s}\Big)\Big(1 - \frac{1}{2^s}\Big)\zeta(s) = 1.
$$

Dividing through leaves,

$$\zeta(s) = \sum_{i=1}^{\infty}\frac{1}{n^s} = \prod_{p \ \text{prime}}\Bigg(\frac{1}{1 - p^{-s}}\Bigg),$$

which is an infinite product, since we know from Euclid that there are infinitely many primes.

\section{Deriving the Golden Ratio}

This section presents the concept of continued fractions and the distinctive continued fraction expression for the golden ratio.

\subsection{Continued Fractions}
Continued fractions are a way of specifying fractional numbers as a sequence of nested fractions,
$$a_0 + \cfrac{1}{a_1 + \cfrac{1}{a_2 + \cfrac{1}{\ddots + \cfrac{1}{a_n}}}},$$
where $a_0, a_1, \dots, a_n$ are integers. For example, take the fraction, $ f = \frac{387}{259}$. We can then write, $f = 1 + \frac{128}{259} = 1 + \frac{1}{259/128}$. Following this procedure,

$$
f = \cfrac{387}{259}
= 1 + \cfrac{1}{259/128}
= 1 + \cfrac{1}{2 + \cfrac{1}{128/3}} 
= 1 + \cfrac{1}{2 + \cfrac{1}{42 + \cfrac{1}{3/2}}} 
= 1 + \cfrac{1}{2 + \cfrac{1}{42 + \cfrac{1}{1 + \cfrac{1}{2}}}}.
$$

Standard notation takes the values on the diagonal and writes them as, $f = [1; 2, 42, 1, 2]$.

\subsection{Fibonacci Sequences}

A Fibonacci sequence is a sequence of numbers for seed values, $F_0$ and $F_1$, and recurrence relationship,

$$F_n = F_{n-1} + F_{n-2},$$

for $n \geq 2$. If we take the ratio of the $n+1$th and $n$th terms in the sequence we have,

$$
\cfrac{F_{n+1}}{F_n}
= \cfrac{F_n + F_{n - 1}}{F_n}
= 1 + \cfrac{1}{F_{n}/F_{n - 1}}
= 1 + \cfrac{1}{1 + \cfrac{1}{F_{n-1}/F_{n - 2}}}
= \cdots
= 1 + \cfrac{1}{1 + \cfrac{1}{\ddots + \cfrac{F_0}{F_1}}},
$$

the fraction continuing for $n - 1$ levels.

\subsection{The Golden Ratio}

The golden ratio is a number with many interesting properties, and with some mythical connections to the natural world. It is the ratio that arises between two numbers, $a$ and $b$, when $a : b = (a + b) : a$. It is also the limiting value of the ratio of successive terms in a Fibonacci sequence. We may discover the self-referential golden-ratio, $\varphi$, by looking at the self-referential infinite continued fraction, $\varphi = [1; 1, 1, 1, \dots]$. That is, the ratio of $F_{n+1}/F_n$ as $n \to \infty$.

$$
\varphi = 1 + \cfrac{1}{1 + \cfrac{1}{1 + \cfrac{1}{\ddots}}} = 1 + \frac{1}{\varphi},
$$

noting the recursion. Multiplying through gives the quadratic, $\varphi^2 - \varphi - 1 = 0$. Applying the quadratic formula gives,

$$\varphi = \frac{1 \pm \sqrt{5}}{2}.$$

\section{The Exponential Function}

The exponential function is a transcendental entire function, with its base $e$ a transcendental number\footnote{A transcendental number is one that is not algebraic. An algebraic number is a number that is the root of a non-zero polynomial function with rational coefficients, and is a generalisation of the ancient Greek concept of \emph{constructible} numbers. Another way of putting this is that if a number is transcendental there exists no expression containing a finite number of integers and elementary operations that can express the number. To illustrate, $\sqrt{17/4}$ is not transcendental (though it is irrational) as it is the square root of a quotient of integers. Formally, it would solve the expression, $4x^2 = 17$. It is known that most numbers are transcendental.}. It makes many appearances throughout number theory and statistics.

\subsection{Deriving the Exponential Function}

The starting point for deriving the exponential function is to consider the differential equation,

$$f = \frac{df}{dx},$$ that is, a function that is its own derivative. This equation models many common physical problems. One solution to this is the infinite series,

$$f(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!},$$ since $f'(x) = \sum_{n=1}^{\infty} \frac{x^{n-1}}{(n - 1)!} = \sum_{n=0}^{\infty} \frac{x^{n}}{n!} = f(x)$. Note that this is the Maclaurin series for a function whose derivatives are all equal to 1 at 0. Evaluating the function at 1 gives,

$$f(1) = \sum_{n=0}^{\infty} \frac{1}{n!} = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \frac{1}{4!} + \cdots,$$ which must converge since the rate of growth after the second term is inferior to that of geometric series with base $1/2$. The number it converges to is know as $e = \sum_{k=0}^{\infty}  \frac{1}{k!} \approx 2.718.$ With the binomial theorem, an equivalence may be shown between this infinite sum, and the product,

$$e = \lim_{n \to +\infty} \bigg(1 + \frac{1}{n}\bigg)^n.$$

This form is particularly useful in financial modelling. Compound interest gives an expression for the future value of a financial device, $F$, based on its present value, $P$, the interest rate, $r$, and the compounding frequency, $n$, over $t$ time periods, with $F = P(1 + r/n)^{nt}$. As $n\to\infty$, we have what is called \emph{continuous} compound interest, and the formula clearly converges to $F = Pe^{rt}$.

The closed form of the function may be deduced by considering how it grows. Consider,

\begin{align}
f(x + 1) &= \sum_{n=0}^{\infty} \frac{(x + 1)^n}{n!} \notag \\
&= \sum_{n=0}^{\infty} \sum_{k=0}^{n}{{n}\choose{k}}\frac{x^k \cdot 1^{n-k}}{n!} \label{eq:e1}, \\
&= \sum_{n=0}^{\infty} \sum_{k=0}^{n}\frac{x^k}{k!(n-k)!} \label{eq:e2}, \\
&= \begin{array}{cccccccc}
\frac{1}{0! \cdot 0!} & + & & & & \\
\frac{1}{0! \cdot 1!} & + & \frac{x}{1! \cdot 0!} & + & & & & \\
\frac{1}{0! \cdot 2!} & + & \frac{x}{1! \cdot 1!} & + & \frac{x^2}{2! \cdot 0!} & + & &\\
\frac{1}{0! \cdot 3!} & + & \frac{x}{1! \cdot 2!} & + & \frac{x^2}{2! \cdot 1!} & + & \frac{x^3}{3! \cdot 0!} & +\\
\vdots & \vdots  & \vdots  &\vdots  & \vdots  & \vdots  & \vdots  & \vdots 
\end{array} \notag \\
&= \sum_{n=0}^{\infty} \frac{x^n}{n!} \sum_{k=0}^{\infty} \frac{1}{k!} \notag, \\
&= f(x) \cdot e \notag,
\end{align}

where (\ref{eq:e1}) comes from the binomial theorem, and (\ref{eq:e2}) substitutes the binomial coefficient. Therefore, $f(1 + \delta) / f(1) = f(\delta)$. Thus, the ratio of growth of the function is fixed for an increase $\delta$ in $x$, regardless of the value of $x$. Therefore,

$$\frac{f(\delta)}{f(0)} = \frac{f(2\delta)}{f(\delta)} = \cdots = \frac{f(1)}{f((n-1)\delta)},$$

where $\delta = 1 / n$, for any choice of n. Multiplying the $n$ terms together gives $f(1/n)^n = f(1) / f(0) = e$. Therefore, $f(x/n)$ is the $x/n\textsuperscript{th}$ root of $e$. Thus, $f(x/n) = \sqrt[n]{e^x}$. Or simply, $f(x) = e^x$.

\subsection{Other Solutions}

Could any other function solve our differential equation? No, as can be seen from separation of variables, $$\frac{1}{f}df = {dx}.$$ Integrating both sides gives $$\ln{f} = x + C.$$ So, $$f = Ce^x.$$

\subsection{Euler's Formula}

Here we take the opportunity to derive a very famous equation\footnote{The equation is in fact a special case of a more general result found earlier by de Moivre.}, based on what we have covered so far. Consider the Maclaurin series,

\begin{align}
e^{ix} &= 1 + ix + \frac{(ix)^2}{2!} + \frac{(ix)^3}{3!} + \frac{(ix)^4}{4!} + \frac{(ix)^5}{5!} + \cdots \notag \\
&= 1 + ix - \frac{x^2}{2!} - i\frac{x^3}{3!} + \frac{x^4}{4!} + i\frac{x^5}{5!} - \cdots \notag
\end{align}

The reader may now confirm that the even terms correspond with $\cos x$, and the odd terms with $i\sin x$. Thus, $$e^{ix} = \cos x + i\sin x,$$ a fantastic result! Now, just as coordinates $(\cos x, \sin x)$ trace a unit circle on the real plane, the complex numbers arising from $e^{ix} = \cos x + i\sin x$ do so on the complex plane (Figure \ref{fig:eulersformula}). Thus, we have a basis on which to perform complex exponentiation, for example, $z^{a + bi} = z^a \cdot (e^{\ln z\cdot bi}) = z^a \cdot (\cos(b\ln z) + i\sin(b\ln z))$. Furthermore, for any integer $k$,

$$
e^{i(k\pi)} = \cos (k\pi) + i\sin (k\pi) \notag = -1.
$$

For the case of $k = 1$, we may rearrange to acquire Euler's\footnote{Euler (1707-1783) was a Swiss mathematician, considered one of the greatest of all time.} identity,

$$
e^{i\pi} + 1 = 0,
$$

uniting the base of the natural logarithm, $e$, the unit of imaginary numbers, $i$, the ratio of a circle's circumference to its diameter, $\pi$, the multiplicative identity, $1$, and the additive identity, $0$, in an equation involving a single sum, product, and exponentiation.

\subsubsection{Angle Sum Identities}

A nice way to derive the trigonometric identities for sums of angles is with Euler's formula. First note that,

$$e^{i(\alpha + \beta)} = \cos (\alpha + \beta) + i\sin (\alpha + \beta).$$

But also that,

\begin{align}
e^{i(\alpha + \beta)} = e^{i\alpha}e^{i\beta} &= (\cos \alpha + i\sin \alpha)(\cos \beta + i\sin \beta) \notag \\
&= (\cos\alpha\cos\beta - \sin\alpha\sin\beta) + i(\sin\alpha\cos\beta + \cos\alpha\sin\beta).\notag
\end{align}

Equating real and imaginary parts gives the trigonometric identities for sums of angles,

$$\cos (\alpha + \beta) = \cos\alpha\cos\beta - \sin\alpha\sin\beta,$$

and,

$$\sin (\alpha + \beta) = \sin\alpha\cos\beta + \cos\alpha\sin\beta.$$

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{Figures/eulersformula.png}
\caption{$e^{ix}$ traces a unit circle on the complex plane.\cite{eulerformula}}
\label{fig:eulersformula}
\end{figure}

\section{Integral Transforms}

Integral transforms are a technique for transforming a function in one domain to an equivalent function is another domain. It just so happens that some problems are more easily solved in one domain than the other.

%\subsection{Fourier Series}
%
%Fourier series are a 

\subsection{The Fourier Transform}

Fourier transforms are widely used in statistics, engineering, and physics. The Fourier transform maps a function in the time domain to a function in the frequency domain. The Fourier transform is defined as,
$$\hat{f}(\xi) = \int_{-\infty}^{\infty}f(x)e^{-2\pi ix\xi}\mathop{dx}.$$

The inverse Fourier transform is defined as,
$$f(x) = \int_{-\infty}^{\infty}\hat{f}(x)e^{2\pi i\xi x}\mathop{d\xi}.$$

\subsection{The Laplace Transform}

The Laplace transform is an integral transform defined as,

$$F(s) = \mathcal{L}\{f(t)\} = \int_{0}^{\infty} f(t)e^{-st}\mathop{dt}.$$

It is a more generalised transform than the Fourier transform, and maps a function, $f(t)$, from the time ($t$) domain to the $s$ domain. The inverse Laplace transform is a more complicated function and usually it is more practical to take inverse transforms from a reference table. For example, let $f(x) = \cos kt$, we first use a trick to convert it to a more amenable form,

\begin{align}
\cos(kt) &= \frac{1}{2}\cos(kt) + \frac{1}{2}i\sin(kt) + \frac{1}{2}\cos(kt) - \frac{1}{2}i\sin(kt) \notag \\
&= \frac{1}{2}(\cos(kt) + i\sin(kt)) + \frac{1}{2}(\cos(-kt) + i\sin(-kt)) \notag \\
&= \frac{1}{2}e^{kit} + \frac{1}{2}e^{-kit}. \notag
\end{align}

Then,

\begin{align}
\mathcal{L}\{\cos(kt)\} &= \int_{0}^{\infty} \frac{1}{2}e^{kit}e^{-st} \mathop{dx} + \int_{0}^{\infty} \frac{1}{2}e^{-kit}e^{-st} \mathop{dt} \notag \\
&= \int_{0}^{\infty} \frac{1}{2}e^{(-s + ki)t} \mathop{dx} + \int_{0}^{\infty} \frac{1}{2}e^{(-s - ki)t}\mathop{dt} \notag \\
&= \frac{1/2}{s - ki} + \frac{1/2}{s + ki} = \frac{s}{s^2 + k^2}. \notag
\end{align}

If we are to apply the Laplace transform to differential equations, it is useful to establish a result for the transform of a derivative. By parts,

\begin{align}
\mathcal{L}\{f(t)\} = \int_{0}^{\infty} f(t)e^{-st}dt &= \Bigg[\frac{f(t)e^{-st}}{-s}\Bigg]_0^{\infty} - \int_{0}^{\infty} f'(t)\frac{e^{-st}}{-s}dt \notag \\
&= \frac{f(0)}{s} + \frac{1}{s}\mathcal{L}\{f'(t)\} \notag,
\end{align}

thus in general, $\mathcal{L}\{f^{(n)}(t)\} = s^n \mathcal{L}\{f(t)\} - s^{n-1}f(0) - \dots - f^{(n-1)}(0)$. Now, the linear homogeneous ordinary differential equation,

$$y'' - 5y' + 6y = 0,$$

with initial conditions $y(0) = 2$, and $y'(0) = 2$, can be solved with the Laplace transform. Using the derivatives formula on the left-hand side of the equation,

$$\mathcal{L}\{y\} - 5\mathcal{L}\{y'\} + 6\mathcal{L}\{y''\} = s^2\mathcal{L}\{y\} - 2s - 2 - 5s\mathcal{L}\{y\} +10 + 6\mathcal{L}\{y\},$$

from which we have,

$$\mathcal{L}\{y\} = \frac{2s - 8}{s^2 - 5s + 6} = \frac{4}{s - 2} - \frac{2}{s - 3},$$

hence $$y = \mathcal{L}^{-1}\bigg\{\frac{4}{s - 2}\bigg\} - \mathcal{L}^{1}\bigg\{\frac{-2}{s - 3}\bigg\} = 4e^{2t} - 2e^{3t}.$$

\section{The Factorial Function}

The factorial function, written $n!$, for a positive integer $n$ is the product of all natural numbers less than or equal to n. That is, $$n! = n \times (n - 1) \times (n - 2) \times \cdots \times 1.$$ The value of $0!$ is 1, not as a matter of mathematical necessity, but because it continues the pattern: $0! = (n - n)! = n!/n! = 1$. Otherwise, we can note that the factorial function models the number of permutations of $n$ objects, that is, $^nP_n = n!$. The number of ways of permuting \emph{no} objects is assuredly 1, as no reorderings of \emph{no} objects are possible. There is also such a thing as the \emph{double} factorial function,

$$n!! = n \times (n - 2) \times (n - 4) \times \cdots,$$

which multiples all even numbers $\leq n$ if n is even, and all odd numbers $\leq n$ if n is odd.  The factorial function is undefined for non-natural numbers, but the gamma function extends it to all real and complex numbers. The factorial function grows faster than exponentiation, but not as fast as double exponentiation or tetration, the hyperoperation of repeated exponentiation. That is,

$$\mathcal{O}(a^x) < \mathcal{O}(x!) < \mathcal{O}(a^{b^x}) < \mathcal{O}(^nx = x^{x^{\iddots^{x}}})$$

\subsection{Stirling's Approximation}

Stirling's approximation is a powerful approximation for the factorial function that converges as $n \to \infty$. It was first discovered by de Moivre, but Scottish mathematician James Stirling (1692-1770) found a nice simplification. Its derivation begins by taking the log of the factorial function,

$$\ln(n!) = \ln(1) + \ln(2) + \ln(3) + \cdots + \ln(n).$$

Now we notice that subtracting $\frac{1}{2}(\ln(1) + \ln(n))$ gives the trapezoidal rule\footnote{The trapezoidal rule is a part of a family of techniques for approximating integrals known as Riemann sums.} approximation to the integral of $\ln(n)$ with $\Delta x = 1$. That is,

\begin{align}
\ln(n!) - \cancelto{0}{\frac{1}{2}\ln(1)} - \frac{1}{2}\ln(n) &= \frac{\cancelto{0}{\ln(1)} + \ln(2)}{2} + \frac{\ln(2) + \ln(3)}{2} + \frac{\ln(3) + \ln(4)}{2} \cdots + \frac{\ln(n + 1) + \ln(n)}{2} \notag \\
&\approx \int_{1}^{n} \ln{x} \mathop{dx} = n\ln n - n + 1 + \sum_{k=2}^{m} \frac{(-1)^kB_k}{k(k-1)}\Bigg(\frac{1}{n^{k-1}} - 1\Bigg) + R_{m,n}. \notag
\end{align}

The error term is expressed in terms of Bernoulli numbers, $B_k$ and the remainder term, $R_{m,n}$, from the Euler-Maclaurin formula. The index, $m$, may be chosen freely, with $R_{m, n} \to 0$ as $m \to \infty$. We next consider the behaviour of the error term as $n$ tends to $\infty$,

$$\lim_{n \to \infty}\Big(\ln(n!) - \frac{1}{2}\ln n - n\ln n + n\Big) = 1 - \sum_{k=2}^{m} \frac{(-1)^kB_k}{k(k-1)} + \lim_{n \to \infty}R_{m,n},$$

and it is known that $\lim_{n \to \infty}R_{m,n} = R_{m,n} + O\Big(\frac{1}{n^m}\Big)$. Thus, the difference tends toward a constant, denoted $c$. Returning to our equation, we have,

$$\ln(n!) = n\ln n - n + \frac{1}{2}\ln n + 1 + c + \sum_{k=2}^{m} \frac{(-1)^kB_k}{k(k-1)}\Bigg(\frac{1}{n^{k-1}}\Bigg) + \mathcal{O}\Bigg(\frac{1}{n^m}\Bigg).$$

To eliminate the bothersome sum, we can choose the parameter, $m = 1$, incurring an error term of greater magnitude, $O(1/n)$. Exponentiating both sides gives,

$$n! = e^c\cdot\Bigg(\frac{n}{e}\Bigg)^n \cdot \sqrt{n} \cdot \Bigg(1 + \mathcal{O}\Bigg(\frac{1}{n}\Bigg)\Bigg),$$

as $n \to \infty$. Note that the error term comes from the fact that $e^x \approx 1 + x$, for $x$ close to 0. To deal with the constant, $e^c$, which we will denote, $C$, recall from Wallis' product that,

$$\frac{1}{2n + 1} \cdot \frac{2^{4n}(n!)^4}{[(2n)!]^2} \to \frac{\pi}{2},$$

as $n \to \infty$. Substituting our convergent expression for $n!$,

$$\frac{1}{2n + 1} \cdot \frac{2^{4n}[C(n/e)^n\sqrt{n}]^4}{[C(2n/e)^{2n}\sqrt{2n}]^2} = \frac{n}{4n + 2}\cdot C^2 \to \frac{\pi}{2},$$

from which we can see $C \to \sqrt{2n}$, as $n \to \infty$. Finding the convergent value through application of Wallis' product was Sterling's contribution. Our approximation therefore simplifies to Sterling's approximation,

$$n! \to \sqrt{2\pi n}\Bigg(\frac{n}{e}\Bigg)^n.$$

\subsection{The Gamma Function}

The gamma function is a continuous function that interpolates the factorial function (Figure \ref{fig:factorial}), and extends its domain from the natural numbers to the real and complex numbers. It is defined as,

$$\Gamma(t) = \int_0^{\infty} x^{t-1}e^{-x}\mathop{dx}.$$

To show its connection to the factorial function, we integrate by parts,

\begin{align}
\Gamma(n) &= \big\{-x^{n-1}e^{-x}\big\}_{x=0}^{\infty} + (n - 1)\int_0^{\infty} x^{n-2}e^{-x}\mathop{dx} \notag \\
&= (n-1)\Gamma(n-1).\notag
\end{align}

Noting that $\Gamma(1) = 1$, it is clear that $\Gamma(n) = (n - 1)!$ for all positive integers, $n$. However, the gamma function is further defined for all complex numbers, $t$. It is worth nothing that there exists a related function, the pi function, $\Pi(t) = \Gamma(t + 1)$, that does away with the offset term, so as to coincide exactly with the factorial function. Another related function, the beta function, is a composite of the gamma function, where $B(x, y) = \Gamma(x)\Gamma(y)/\Gamma(x+y)$. The gamma function makes many appearances in the domain of statistics, for example in Student's t-distribution. There is also an intimate link to the Normal distribution, which we can show by first observing that the form of the function $\varphi(x) = e^{-x^2}$ is that of the bell curve. Now, the infinite integral\footnote{An integral taken over an infinite integral is known as \emph{improper} and is an abuse of notation to do away with limit notation.} of this function is,

$$\int_{-\infty}^{\infty} e^{-t^2} \mathop{dt} = 2\cdot\int_{0}^{\infty} e^{-t^2} \mathop{dt}.$$

Defining $t = u^{1/2}$, we have it that $\mathop{dt} = \frac{1}{2}u^{-1/2}\mathop{du}$ and integrating by substitution gives,

$$\int_{-\infty}^{\infty} e^{-t^2} \mathop{dt} = 2\cdot\frac{1}{2}\cdot\int_{0}^{\infty}u^{1/2 - 1} e^{-u}\mathop{du} = \Gamma(1/2).$$

Given that we know the area under the non-normalised bell curve to be $\sqrt{\pi}$, we may write, $\Gamma(1/2) = \sqrt{\pi}$. Though there exist many such identities for points on the gamma function, to evaluate the gamma function at any positive real point, the Stirling approximation was historically used as a good approximation. Its use has been supplanted in recent decades by the numerical algorithm, the Lanczos approximation. An interesting property of the gamma function is that it is the only function that interpolates the factorial function that is also log-convex. This is according to a result known as the Bohr-Mollerup theorem.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.6\textwidth]{Figures/factorial.pdf}
\caption{Comparison of the discrete factorial points, the gamma function, which interpolates them, and Stirling's approximation.\cite{factorial}}
\label{fig:factorial}
\end{figure}

\section{Fundamentals of Statistics}
\subsection{Philosophical Notions of Probability}

Pierre-Simon Laplace (1749-1827) described probability as, `nothing but common sense reduced to calculation.' As it is, there are two competing interpretations of the meaning of probabilities: the \emph{frequentist} interpretation, and the \emph{bayesian} interpretation. In the frequentist interpretation, probabilities represent frequencies or rates. Thus, a probability of $0.5$ that a coin comes up heads\footnote{Although recent research has shown the probability is slightly biased towards the upward-facing side of the coin when flipped, with about a $51\%$ chance of landing on that same side. This is linked somehow to the physics of the problem. See the Diaconis-Holmes-Montgomery coin-tossing theorem.} means that, with sufficient throws, this proportion of heads will prevail. In the bayesian (or epistemological) interpretation, probabilities quantify an uncertainty or `degree of belief' in an outcome, and so is more closely aligned with information theory. In information theory, a fair coin flip (50-50) has maximum entropy, because a uniform distribution maximises uncertainty. Though both views are fully compatible, adopting one or the other changes the emphasis slightly.

\subsection{Fundamental Rules}

\subsubsection{Probability Functions}

Probabilities express the uncertainty (or frequency) of a random variable, $X$, of assuming states (events) $A, B, C, \dots$, as a numeric value between 0 (impossible) and 1 (certain). A distribution may be defined by a function to allocate the probabilities. We write $p(X=A)$ or simply $p(A)$ for the probability $X$ assumes state $A$.

Discrete random variables take values in a finite or countably infinite set (such as the set of integers). The function that assigns each event a probability is called the probability mass function (pmf). The probabilities of events in a distribution must sum to 1. Thus,

$$\Bigg(\sum_{k = 1}^{K} p(X = k)\Bigg) = 1.$$

Continuous probability distributions express probabilities for random variables taking values in a continuum. For this reason, it no longer makes sense to model probabilities of the form $p(X = k)$. Such an event taken apart has an infinitesimal probability. Rather, we compute the probability of intervals, such as $p(X \leq k)$. The function that gives probabilities is the cumulative density function (cdf),

$$
P(X \leq a) = \int_{-\infty}^{a} p(x) \mathop{dx}.
$$

Should we wish to compute the probability of a closed interval, we take,

$$P(a \leq X \leq b) = P(X \leq b) - P(X \leq a).$$

The cdf is the integral of the \emph{probability density function} (pdf), written $p(x)$, a function that expresses \emph{densities}. A density is a quantity related to probability by the expression, $P(x \leq X \leq x+dx) \approx p(x)dx$, so that the value of $p(x)$ can be though of as the relative weight of probabilities measured on intervals local to $x$. In actual fact, the pdf is the derivative of the cdf with,

$$p(x) = \lim_{\Delta x \to 0} \frac{P(x + \Delta x) - P(x)}{\Delta x},$$

and for this reason, the pdf must be non-negative, and must integrate to $1$.

\subsubsection{Union of Events}

The probability of a union of events $A$ and $B$, that is, the probability that $A$ \emph{or} $B$ occurs is, $$p(A \lor B) = p(A) + p(B) - p(A \land B).$$ When $A$ and $B$ are \emph{mutually exclusive}, the joint probability, $p(A \land B) = 0$. Mutually exclusive events are events that cannot happen simultaneously, such as $A$, the event of rolling a 2 on a die, and $B$, the event of rolling a 3 on a die. An example of events that are \emph{not} mutually exclusive is $A$, the event we draw a king from a deck of cards, and $B$, the event we draw a diamond. The events can happen both separately and together. Measuring the probability of the union of events seems rarely to be of interest, however.

\subsubsection{Inclusion-Exclusion Principle}

The generalisation to the union of events formula is arrived at using the inclusion-exclusion principle, used for counting unions of sets. The case corresponding to the probability of the union of two events stated above is,

$$|A \cup B| = |A| + |B| - |A \cap B|,$$

for sets $A$ and $B$. If we include a third set, $C$, we acquire,

$$|A \cup B \cup C| = |A| + |B| + |C| - |A \cap B| - |A \cap C| - |B \cap C| + |A \cap B \cap C|,$$

These ideas are most easily understand by looking at Venn diagrams (Figure \ref{fig:inclusionexclusion}), where the probability spaces of the events are depicted as sets. In general, the probability of the union of $n$ events is,

$$p(X_1 \lor \dots \lor X_N) = \sum_{i = 1}^{N}p(X_i) - \sum_{1 \leq i < j \leq N}p(X_i \land X_j) + \sum_{1 \leq i < j < k \leq N}p(X_i \land X_i \land X_j) - \cdots + (-1)^{N-1}p(X_1 \land \dots \land X_N)$$

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{Figures/inclusionexclusion.png}
\caption{The inclusion-exclusion principle is best understood with Venn diagrams\cite{inclusionexclusion}.}
\label{fig:inclusionexclusion}
\end{figure}

\subsubsection{Intersection of Events}

The joint distribution, $p(A \land B)$, or simply, $p(A, B)$, is the probability that events $A$ and $B$ occur together. The joint distribution is factorised in accordance with the dependencies of the events. If two events, $A$ and $B$, are independent, we write $A \indep B$. Independence is distinct from mutual exclusivity, which is exclusive or. Independent events may occur apart or together, but the essence of independence is that the occurrence of one event does not affect the occurrence of the other. Hence, the joint distribution of independent $A$ and $B$ is,

$$p(A, B) = p(A)p(B),$$

that is, the product of the probabilities of the events taken separately. When $A$ and $B$ are \emph{dependent}, the occurrence of one changes the probability of the other occurring. In this case we write,

\begin{align}
p(A, B) &= p(A|B)p(B) \label{eq:condition1} \\
&= p(B|A)p(A). \label{eq:condition2}
\end{align}

The probability, $p(A|B)$, is the conditional probability of the event $A$ \emph{given} the occurrence of event $B$. Notice the symmetry of the conditioning--we can condition on any variable we like. The definition of independence is that $p(A|B) = p(A)$.

\subsection{Bayes' Theorem}

We may derive the all-important Bayes' theorem by equating equations (\ref{eq:condition1}) and (\ref{eq:condition2}) and rearranging,

$$p(A | B) = \frac{p(B | A)p(A)}{p(B)}.$$

Bayesians interpret this as an expression relating event $A$ and evidence $B$. The probability $p(A)$ is known as the `prior' distribution for the event taken alone (that is, `before' the evidence is presented). The `posterior' distribution, $p(A | B)$, represents the change in the uncertainty of the event after the evidence of the likelihood, $p(B | A)$, has been presented.

\subsubsection{Chain Rule}

Extending the intersection rule gives us a general result for $N$ random variables, $X_!, X_2, \dots, X_N$, known as the chain rule. The joint distribution may be factorised,

\begin{align}
p(X_1, X_2, X_3,\dots,X_N) &= p(X_1)p(X_2, X_3, \dots, X_N|X_1) \notag \\
&= p(X_1)p(X_2|X_1)P(X_3, \dots, X_N|X_1, X_2) \notag \\
&= p(X_1)p(X_2|X_1)P(X_3|X_1, X_2)\cdots P(X_N|X_1, X_2, \dots, X_{N-1}) \notag
\end{align}

\subsubsection{Law of Total Probability}

The law of total probability allows us to calculate the probability of an event by summing over a joint distribution,

\begin{align}
p(X=x) &= \sum_{y \in Y} p(X=x, Y=y) \notag \\
&= \sum_{y \in Y} p(X=x|Y=y)p(Y=y). \notag
\end{align}

This is called the marginal distribution on $X$, and we say that $Y$ has been `marginalised out'.

\subsubsection{Markov Chain}
A Markov chain is a sequence of random variables, $X_1, X_2, \dots, X_N$ written,

$$X_1 - X_2 - \cdots - X_N.$$

A Markov chain exhibits the Markov property, meaning each variable is dependent only on the previous variable in the sequence. Otherwise put, $p(X_{N+1}|X_N, X_{N-1}) = p(X_{N+1}|X_N)$. The chain rule applied to a Markov chain therefore simplifies to,

\begin{align}
p(X_1, X_2, X_3,\dots,X_N) &= p(X_1)p(X_2|X_1)P(X_3|X_1, X_2)\cdots P(X_N|X_1, X_2, \dots, X_{N-1}) \notag \\
 &= p(X_1)p(X_2|X_1)P(X_3|X_2)\cdots P(X_N|X_{N-1}) \notag
\end{align}

To give an example of what a Markov chain might model, let $X$ be the event that we see clouds on the horizon this morning; $Y$ it is raining by noon; $Z$ it is raining this evening. These events form a Markov chain, $X-Y-Z$. The probability of rain at noon, $Y$, is clearly dependent on there being clouds in the morning, $X$. That it rains in the evening, $Z$, also depends on there being clouds in the morning, $X$, but not once we have knowledge of $Y$. Thus, $Z$ is independent of $X$ given (conditioned on) the more recent and informative event $Y$.

\subsection{Quantiles}

Quantiles mark milestones in the cumulative distribution, that is, the values for which certain cumulative probabilities are reached. Quantiles may be taken anywhere, but the most common ones are:

\begin{itemize}
\item median - the value for which there is a 0.5 probability of exceeding and 0.5 probability of falling short, that is, the midpoint of the distribution;
\item quartile ($Q_1, Q_2, Q_3$) - the values marking 0.25, 0.5, and 0.75, probabilities in the cumulative distribution ($Q_2$ is the median);
\item percentile - the values marking the $0.01, 0.02, \dots, 0.99$ cumulative probabilities.
\end{itemize}

For a continuous distribution, any quartile, $q$, can be computed by solving the integral, for the desired probability,

$$
p = \int_{-\infty}^{q} p(x) \mathop{dx}.
$$


\subsection{Moments}

Moments are quantitative measures (statistics) of a distribution first formulated by English statistician Karl Pearson (1857-1936). The general form is,

$$
\mathbb{E}[(X - c)^N],
$$

for some constants, $c$ and $N$. When $c = 0$, we have what is called the \emph{raw} moment. Usually $c = \mathbb{E}[X]$, as we are chiefly interested in how $X$ moves about its mean, and these are called \emph{central} moments. Further, standardised moments are normally used in higher orders, presumably to moderate the effect of higher powers.

\subsubsection{Expected Value}

The expected value (or mean, or average), denoted $\mu$, of a discrete random variable is the average of the random variable, weighted by its probabilities. It is the first-order ($N=1$) \emph{raw} moment (the central moment is 0 by definition). It is defined as,

$$
\mathbb{E}[X] \triangleq \sum_{x} p(x) x.\\
$$

Similarly, for continuous, $X$,

$$
\mathbb{E}[X] \triangleq \int_{-\infty}^{\infty} p(x) x \mathop{dx}.\\
$$ 

In general, the expectation of a transformation, $f(X)$, is $\mathbb{E}[f(X)] = \sum_x p(x)f(x)$

\subsubsection{Variance}

Variance, $\text{Var}[X]$, or $\sigma^2$, is the expected (average) squared deviation from the mean, and therefore a measure of spread. Clearly, we need a second order measure to indicate spread since the first moment will be 0 for a symmetric distribution (the negative scores cancelling out the positive scores). Thus, variance,

\begin{align}
\text{Var}[X] &= \mathbb{E}[(X - \mathbb{E}[X])^2] \notag \\
&= \mathbb{E}[X^2 - 2\mathbb{E}[X]X + \mathbb{E}[X]^2] \notag \\
&= \mathbb{E}[X^2] - 2\mathbb{E}[X]^2 + \mathbb{E}[X]^2 \notag \\
&= \mathbb{E}[X^2] - \mathbb{E}[X]^2. \notag
\end{align}

Another measure of spread is the standard deviation, $\sigma$, defined as the square root of the variance, $\sqrt{\sigma^2}$.

\subsubsection{Skewness}

Skewenss, $\gamma_1$, is the third order \emph{standardised} moment. It is a measure of the skew or \emph{asymmetry} of a distribution, and is simply the third order central moment standardised,

$$
\gamma_1 = \mathbb{E}\Bigg[\frac{(X - \mathbb{E}[X])^3}{\sigma^3}\Bigg]
$$

\subsubsection{Kurtosis}

Kurtosis, $\text{Kurt}[X]$, is the fourth-order standardised central moment. It is a measure of the heaviness of the tails of the distribution. It is defined to be,

$$
\text{Kurt}[X] = \frac{\mathbb{E}[(X - \mathbb{E}[X])^4]}{\mathbb{E}[(X - \mathbb{E}[X])^2]^2}
$$

\subsection{Sample Statistics}

Given a set of sample data $x_1, x_2, \dots, x_N$, we can compute estimates of the distribution's statistics. Samples are drawn at random \emph{with} replacement. The sample mean is simply the arithmetic average,

$$\bar{x} = \frac{1}{N}\sum_{i=1}^N x_i.$$

At first it may appear that through the $\frac{1}{N}$ term that we are assuming a uniform distribution. This is not the case, however. Wherever non-uniformity exists, the data will `find' the right proportion as a matter of course, by supplying more likely scores in greater number. That is, the higher probabilities will feature more heavily, and receive more weight, effecting higher probabilities. The sample variance is a lot more curious. We might take the sample variance in a similar way to the mean,

$$\sigma_x^2 = \frac{1}{N}\sum_{i=1}^N (x_i - \bar{x})^2$$

This would be, surprisingly, only \emph{almost} correct. This is due to the fact we are using the sample mean only, not the true mean. Given all possible samples, the expectation of the sample variance is,

\begin{align}
\mathbb{E}[\sigma_x^2] &= \mathbb{E}\Bigg[\frac{1}{N}\sum_{i=1}^N\Bigg(x_i - \frac{1}{N}\sum_{i=1}^N x_i\Bigg)^2 \Bigg] \notag \\
&=\frac{1}{N}\sum_{i=1}^N\mathbb{E}\Bigg[x_i^2 - \frac{2}{N}x_i\sum_{j=1}^N x_j + \frac{1}{N^2}\sum_{j=1}^N x_j \sum_{k=1}^N x_k\Bigg] \notag \\
&=\frac{1}{N}\sum_{i=1}^N\mathbb{E}\Bigg[\frac{N-2}{N}\mathbb{E}[x_i^2] - \frac{2}{N}\sum_{j\neq i}^N \mathbb{E}[x_ix_j] + \frac{1}{N^2}\sum_{j=1}^N \sum_{k\neq j}^N \mathbb{E}[x_jx_k] + \frac{1}{N^2}\sum_{j=1}^N \mathbb{E}[x_j^2]\Bigg] \notag \\
&=\frac{1}{N}\sum_{i=1}^N\mathbb{E}\Bigg[\frac{N-2}{N}(\sigma^2 + \mu^2) - \frac{2(N - 1)}{N} \mu^2 + \frac{N(N-1)}{N^2} \mu^2 + \frac{1}{N} (\sigma^2 + \mu^2)\Bigg] \notag \\
&= \frac{N-1}{N}\sigma^2, \notag
\end{align}

where $\sigma^2$ and $\mu$ are the true variance and mean. Note the substitution in the second to last step comes from the definition of variance, $\text{Var}[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2$. Thus, our formula becomes,

$$\sigma_x^2 = \frac{1}{N-1}\sum_{i=1}^N (x_i - \bar{x})^2.$$

This is called the unbiased sample variance, and the leading quotient is known as Bessel's\footnote{Friedrich Bessel (1784-1846) was a German mathematician and scientist.} correction. This formula is usually presented without the above justification, and may be baffling to the student. The same correction is used for the sample covariance and other statistics. Given independent samples, these statistics converge to the true statistics as the sample size increases, as per the law of large numbers. That is, the sample variance converges to the expected sample variance, and the Bessel's correction converges to 1.

\subsection{Covariance}

Covariance measures the interaction between random variables, that is, it quantifies how much two random variables move together. It is defined as,

\begin{align}
\text{Cov}[X, Y] &= \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] \notag \\
&= \mathbb{E}[X^2 - \mathbb{E}[Y]X - \mathbb{E}[X]Y + \mathbb{E}[X]\mathbb{E}[Y]] \notag \\
&= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]. \notag
\end{align}

In the multivariate case, we have the covariance matrix,

\begin{align}
\text{Cov}[\mathbf{x}] &\triangleq \mathbb{E}[(\mathbf{x} - \mathbb{E}[\mathbf{x}])^T(\mathbf{x} - \mathbb{E}[\mathbf{x}])] \notag \\
&= \begin{bmatrix}
\text{Var}[X_1] & \text{Cov}[X_1, X_2] & \dots & \text{Cov}[X_1, X_n] \\
\text{Cov}[X_2, X_1] & \text{Var}[X_2] & \dots & \text{Cov}[X_2, X_n] \\
\vdots & \vdots & \ddots & \vdots \\
\text{Cov}[X_n, X_1] & \text{Cov}[X_n, X_2] &\dots & \text{Var}[X_n]
\end{bmatrix} \notag
\end{align}

\subsection{Correlation}

Correlation is covariance normalised by the variance of each of the variables taken apart. It measures the extent to which two variables are \emph{linearly} related and computes a correlation coefficient in the range $[-1, 1]$. A correlation coefficient of 0 indicates no relation. The formula for correlation is,

$$\text{corr}(X, Y) = \frac{\text{Cov}[X, Y]}{\sqrt{\text{Var}[X]\text{Var}[Y]}}.$$

Similary, for the multivariate case, the correlation matrix is,

$$
\text{Corr}[\mathbf{x}] = \begin{bmatrix}
\text{Corr}[X_1, X_1] & \text{Corr}[X_1, X_2] & \dots & \text{Corr}[X_1, X_n] \\
\text{Corr}[X_2, X_1] & \text{Corr}[X_2, X_2] & \dots & \text{Corr}[X_2, X_n] \\
\vdots & \vdots & \ddots & \vdots \\
\text{Corr}[X_n, X_1] & \text{Corr}[X_n, X_2] &\dots & \text{Corr}[X_n, X_n]
\end{bmatrix}. \notag
$$

The diagonal elements of this matrix will of course be 1.

\subsection{Transformation of Random Variables}

For random variable, $X$, define a new random variable, $Y = aX + b$. How have its properties changed? Recall for discrete variables that $\mathbb{E}[f(X)] = \sum_x p(x)f(x)$. Then we have,

\begin{align}
\mathbb{E}[Y] &= \sum_x p(x)(ax + b) \notag \\
&= a\sum_x p(x)x + b\sum_x 1 = a\mathbb{E}[X] + b. \notag
\end{align}

The same result is arrived at for continuous random variables. This effect is known as the `linearity of expectation'. Variance behaves a little differently,

\begin{align}
\text{Var}[Y] &= \mathbb{E}[(Y - \mathbb{E}[Y])^2] \notag \\
&= \mathbb{E}[(aX + b - (a\mathbb{E}[X] + b))^2] \notag \\
&= \mathbb{E}[a^2(X  - \mathbb{E}[X])^2] \notag \\
&= a^2\text{Var}[X] \notag
\end{align}

Thus, adding a constant to a random variable does not change its variance, but scaling a random variable scales its variance by the square of that factor.

\subsection{Change of Variables}

In the general case, given transformation, $Y = f(X)$, a discrete distribution can be derived by,

$$p_y(y) = \sum_{x:f(x)=y} p_x(x),$$

that is, by mapping the probabilities of the values of x corresponding to y via the mapping. Things are not so simple for continuous random variables, in which case we must address the cdf. Being a cumulative function, it is monotonic\footnote{A monotonic function holds the property that $f(b) > f(a)$ for any $b > a$.}. If it is monotonic, it is bijective\footnote{If $f$ is bijective, it is a one-to-one mapping.}, and if it is bijective, it is invertible. Therefore, we can write the cumulative distribution,

$$P_y(y) = P(f(X) \leq y) = P(X \leq f^{-1}(y) = P_x(f^{-1}(y))$$

Now,

$$
p_y(y) \triangleq \frac{d}{dy}P_y(y) = \frac{d}{dy}P_x(f^{-1}(y)) = \frac{dx}{dy}\frac{d}{dx}P_x(x),
$$

giving us the \emph{change of variables} formula,

$$p_y(y) = p_x(x)\Big|\frac{dx}{dy}\Big|.$$

An example here would perhaps be useful, so let $X \sim U(-1, 1)$ and $Y = X^2$. Since X is uniformly distributed on an interval of length 2, its pdf, $p_x(x) = 1/2$. Therefore, by the change of variables formula, $p_y(y) = \frac{1}{2} \cdot \frac{1}{2}y^{1/2} = \frac{1}{4}y^{1/2}$.

In the multivariate case, we have need of the Jacobian matrix from vector calculus, and the change of variables formula becomes,

% that is, the matrix of partial derivatives\footnote{Not to be confused with the Hessian, which is the matrix of partial \emph{second} derivatives.}, defined as,
%
%$$\mathbf{J}_{\mathbf{x} \to \mathbf{y}} \triangleq \frac{\partial(y_1, \dots, y_n)}{\partial(x_1, \dots, x_n)} \triangleq 
%\begin{bmatrix}
%\frac{\partial y_1}{\partial x_1} & \dots & \frac{\partial y_1}{\partial x_n} \\
%\vdots & \ddots & \vdots \\
%\frac{\partial y_n}{\partial x_1} & \dots & \frac{\partial y_n}{\partial x_n} \\
%\end{bmatrix},
%$$
%
%and the change of variables formula becomes,

$$p_y(\mathbf{y}) = p_x(\mathbf{x})\big|\det \Bigg(\frac{\partial\mathbf{x}}{\partial\mathbf{y}}\Bigg)\big| = p_x(\mathbf{x})|\det \mathbf{J}_{\mathbf{y} \to \mathbf{x}}|.$$

\subsection{Monte Carlo Statistics}

Monte Carlo statistics are a branch of numerical techniques for providing estimations using random sampling. The name comes from the famous casino in the city in the principality of Monaco near the south of France. One famous example of applying Monte Carlo techniques is that of sampling from a unit circle inscribed inside a $2 \times 2$ unit square. The ratio of the areas of circle to square is $\pi : 4$. We can therefore estimate the value of $\pi$ by randomly sampling points within the square, and comparing the number of points that fall inside and outside the circle (Figure \ref{fig:montecarlo}). A sufficiently large sample would converge to the correct result by the law of large numbers.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{Figures/montecarlo.pdf}
\caption{Monte Carlo approach to estimating $\pi$. 1963 of the 2500 random points landed in the circle, giving an estimation of $\pi \approx 3.1408$. Generated with \texttt{monteCarloDemo.m}.}
\label{fig:montecarlo}
\end{figure}

\subsubsection{Buffon's Needles}
An extremely nice and demonstrative Monte Carlo problem is know as the Buffon's needle problem. It also derives a technique for numerically computing the digits of $\pi$ using probabilities, but without invoking any circles. The problem consists of a set of parallel lines, separated by a distance of $2L$, where L is the length of each of the needles. The needles are then scattered randomly between the parallel lines. The proportion of needles intersecting a line gives us an approximation of $\pi$! There are two random variables at play here: the distance, $x$, of a needle from the nearest line; and the orientation, $\theta$, of the needle with respect to the lines. The assumption we make is that these are uniformly randomly distributed, hence $x$, which varies between 0 and $L$, has probability function $p_x = 1/L$, and $\theta$, which varies between $0$, $p_{\theta} = 2/\pi$. To determine the probability of a needle crossing a line, we note that it is sufficient that $x < \frac{L}{2}\sin\theta$. This is most easily seen with a diagram. These conditions alone are enough to calculate the probability,

$$
p = \int_{0}^{\pi/2}\int_{0}^{\frac{L}{2}\sin\theta}p_xp_{\theta}\mathop{dx}\mathop{d\theta} = \frac{2}{L\pi}\int_{0}^{\pi/2}\frac{L}{2}\sin\theta\mathop{d\theta} = \frac{1}{\pi}.
$$

Therefore, the proportion of a sample of $n$ scattered needles crossing a line gives, $\frac{\text{\# crosses}}{n} \approx \frac{1}{\pi} \implies \pi \approx \frac{n}{\text{\# crosses}}$

\section{Common Discrete Probability Distributions}

\subsection{Uniform Distribution}

A uniform distribution models a coin toss, or a die roll, or any other probability of $K$ events with equal chance, defining an interval, $[a, b]$. The probability mass function for a uniform distribution is,

$$p(k) = \frac{1}{K}.$$

The mean of this distribution is, $\mu = \frac{a + b}{2}$. Note there is also a continuous version of the uniform distribution. Laplace's \emph{principle of insufficient reason} argues to assume a uniform distribution for a discrete variable in the absence of further information (Gaussian for continuous variables).

\subsection{Bernoulli Distribution}

A Bernoulli distribution models an event with two alternative outcomes. It may therefore model a coin toss, but is parameterised with probability $\theta$ of outcome 1, and $1 - \theta$ of outcome 2. Thus, any binary event may be modelled. We write $X \sim \text{Ber}(\theta)$. Thus,

$$\text{Ber}(x|\theta) = \theta^{1_{x=1}}(1 - \theta)^{1_{x=0}}.$$

The expected value is therefore,

$$\mathbb{E}[X] = \theta \cdot 1 + (1 - \theta) \cdot 0 = \theta$$

\subsection{Binomial Distribution}

A Binomial distribution may model the number of heads, $k$, from the flipping of a coin (or any other binary event) $n$ times. The Binomial distribution generalises the Bernoulli distribution to $n$ binary trials (rather than a single trial). Its pmf is,

$$\text{Bin}(k | n,\theta) \triangleq {{n}\choose{k}}\theta^{k}(1 - \theta)^{n-k}$$

Note the resemblance in form to the Bernoulli distribution. The multinomial distribution further generalises the binomial distribution to $k$-ary events $n$ times.

\section{Common Continuous Probability Distributions}

\subsection{Laplace Distribution}

The Laplace pdf resembles two opposing exponential functions that meet in the middle, and is written,

$$f(x ; \mu, b) = \frac{1}{2b}\exp\Bigg\{-\frac{|x - \mu|}{b}\Bigg\},$$

where $b$ is a normalising constant. This distribution is similar in form to the Gaussian distribution, but the exponent is linear, and so the log probability decreases linearly, whereas a Gaussian's `tails' decrease quadratically. Thus, we say the Laplace distribution has `heavy' tails. This has implications when we fit a distribution to data (such as in machine learning), where the Laplace distribution is less sensitive to outliers, as extreme events have higher probability in a Laplace distribution than in a Gaussian. One advantage is it is easily integrable, whereas the Gaussian has no closed-form solution, as we shall see. Integrating the pdf gives,

\begin{align}
F(x) &= \int_{-\infty}^{+\infty} \frac{1}{2b}\exp\Bigg\{-\frac{|x - \mu|}{b}\Bigg\}dx \notag \\
&= \int_{-\infty}^{\mu} \frac{1}{2b}\exp\Bigg\{\frac{x - \mu}{b}\Bigg\}\mathop{dx} + \int_{\mu}^{+\infty} \frac{1}{2b}\exp\Bigg\{\frac{\mu - x}{b}\Bigg\}\mathop{dx} \notag \\
&= \frac{1}{2}\Bigg[\exp\Bigg\{\frac{x - \mu}{b}\Bigg\}\Bigg]_{x=-\infty}^{x=\mu} + \frac{1}{2}\Bigg[\exp\Bigg\{\frac{\mu - x}{b}\Bigg\}\Bigg]_{x=\mu}^{x=+\infty} \notag \\
&= \frac{1}{2}(1 - -0) + \frac{1}{2}(0 - -1) \notag
= 1, \notag
\end{align}

which confirms its validity as a probability distribution.

\subsection{Gaussian Distribution}

The Gaussian or Normal distribution was first proposed by Karl Frederich Gauss (1777-1855) in 1809. In the same paper, he presented other fundamental tenets of statistics--the method of least squares (for fitting data with Gaussian error), and maximum likelihood estimation. The pdf of a Gaussian distribution is,

$$f(x;\mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}}\exp\Bigg\{\frac{-(x - \mu)^2}{2\sigma^2}\Bigg\}.$$

Its shape is the well-known bell curve, the distribution so often seen in real-world data (the profound central limit theorem offers an explanation for why that is). The cdf for a Gaussian distribution,

$$\Phi({z;\mu,\sigma^2}) = \int_{-\infty}^{x}\mathcal{N}(z | \mu, \sigma^2)\mathop{dz}. \notag$$

Defining $u = \frac{z - u}{\sqrt{2}\sigma}$, and so $du = dz/\sqrt{2}\sigma$, we can integrate by substitution,

\begin{align}
\Phi({z;\mu,\sigma^2}) &= \frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\frac{z - \mu}{\sqrt{2}\sigma}} e^{-u^2} \sqrt{2}\sigma\mathop{du} \notag \\
&= \frac{1}{2}\Big[\text{erf}(u)\Big]_{u=-\infty}^{u=(z-\mu)/\sqrt{2}\sigma} \notag \\
&= \frac{1}{2}\Bigg(1 + \text{erf}\Bigg(\frac{z - u}{\sqrt{2}\sigma}\Bigg)\Bigg), \notag
\end{align}

where $\text{erf}(x) = \frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-t^2}\mathop{dt}$ is the error function. This is a dead-end for closed-form solutions, and we must have recourse to an infinite series. Substituting the Taylor expansion for $e^{-t^2}$,

\begin{align}
\text{erf}(x) &= \frac{2}{\sqrt{\pi}}\int_{0}^{x} \sum_{k=0}^{\infty} \frac{(-t^2)^k}{k!} \mathop{dt} \notag \\
%&= \frac{2}{\sqrt{\pi}}\int_{0}^{x} \sum_{k=0}^{\infty} \frac{(-1)^kt^{2k}}{k!} dt \notag \\
&= \frac{2}{\sqrt{\pi}}\sum_{k=0}^{\infty} \frac{(-1)^kx^{2k + 1}}{k!(2k + 1)}, \notag
\end{align}

giving us an infinite series for the error function. Thus, approximations to the Gaussian CDF can be computed by statistical software, and placed in those mysterious statistics tables with which every student of statistics is familiar.

\subsubsection{Standardisation}

The standard Normal distribution is the Normal distribution with mean $0$ and variance $1$, $\mathcal{N}(0, 1)$. It is possible to \emph{standardise} a random variable, $X \sim \mathcal{N}(\mu, \sigma^2)$, by first subtracting its mean, $\mu$ (which adjusts the mean to 0 without changing the variance), then dividing by its standard deviation, $\sigma$, (which does not change the mean but shrinks the variance from $\sigma^2$ to 1), yielding,

$$Z = \frac{X - \mu}{\sigma}.$$

Since these transformations do not change the type of distribution, and a Gaussian is uniquely defined by its mean and variance, the transformed random variable, $Z$, is distributed according to the standard normal distribution.

\subsection{Chi-squared Distribution}

The chi-squared ($\chi^2$) distribution expresses a probability distribution over the sums of squares of $k$ normally distributed random variables. That is, the distribution of $Y$ where $Y = X_1^2 + X_2^2 + \dots + X_k^2$ for $k$, and where the $X_i \sim \mathcal{N}(0, 1)$ are i.i.d\footnote{independent and identically distributed} random variables. Note that by the central limit theorem, the chi-squared distribution converges to a normal distribution as $k \to \infty$. We write $Y \sim \chi^2_k$ where $k$ is the number of variables in the sum, also known as the \emph{degrees of freedom}. When the random variable $Y$ takes on a value, that value is like the dial of radius of a $k$-dimensional sphere, where the $k$ random variables are free to vary. The chi-squared distribution is the basis of some common statistical tests, and is also crucial to the derivation of other distributions, such as Student's t-distribution. The pdf of a chi-squared distribution is,

$$p(y; k) = \frac{1}{2^{\frac{k}{2}}\Gamma\big(\frac{k}{2}\big)}y^{\frac{k}{2} - 1}e^{-\frac{y}{2}},$$

where the parameter, $k$, is the number of degrees of freedom (number of variables in the sum), and $\Gamma$ is the gamma function. In the unique case that $k = 2$, the expression simplifies to $p(y; 2) = \frac{1}{2}e^{-y/2}$. We take the time to derive the pdf of the chi-squared distribution, as it involves several of the topics we have covered already.

In the simplest case, $k = 1$, we have $Y = X^2$. Since $X$ is a standardised Gaussian variable, its square makes all previously negative values positive. The shape of the pdf is hence something like a stretched half bell curve. We can derive the pdf of the transformed variable using the change of variables formula. Because of the symmetry of the squared variable, we have, $f_Y(y) = 2f_X(f^{-1}(y))\Big|\Big(\frac{\mathop{dx}}{\mathop{dy}}\Big)\Big|$. Now, $X = \sqrt{Y}$, so $dx/dy = 1/2y^{-1/2} $. Hence,

$$f(y; 1) = 2\frac{1}{\sqrt{2\pi}}e^{-(\sqrt{y})^2/2}\frac{1}{2}y^{-1/2} = \frac{1}{\sqrt{2}\Gamma(1/2)}e^{-y/2}y^{-1/2},$$

where we recall that $\Gamma(1/2) = \sqrt{\pi}$. The $k$th order case may be derived with a more involved version of the same technique. In the general case, it becomes clear that the variable $Y$, is directly related to a k-dimensional sphere, with surface area, $S$, given by,

$$S = \frac{2\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}.$$

This is how the gamma function comes to be part of the function.

\subsection{Student's T-Distribution}

The Student's t-distribution has an interesting history. Student is a pseudonym for the English mathematician who designed it, William Sealy Gosset (1876-1937), unable to attribute his true name due to his employment as a statistician at the Guiness brewery in Dublin. According to Gosset, the t-distribution expresses a distribution of the value of the sample mean of a Gaussian variable given the true standard deviation of the samples is unknown. The probability density function is,

$$f(t) = \frac{\Gamma(\frac{\nu + 1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}\Bigg(1 + \frac{t^2}{\nu}\Bigg)^{-\frac{\nu + 1}{2}},$$

where $\Gamma(v)$ is the gamma function, and $\nu$, the Greek letter \emph{nu}, are the degrees of freedom. The variable, $t = \frac{\hat{x} - \mu}{s/\sqrt{n}}$, where $\hat{x} = (x_1 + \cdots + x_n)/n$ is the sample mean, $\mu$ is the true mean, $s^2 = \frac{1}{N - 1}\sum_{i=1}^{n} (x_i - \hat{x})^2$ is the sample variance, and $n$ is the sample size. The degrees of freedom is therefore $\nu = n - 1$. Thus, the Student's t-distribution distributes $t$, the sample mean.

\section{Hypothesis Testing}

\subsection{Standard Error}

Standard error is the standard deviation of the sample mean, $\bar{X} = \frac{1}{n}(X_1 + X_2 + \cdots X_n)$ from $n$ independent samples, $X_1, X_2, \dots, X_n$, where the true standard deviation $\sigma$ is known. The total, $$T = X_1 + X_2 + \dots + X_n,$$ has variance, $\sigma_T^2 = n\sigma^2$. Then, the variance of $\bar{X} = T/n$ is therefore, $$\sigma_{\bar{X}}^2 = \frac{1}{n^2}n\sigma^2 = \sigma^2/n.$$ We write finally the standard error as, $$\text{SE}_{\bar{X}} = \sigma/\sqrt{n}.$$ Clearly as $n \to \infty$, the standard error goes to $0$, reflecting that the sample mean converges to the true mean. Note that in practice $\sigma$ must be approximated by the sample variance.

\subsection{Confidence Intervals}

A confidence interval is the interval, expressed in standard deviations, around a sample mean, $\bar{X}$, known to contain the true mean, $\mu$, to a desired degree of certainty. Because of the central limit theorem, we know the sum of n random variables is normally distributed, hence the sample mean also. The standard error statistic tells us the standard deviation of this distribution. Because the Normal cdf has no closed form, it must be computed numerically. In practice, it is necessary to have a lookup table of precomputed probabilities from a standard normal distribution. The value of our variable can then be normalised and the corresponding probability retrieved. In a standard Normal distribution, $Pr(Z \leq 1.96) = 0.9775$ (approximately), and due to symmetry, $Pr(Z \geq - 1.96) = 0.9775$. That is, $95\%$ of the distribution sits in the interval $[-1.96, 1.96]$. In other words, $Z$ has a 95\% chance of being within 1.96 standard deviations ($\sigma = 1$ when standardised) of the mean. Thus, we may solve for our own $95\%$ confidence interval by writing,

$$1.96 = \frac{\bar{X} - \mu}{\sigma}.$$

Rearranging gives,

$$\mu = [\bar{X} - 1.96\times\text{SE}, \bar{X} + 1.96\times\text{SE}],$$

which is the general form for a $95\%$ confidence interval. Confidence intervals of arbitrary size can be constructed by looking up the normalised value of the desired probability.

\subsection{Hypothesis Testing}

We can use our knowledge of statistics to perform tests on sample data. The formal framework for this is to propose two hypotheses, one the null (or default) hypothesis, $H_0$, the other the alternative hypothesis, $H_1$. The procedure is to compute a statistic to challenge the statistical implications of the null hypothesis, producing a probability that the sample data is consistent with $H_0$. If the probability is lower than some conventional threshold (usually 0.05 or 0.1), $H_0$ is rejected and $H_1$ accepted, in a sort of probabilistic proof by contradiction. Erroneously rejecting $H_0$ is referred to as a type 1 error (false positive); erroneously accepting $H_0$ is referred to as a type 2 error (false negative).

\subsubsection{Z-test}

Many statistical tests exist, and perhaps the simplest is the $Z$-test, which can be used to decide whether a set of n samples conforms to a given distribution, by calculating the probability of the sample mean. The calculations are similar to creating a confidence interval, as it involves computing the SE and the Z-score (implying the true mean and standard deviation be known), $Z = (\hat{X} - \mu)/\text{SE}$. The value of $Z$ is looked up in standard normal tables, yielding a probability. The probability (or p-value) is compared with a conventional threshold (typically 0.05 or 0.1) and the null hypothesis accepted or rejected depending on where it falls. We may interpret the Z-score as asking the question, `can this data reasonably be believed to have been drawn from our distribution?'

\subsubsection{Pearson's Chi-Squared Test}

This test checks for bias in categorical variables. The null hypothesis is that the $n$ categories of a variable are uniformly distributed. If, for example, we have a die producing a set of sample data from $N$ throws, we can compute the difference between the observed total of each category, $O_i$, $i = 1, \dots, 6$, and the expected total for a fair die, $E_i = \frac{1}{6}\times N$. This difference is assumed to be a continuous, normally distributed random variable. The normalised squared error for each variable is $(O_i - E_i)^2/E_i$. We can therefore sum these squared errors and use a chi-squared distribution with $k = 5$ degrees of freedom\footnote{In such problems, the degrees of freedom is $n - 1$, where $n$ is the number of categories. This is due to the fact that once the first $n-1$ variables are given, the final one is determined because the sum is constant.} to give us a probability for the null hypothesis.

\section{The Central Limit Theorem}
\subsection{Moment-Generating Functions}

The moment-generating function (MGF) of a random variable, $X$, is defined as,

\begin{align}
M_X(t) = \mathbb{E}[e^{tX}] &= \mathbb{E}\Bigg[\sum_{k = 0}^{\infty} \frac{(tX)^k}{k!}\Bigg] \notag \\
&= \sum_{k = 0}^{\infty} \frac{\mathbb{E}[X^k]t^k}{k!}. \notag
\end{align}

Such a function is an alternative specification of a distribution to a pdf. By L\'evy's continuity theorem, convergence in MGF implies convergence in distribution, a result that underpins the proof of the central limit theorem. The function is `moment-generating' in that the coefficients of the series expansion are the central moments of the distribution. The MGF of a sum of N i.i.d variables, $T = X_1 + X_2 + \cdots + X_N$ is,

\begin{align}
M_T(t) = \mathbb{E}[e^{tX_1 + tX_2 + \cdots + tX_N}] &= \mathbb{E}[e^{tX_1}]\mathbb{E} [e^{tX_2}] \cdots\mathbb{E}[e^{tX_N}] \notag \\
&= M_X(t)^N. \notag
\end{align}

The MGF of a Gaussian distribution\footnote{Here standardised for simplicity.} is,

$$M_z(t) = \mathbb{E}[e^{tX}] = \int_{-\infty}^{+\infty}e^{zt}\frac{1}{\sqrt{2\pi}}e^{-z^2/2}\mathop{dz}.$$

If we consider the exponents, we have $e^{zt - z^2/2}$. Completing the square gives $e^{-z^2/2 + zt - t^2/2 + t^2/2} = e^{(z-t)^2/2}\cdot e^{t^2/2}$. Therefore we have,

\begin{align}
M_z(t) &= e^{t^2/2}\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{(z-t)^2/2}\mathop{dz} \notag \\
&= e^{t^2/2} \cdot 1, \notag
\end{align}

since the integrated expression defines $\mathcal{N}(z | t, 1)$. 

\subsection{Proof of the Central Limit Theorem}

The central limit theorem (CLT) is the crowning jewel of statistics--a profound result, with far-reaching applications. It explains, at least in part, the uncanny ubiquitousness of the bell curve in nature. Its proof is therefore the demonstration of a universal truth. This is where Laplace's statement that `probability is common sense reduced to calculation' resonates most strongly. The theorem states that the sum of $n$ random variables, whatever their individual distributions, converges to a Gaussian distribution as $n \to \infty$. If we then suppose natural phenomena to be aggregations of many random events, the central limit theorem explains how the bell curve appears with such regularity.

Given $X_1, X_2, ... X_n$ are i.i.d. random variables with mean 0, variance $\sigma^2$, and moment-generating function, $M_x(t)$, denote their standardised sum, $Z = (X_1 + X_2 + \cdots X_n)/\sqrt{n\sigma^2}$. Therefore,

$$M_Z(t) = \Bigg(M_x\Bigg(\frac{t}{\sqrt{n\sigma^2}}\Bigg)\Bigg)^n.$$

A Taylor approximation for $M_x$ is,

$$M_x(s) = M_x(0) + sM_x(0) + \frac{1}{2}s^2M_x''(0) + o(s^2),$$

where $o(s^2)$ indicates a function that shrinks faster than a quadratic function as $s \to 0$. By definition, $M_x(0) = 0$, $M_x'(0) = 1$, and $M_x''(0) = \sigma^2$. Combining these results gives,

\begin{align}
M_Z(t) &= \Bigg(1 + \frac{t^2/2}{n} + o\Bigg(\frac{t^2}{n\sigma^2}\Bigg)\Bigg)^n \notag \\
&\to e^{t^2/2}, \ n \to \infty, \notag
\end{align}

that is, the MGF of a Normal distribution. Thus, the MGF, $M_Z(t)$, converges to the MGF of a Normal distribution. Hence, by the L\'evy continuity theorem, it converges in distribution to a Gaussian.

\section{The Law of Large Numbers}

In this section we prove one of the fundamental laws of statistics--the law of large numbers--first deriving the Markov and Chebyshev inequalities that are used in the proof.

\subsection{Markov Inequality}

The Markov inequality expresses a general property of probability distributions. An elegant proof exists, but it may be derived simply by the following observation: it is not possible for $(1/n)$th of a population to be greater than $n$ times the average value. Expressed mathematically this is,

$$\text{Pr}(X \geq a) \leq \frac{\mathbb{E}[X]}{a}$$

\subsection{Chebyshev Inequality}

The Chebyshev inequality is another general result for probabilities, expressing a bound on the probability of a random variable, $X$, straying from its mean, $\mu$, by more than $k$ standard deviations. To derive this, we first define a random variable, $Y = (X - \mu)^2$, and constant, $a = (k\sigma)^2$. Then, by the Markov inequality,

$$\text{Pr}((X - \mu)^2 \geq (k\sigma)^2) \leq \frac{\mathbb{E}[(X - \mu)^2]}{(k\sigma)^2},$$

which we may rewrite as,

\begin{align}
\text{Pr}(|X - \mu| \geq k\sigma) &\leq \frac{\text{Var}(X)}{k^2\sigma^2} \notag \\
&= \frac{1}{k^2} \notag
\end{align}

\subsection{Law of Large Numbers}

The law of large numbers states that given a sequence $X_1, X_2, \dots, X_N$ of i.i.d random variables, the sample average, $\bar{X} = \frac{1}{n}(X_1 + X_2 + \cdots + X_n)$ converges to the true mean, $\mu$, as n grows. That is, $\bar{X}_n \rightarrow \mu$ as $n \rightarrow \infty$.

Note first that,

\begin{align}
\text{Var}(\bar{X}_n) &= \text{Var}\bigg(\frac{X_1 + X_2 + \cdots + X_n}{n}\bigg) \notag \\
&= \frac{1}{n^2}\big(\text{Var}(X_1) + \text{Var}(X_2) + \cdots + \text{Var}(X_n)\big) \notag \\
%&= \frac{n\sigma^2}{n^2} \notag \\
&= \frac{\sigma^2}{n} \notag
\end{align}

Now, by the Chebyshev inequality,

$$
\text{Pr}(|\bar{X}_n - \mu| \geq k\frac{\sigma}{\sqrt{n}}) \leq \frac{1}{k^2}.
$$

Choosing $k = \frac{\sqrt{n}}{\sigma}\epsilon$ for any arbitrary choice of $\epsilon$,

$$
\text{Pr}(|\bar{X}_n - \mu| \geq \epsilon) \leq \frac{\sigma^2}{n\epsilon^2},
$$

that is,

$$
\text{Pr}(|\bar{X}_n - \mu| < \epsilon) \geq 1 - \frac{\sigma^2}{n\epsilon^2},
$$

which converges to 1 for a sufficiently large choice of $n$.

\section{Information Theory}

This section presents the basics of information theory.

\subsection{Entropy}

The entropy of a random variable, $X$, is a measure of its uncertainty. It is a core tenet of information theory, the science underpinning coding and signal processing. Entropy measures the average number of bits required to encode an alphabet, and is the lower bound on compression (coding). Entropy is defined as,

$$\mathcal{H}(X) = \sum_{x \in X}p(x)\log\frac{1}{p(x)}.$$

Entropy clearly must be positive, and is minimised for a distribution where a single event has probability 1, that is, a deterministic variable. In this case, the entropy of the variable is 0, that is, minimally uncertain. Hence, $\mathcal{H}(X) \geq 0$. When we have binary events, $x \in \{0, 1\}$, we have the binary entropy function, which simplifies to,

$$h_2(p) = -p\log p - (1-p)\log (1 - p).$$

\subsection{Kullback-Liebler Divergence}

Kullback-Liebler (KL) divergence is a measure of the disparity between two probability distributions, $p$ and $q$, written,

\begin{align}
\mathcal{D}(p || q) &= \sum_{k=1}^K p_k \log\frac{p_k}{q_k} \notag
\end{align}

KL divergence can therefore be interpreted as the average number of \emph{additional} bits required to encode an alphabet with chosen distribution $q$, given true distribution $p$. Like entropy, divergence must be greater than or equal to zero. To see this, write,

\begin{align}
-\mathcal{D}(p || q) &= -\sum_{k=1}^K p_k \log\frac{p_k}{q_k} \notag \\
&= \sum_{k=1}^K p_k \log\frac{q_k}{p_k} \notag \\
&\leq \log\Bigg(\sum_{k=1}^K p_k\frac{q_k}{p_k}\Bigg) \label{eq:jensen} \\
&= \log 1 = 0, \notag
\end{align}

where the inequality in (\ref{eq:jensen}) comes from applying Jensen's inequality, since $\log$ is a concave function. A corollary to this is that the distribution maximising entropy for discrete variables is the uniform distribution, since,

\begin{align}
0 \leq \mathcal{D}(p || u) &= \sum_{k=1}^K p_k \log\frac{p_k}{u_k} \notag \\
&= \sum_{k=1}^K p_k \log{p_k} - \sum_{k=1}^K p_k \log \frac{1}{K} \notag \\
&= -\mathcal{H}(p) + \log K \notag
\end{align}

\subsection{Mutual Information}

Mutual information uses the KL divergence to measure the difference in the entropy of a random variable, $X$, before and after a second variable, $Y$, (on which $X$ may be dependent) is introduced. The formula for mutual information is,

\begin{align}
I(X; Y) &\triangleq \mathcal{D}(p(X, Y) || p(X)p(Y)) = \sum_x \sum_y p(x, y) \log\frac{p(x, y)}{p(x)p(y)}\notag \\
&= \mathcal{H}(Y) - \mathcal{H}(Y|X) \notag \\
&= \mathcal{H}(X) - \mathcal{H}(X|Y) \notag,
\end{align}

and so it quantifies how much knowing about one variable tells us about the other (note the formula is symmetric). That is, how much uncertainty is lifted from $X$ by learning about $Y$, and vice versa. In other words, it measures \emph{the extent to which one random variable becomes deterministic once we learn about another}. For this reason, mutual information can be used as a more sophisticated correlation measure, as it may reveal non-linear dependencies also.

\section{Convex Optimisation}
\subsection{Convexity}
Convexity is a useful property in optimisation because it guarantees that any local optimum of a function is a \emph{global} optimum.

\subsubsection{Convex Sets}
A set of points, $S$, in a vector space is convex if for any two points $x_1$ and $x_2$ in $S$, all points in-between, that is, all points on the straight line connecting $x_1$ and $x_2$ are also in $S$. In a sense, it is a region that does not have any `holes'. For example, a circular region is convex, but a doughnut is not, nor is a crescent. A line passing through points $x_1$ and $x_2$ can be written, $x_1 + c(x_2 - x_1)$, for some constant $c$. Formally, a set $S$ is convex if,

$$S = \Big\{x_1, x_2 \in S \implies (1 - \lambda) x_1 + \lambda x_2 \in S \Big\},$$

for all $\lambda \in [0, 1]$, that is, all points along the chord between the two points are also part of the convex set. A convex hull of a set of points is the smallest convex region enclosing those points.

\subsubsection{Convex Functions}
A function, $f(x)$, is convex on an interval if,

$$
f((1 - \lambda)x_1 + \lambda x_2) \leq (1 - \lambda) f(x_1) + \lambda f(x_2),
$$

for all $\lambda \in [0, 1]$. That is, the curve lies below a line drawn $x_1$ and $x_2$. A function with more than one turning point in a domain cannot be convex. Examples of convex functions are $e^x$, $x^2$, etc. \emph{Strict} convexity occurs when there is a strict inequality. \emph{Concavity} is the same property with the inequality reversed. Hence, $f(x)$ convex $\Longleftrightarrow$ $-f(x)$ concave. A linear function is both concave and convex.

\subsubsection{Jensen's Inequality}

Jensen's inequality essentially extrapolates the definition of convexity to $n$ dimensions, stating,

$$
f\Bigg(\sum_{i=1}^{N}\lambda_i \mathbf{x}_i\Bigg) = \sum_{i=1}^{N} \lambda_if(\mathbf{x}_i) \notag,
$$

for convex function, $f$, $\mathbf{x} \in \mathcal{R}^N$, and $\sum_{i=1}^N \lambda_i = 1$. For $N = 2$, we have,

$$
f(\lambda_1 x_1 + \lambda_2 x_2) \leq \lambda_1f(x_1) + \lambda_2f(x_2),
$$

for any $\lambda_1, \lambda_2$ such that $\lambda_1 + \lambda_2 = 1$ (definition of convexity). Then suppose, $f\big(\sum_{i=1}^k\lambda_i x_i\big) \leq \sum_{i=1}^k \lambda_if(x_i).$ We define,

$
\lambda_j = \begin{cases}
\ \ \ \ \lambda_i & j = 1: k-1 \\
\lambda_k + \lambda_{k+1} & j = k
\end{cases} $ and $
x_j = \begin{cases} 
\ \ \ \ \ \ \ \ \ \ \ \ \ \  x_i & j = 1: k-1 \\
\frac{\lambda_k}{\lambda_k + \lambda_{k+1}}x_k + \frac{\lambda_{k+1}}{\lambda_k + \lambda_{k+1}}x_{k+1} & j = k
\end{cases}$, then,

\begin{align}
f\Bigg(\sum_{i=1}^{k+1}\lambda_i x_i\Bigg) &= f\Bigg(\sum_{j=1}^{k}\lambda_j x_j\Bigg) \notag \\
&\leq \sum_{j=1}^k \lambda_jf(x_j) \notag \\
&= \sum_{i=1}^{k-1} \lambda_if(x_i) + (\lambda_k + \lambda_{k+1})f\Bigg(\frac{\lambda_k}{\lambda_k + \lambda_{k+1}}x_k + \frac{\lambda_{k+1}}{\lambda_k + \lambda_{k+1}}x_{k+1}\Bigg) \notag \\
&\leq \sum_{i=1}^{k-1} \lambda_if(x_i) + (\lambda_k + \lambda_{k+1})\frac{\lambda_k}{\lambda_k + \lambda_{k+1}}f(x_k)+ \frac{\lambda_{k+1}}{\lambda_k + \lambda_{k+1}}f(x_{k+1}) \notag \\
&= \sum_{i=1}^{k+1} \lambda_if(x_i) \notag, 
\end{align}

and Jensen's inequality follows from the principle of mathematical induction.

\subsubsection{Sums of Convex Functions}
An important result is that if we sum two convex functions, the result is convex also. To see this in two dimensions, define convex functions $f(x)$ and $g(x)$, and their sum $h(x)$. Then,

\begin{align}
h((1 - \lambda)x_1 + \lambda x_2) &= f((1 - \lambda)x_1 + \lambda x_2) + g((1 - \lambda)x_1 + \lambda x_2) \notag \\
&\leq (1 - \lambda)f(x_1) + \lambda f(x_2) + (1 - \lambda)g(x_1) + \lambda g(x_2) \notag \\
&= (1 - \lambda)(f(x_1) + g(x_1)) + \lambda(f(x_2) + g(x_2)) \notag \\
&= (1 - \lambda)h(x_1) + \lambda h(x_2). \notag
\end{align}

If one of the functions, say $f(x)$, is linear, it is sufficient to note that $f((1 - \lambda)x_1 + \lambda x_2) = (1 - \lambda)f(x_1) + \lambda f(x_2) \leq (1 - \lambda)f(x_1) + \lambda f(x_2)$, and the proof holds as before. In fact, a linear function satisfies the definition for convexity \emph{and} concavity.

\subsection{Necessary and Sufficient Conditions for Optimality}

A basis for designing optimisation algorithms are the sets of conditions that tell us when we have found a local or global optimum. Necessity and sufficiency are formal logical terms. If a condition $S$ implies a condition $N$, we write $S \implies N$, meaning if $S$ is true, then $N$ is also. Thus, we say $S$ is a \emph{sufficient} condition for $N$, even though $N$ may be true without $S$. It also means that if $N$ is not true, $S$ cannot be true, even though $N$ may be true independently when $S$ is false. Thus, we say $N$ is a \emph{necessary} condition for $S$. When the implication runs both ways (if and only if), we write $S \iff N$, and say that $S$ is a \emph{necessary and sufficient} condition for $N$, and vice versa.

\subsection{Unconstrained Problems}

When we have a differentiable objective function and no constraints, the optimality conditions are simplified. Consider the second derivative test for one-dimensional problems. From first principles, the second derivative at a stationary point ($f'(x^*) = 0$) is,

$$f''(x^*) = \lim_{h \to 0}\frac{f'(x^* + h) - \cancelto{0}{f'(x^*)}}{h} = \lim_{h \to 0}\frac{f'(x^* + h)}{h}.$$

If this quantity is positive, we must be at a minimum, as the function is increasing to the right of $x^*$. If negative, $x^*$ is a maximum. Similar conditions exist in the multi-dimensional setting. First, we must have a zero gradient,

$$\nabla_{\mathbf{x}}f(\mathbf{x}^*) = \mathbf{0},$$

and we must have a positive semi-definite Hessian matrix, that is,

$$\mathbf{v}^T\cdot\nabla_{\mathbf{xx}}f(\mathbf{x}^*)\cdot\mathbf{v} \geq 0, \forall \ \mathbf{v} \in \mathbb{R}^N,$$

the intuition being that in every direction outward from $\mathbf{x}^*$, the gradient will be positive, indicating a minimum--just as it was in the one-dimensional case. For a maximum, we instead require a negative semi-definite Hessian. These are necessary and sufficient conditions for optimality.

\subsection{Equality Constraints}

Note that geometrically, the gradient at a point is normal to the \emph{level curve} or contour. The directional derivative of \emph{differentiable} $f$ in the direction of some vector, $\mathbf{v}$ is,

\begin{align}
\nabla_{\mathbf{v}}\mathbf{f}(\mathbf{x}) &= \lim_{h \to 0} \frac{f(\mathbf{x} + h\mathbf{v}) - f(\mathbf{x})}{h} \label{eq:dirdiv} \\
&= \nabla\mathbf{f}(\mathbf{x})\cdot\mathbf{v} = \Delta x_1\frac{\partial f}{\partial x_{1}} + \Delta x_2\frac{\partial f}{\partial x_{2}} + \cdots + \Delta x_n\frac{\partial f}{\partial x_{n}}. \notag
\end{align}

If we choose $\mathbf{v}$ in the direction of a contour, then by definition $f(\mathbf{x} + h\mathbf{v}) = f(\mathbf{x})$, hence the directional derivative is $0$, and so $\nabla\mathbf{f}(\mathbf{x})\cdot\mathbf{v} = 0$, hence the gradient is normal to the contour. Now consider an optimisation problem with a single equality constraint,

\begin{align}
\begin{array}{rl}
\displaystyle \min_{\mathbf{x} \in \mathcal{X}} & f(\mathbf{x})\\
\text{subject to} & h(\mathbf{x}) = 0
\end{array} \notag,
\end{align}

noting it is always possible to ensure an equality constraint equals 0 by subtracting the right-hand side terms. Given a feasible point (satisfying the constraint), $\mathbf{x}_F$, we can decrease the objective function by taking a step, $\delta\mathbf{x}$, such that the directional derivative, $\delta\mathbf{x}_F\cdot(-\nabla_{\mathbf{x}}\mathbf{f}(\mathbf{x})) > 0$. From (\ref{eq:dirdiv}) we know this implies $f(\mathbf{x}_F + \delta\mathbf{x}) < f(\mathbf{x}_F)$. Now, in order to move to a new feasible point, we must choose $\delta\mathbf{x}$ such that we move along the constraint surface, that is, parallel to the level curve at $0$ on the surface, $h(\mathbf{x})$. We know that a vector running parallel to a level curve will be normal to the gradient. Thus, we need $\delta\mathbf{x}\cdot\nabla_{\mathbf{x}}\mathbf{h}(\mathbf{x}) = 0$. Combining these ideas, consider where $-\nabla_{\mathbf{x}}\mathbf{f}(\mathbf{x}_F) = \lambda\nabla_{\mathbf{x}}\mathbf{h}(\mathbf{x}_F)$ for some scalar $\lambda$. Multiplying by $\delta\mathbf{x}$, we get the stopping condition,

$$\delta\mathbf{x}\cdot(-\nabla_{\mathbf{x}}\mathbf{f}(\mathbf{x}^*)) = \delta\mathbf{x}\cdot\lambda\nabla_{\mathbf{x}}\mathbf{h}(\mathbf{x}^*) = 0,$$

that is, where the gradient of $f(\mathbf{x})$ runs parallel to the gradient of the level curve. At such a point $\mathbf{x}^*$, we have it that the next feasible point (normal to the constraint surface) will not further minimise the objective function. This stopping condition is incorporated in the method of Lagrange multipliers.

\subsection{The Method of Lagrange Multipliers}

The method of Lagrange multipliers, discovered by Italian-French mathematician Joseph Louis Lagrange (1736-1813), takes a constrained problem and creates a new, \emph{unconstrained} objective function that incorporates these conditions, by introducing auxiliary variables called the Lagrange multipliers. Solving it will solve the original, constrained problem. We define the \emph{Lagrangian},

$$\mathcal{L}(\mathbf{x}, \lambda) = f(\mathbf{x}) + \lambda h(\mathbf{x}).$$

Clearly, at an optimum, $\mathcal{L}(\mathbf{x}^*, \lambda^*) = f(\mathbf{x}^*)$, since $\nabla_{\mathbf{x}}\mathcal{L}(\mathbf{x}^*, \lambda^*) = \mathbf{0} \implies -\nabla_{\mathbf{x}}\mathbf{f}(\mathbf{x}^*) = \lambda\nabla_{\mathbf{x}}\mathbf{h}(\mathbf{x}^*)$, the optimality condition from above, and $\nabla_{\lambda}\mathcal{L}(\mathbf{x}^*, \lambda^*) = 0 \implies h(\mathbf{x}) = 0$, the equality constraint. We finally require $\mathbf{v}^T\cdot\nabla_{\mathbf{xx}}f(\mathbf{x}^*)\cdot\mathbf{v} \geq 0, \forall \ \mathbf{v} : \nabla_{\mathbf{x}}\mathbf{h}(\mathbf{x}^*)^T\mathbf{v} = 0$, that is, positive semi-definiteness for the Hessian (though we need only consider vectors $\mathbf{v}$ along the constraint contour). To illustrate, consider maximising the Shannon entropy equation, $f(p_1, \dots, p_n) = -\sum_i p_i \log p_i$, for some probability distribution, $\mathbf{p}$, subject to the constraint $\sum_i p_i = 1$. Introducing a Lagrange multiplier for the constraint gives us,

$$\mathcal{L}(\mathbf{x}, \lambda) =  -\sum_i p_i \log p_i + \lambda\bigg(\sum_i p_i - 1\bigg),$$

which may be differentiated, giving us an equation for each $p_i$ that show (as before) that the maximising distribution is the uniform distribution, that is, $p_i = 1/n$. It is easy to extend this method to multiple equality constraints. In this case we can write the Lagrangian,

$$\mathcal{L}(\mathbf{x}, \boldsymbol\lambda) = f(\mathbf{x}) + \boldsymbol\lambda^T \mathbf{h}(\mathbf{x}),$$

and the optimality condition $\nabla_{\lambda}\mathcal{L}(\mathbf{x}^*, \boldsymbol\lambda^*) = \mathbf{0} \implies h_i(\mathbf{x}^*) = 0$ for each constraint $i$.

\subsection{Inequality Constraints}

Consider an optimisation problem with a single inequality constraint,

\begin{align}
\begin{array}{rl}
\displaystyle \min_{\mathbf{x} \in \mathcal{X}} & f(\mathbf{x})\\
\text{subject to} & g(\mathbf{x}) \leq 0
\end{array} \notag.
\end{align}

Such a constraint may be either \emph{active} or \emph{inactive}. When it is inactive, the local optimum falls within the feasible region, and the problem is effectively unconstrained. In this case, the optimality conditions are identical to the unconstrained case. Note, however, our conditions do not tell us how to predetermine whether a constraint is active, nor do they (as previously noted) tell us how to find the optimum. At most, the optimality conditions inform the design of optimisation algorithms by telling us what to look for. When the constraint is \emph{active}, the local optimum lies outside of the feasible region. We note first that the constrained optimum must lie on the constraint surface, as this is closest to the local optimum. If this were not the case, it would imply the existence of another (closer) local optimum inside the feasible region. So, in this case we effectively have an equality constraint as before. The optimality condition is therefore,

\begin{align}-\nabla_{\mathbf{x}}\mathbf{f}(\mathbf{x}) = \mu\nabla_{\mathbf{x}}\mathbf{h}(\mathbf{x})\label{eq:inequalityconstraint},\end{align}

for $\mu > 0$. The scalar must be positive as it ensures the gradient is facing `outwards' of the feasible region, indicating that we are at the extreme point closest to the local optimum. The Karush-Kuhn-Tucker (KKT) conditions capture these two cases in a general set of conditions.

\subsection{The KKT Conditions}

The Karush-Kuhn-Tucker (KKT) conditions are a general set of necessary and sufficient conditions for optimisation problems constrained by multiple equality and inequality constraints. For optimisation problem,

\begin{align}
\begin{array}{rl}
\displaystyle \min_{\mathbf{x} \in \mathcal{X}} & f(\mathbf{x})\\
\text{subject to} & g(\mathbf{x}) \leq 0
\end{array} \notag,
\end{align}

we form the Lagrangian,

$$\mathcal{L}(\mathbf{x}, \boldsymbol\lambda) = f(\mathbf{x}) + \mu g(\mathbf{x}),$$

and necessary and sufficient conditions for optimality are,

\begin{enumerate}
\item $\nabla_{\mathbf{x}}\mathcal{L}(\mathbf{x}^*, \mu^*) = \mathbf{0}$. Thus, when the inequality constraint is inactive, $\mu^{*} = 0$ and $\nabla_{\mathbf{x}}\mathbf{f}(\mathbf{x}) = \mathbf{0}$. When the constraint is active, $\mu^* > 0$ and $-\nabla_{\mathbf{x}}\mathbf{f}(\mathbf{x}) = \mu\nabla_{\mathbf{x}}\mathbf{h}(\mathbf{x})$ as in equation (\ref{eq:inequalityconstraint}).
\item $\mu^* \geq 0$, with equality for an inactive constraint.
\item $g(\mathbf{x}^*) \leq 0$ with equality for an active constraint.
\item $\mu^*g(\mathbf{x}^*) = 0 \implies g(\mathbf{x}^*) = 0$ when $\mu^* > 0$ for an active constraint.
\item Positive semi-definite constraints on $\nabla_{\mathbf{x}\mathbf{x}}\mathcal{L}(\mathbf{x}^*, \mu^*)$.
\end{enumerate}

Clearly, extending this to multiple inequality constraints, and further incorporating equality constraints is a trivial matter, giving a Lagrangian function,

$$\mathcal{L}(\mathbf{x}, \boldsymbol\lambda) = f(\mathbf{x}) + \boldsymbol\mu^T \mathbf{g}(\mathbf{x}) + \boldsymbol\lambda^T \mathbf{h}(\mathbf{x}).$$


\section{Computability and Complexity}

\subsection{Decision Problems}

Decision problems, as studied in computability and computational complexity theory, are characterised by a question posed on some arbitrary inputs that has a boolean (true-false/yes-no) answer. The question itself is the decision \emph{problem}, which when paired with a parameter set is called a problem \emph{instance}. For example, a decision \emph{problem} might be to determine if some number $x$ is prime or not. A particular problem \emph{instance} might be very trivial, for example, `is the number 2 prime?'. Exact methods of primality testing do exist, however, making primality testing a \emph{decidable} problem, that is, primality is \emph{computable} for any input, though it may take a long time to compute. On the other hand, there are decision problems that are \emph{undecidable}. An example of this is the \emph{halting problem}. It was in demonstrating the undecidability of the halting problem that the great English mathematician, Alan Turing (1912-1954), devised an abstract model of the computer, the Turing machine.

\subsection{The Halting Problem}

The halting problem was one of the first problems shown to be undecidable, that is, no algorithm exists to solve it for all possible inputs. The halting problem is concerned with determining whether an arbitrary algorithm (in the form of some description), provided with arbitrary inputs, will terminate (halt) or not. For example, a trivial input algorithm might be `divide the input by 2'. This single-instruction algorithm can conceivably be analysed by any number of halting `analysers' and successfully be shown to terminate for any input. However, an algorithm solving the halting problem will need to decide termination for \emph{any} algorithm. It is tempting to suggest simply simulating the input algorithm and declaring a halt when it terminates. The problem here is that the algorithm may never terminate, and it is impossible to know whether it will \emph{not} terminate in a finite amount of time (as it might just be mistaken for a very long-running--yet finite--algorithm). Waiting an infinitely long time to determine the halting problem is against the rules.

\begin{algorithm}
\caption{Pseudocode illustrating a paradox arising from a halting algorithm.}\label{alg:haltingproblem}
\begin{algorithmic}[1]
\Procedure{Halt}{algorithm, parameters}
\If { \//*algorithm halts with parameters*\// }
\State \Return TRUE
\Else
\State \Return FALSE
\EndIf
\EndProcedure
\Procedure{Halt+}{algorithm}
\If {\textsc{Halt}(algorithm, algorithm)}
\State LoopForever()
\Else
\State Terminate()
\EndIf
\EndProcedure
\State \textsc{Halt+}(\textsc{Halt+}) \//*undecidable*\// 
\end{algorithmic}
\end{algorithm}

Turing showed the halting problem to be undecidable with the following proof by contradiction. Suppose there is an algorithm \textsc{Halt} that takes as input some complete description of an algorithm along with some compatible parameters (Algorithm \ref{alg:haltingproblem}) and solves the halting problem, that is, determines in finite time whether this arbitrary algorithm terminates given the set of inputs. Imagine another algorithm \textsc{Halt+} that first runs this \textsc{Halt} algorithm, and if the result is `true' (halts), it loops forever (does not halt), otherwise, if `false' (does not halt), it halts. Finally, consider calling \textsc{Halt+} on itself. Now we see the contradiction--if the \textsc{Halt} subroutine returns `true' for \textsc{Halt+}(\textsc{Halt+}), then \textsc{Halt+}(\textsc{Halt+}) loops forever and does not halt. If \textsc{Halt} returns `false', then \textsc{Halt+}(\textsc{Halt+}) instead terminates. So, \textsc{Halt+} always negates whatever decision \textsc{Halt} makes, making its behaviour undecidable by \textsc{Halt}, and therefore \textsc{Halt} does not solve the halting problem, making the problem undecidable.

\subsection{Turing Machines}

To complete his proof on the halting problem, Turing required an abstract model of computation on which to base his formal reason. Clearly, it would be infeasible to make assertions about the behaviours of algorithms without a clear definition of what an algorithm actually is. Turing therefore came up with the Turing machine, an abstract computing model generalising the finite state machine (FSM) or automaton\footnote{A finite state machine is an abstract model of computation consisting of a finite number of states, representing actions. Movement between states is dictated by the conjunction of an input value and the identity of the current state. Finite state machines are usually visualised with a diagram.}, sufficiently general to simulate any computer program or algorithm. A Turing machine consists of an infinitely (arbitrarily) long tape divided into cells. A read-write head focuses on one cell at a time, and depending on the cell contents (symbol) and the current internal state of the machine, the machine writes a symbol, moves the tape left or right, and transitions to a new internal state. Formally, the transition function, $\delta$, of a Turing machine can be written,

$$\delta : S \times Q \to S \times \{\text{left}, \text{right}\} \times Q,$$

that is, given a current read symbol $\in S$ and internal state $\in Q$, the transition function defines a write symbol, left or right movement of the tape, and a new internal state. Any symbol set may be used (though binary code is regularly used to illustrate) without changing the computing power of the machine. Intuitively, the tape is providing the machine with memory, and the transition function defines conditional instruction sets to be performed. It may still be a stretch, however, to fathom that this simple design is capable of performing any algorithm, procedure, or computation imaginable.

\subsubsection{Universal Turing Machines}

If we take a Turing machine to be a model of computation for some function, a \emph{universal Turing machine} (UTM) is a Turing machine whose instruction set is configured to be able to run any Turing machine (whose own instruction sets can be encoded as strings in the UTM symbol set) and any arbitrary input. Thus, we would have an interpreter for any Turing machine, and with it a model for a stored-program \emph{computer}, rather than the more abstract model \emph{computation} that Turing machines represent. This model is the original concept behind all computer code (software) as executed on computer hardware. In fact, almost all modern computers can be described as universal Turing machines. A programming language is said to be \emph{Turing complete} if it is capable of simulating any Turing machine, making it a universal Turing machine. All imperative programming languages (\textsc{Python}, C++, \textsc{Java}) have this property--all that is required is the ability to execute instructions conditionally (for example with `if' statement) and to allocate an arbitrary amount of memory. It is strongly argued that the universal Turing machine prefaced John von Neumann's design for the stored-program computer, the von Neumann architecture, the basis of all modern computers.

\subsubsection{Non-deterministic Turing Machines}

A non-deterministic Turing machine (NTM) is a Turing machine for which multiple operations may exist for each input-state pair. The NTM can pursue each of these alternatives simultaneously and at no extra cost. Thus, an operation with exponentially growing alternative paths, such as a tree search, can be computed in linear time (in the depth of the search). It is crucial to note the direct equivalence between how a NTM finds a solution and how a solution can be verified. By pursuing each alternative, the NTM is \emph{guessing} its way through the solution space, without incurring the cost of erring. If a specific solution is provided, a deterministic Turing machine can verify it in the same time (by \emph{guessing} its way through a single solution). Thus, NTM problem solving is exactly equivalent to TM problem verification. Further note that any computation completed on an NTM can also be computed on a deterministic Turing machine, though perhaps not as efficiently. NTMs have no real-world implementation (unlike Turing machines), and remain a purely theoretical idea. Rather, they exist as a model of \emph{solution verification}, a key concept for classifying algorithm complexities.

\subsubsection{Other Turing Machines}

Many other Turing machines exist, such as Quantum Turing machines, which model the behaviour of a quantum computer. An important unresolved question in physics asks whether such a computing model could efficiently simulate any physical system.

\subsection{P versus NP}

P versus NP is one of the most well-known unresolved problems in computer science, and is in fact one of the seven Clay Institute Millennium problems. It refers to whether all problems that may be verified in polynomial time may be solved in polynomial time, and thus whether complexity class P is a subset of NP (P $\neq$ NP) or that they are the same class (P = NP). The former case is widely considered to be more likely, although theorists as eminent as Donald Knuth (1938-) side with the latter, albeit with the catch that the degree of polynomial may be astronomical.

\subsection{Complexity Classes}

Turing machines also provide the basis for discussing computational complexity classes. Complexity classes are most commonly discussed for decision problems, but equivalents exist for functional problems (for example the FP class), counting problems (\#P class), and others.

\subsubsection{Reducibility}

We say that one problem, $A$, may be reduced to another problem, $B$, if an algorithm solving $B$ could be used as a subroutine in an algorithm solving $A$. For example, if we had such an algorithm for $B$, we could write our algorithm for $A$ in terms of a sequence of executions of an \emph{oracle} (black box) executing the algorithm for $B$. If a polynomial number of executions were required, we could say $A$ is polynomially reducible to $B$, formally $A \leq_p B.$ This is an important property for comparing algorithms of different complexity classes.

\subsubsection{P class}

The P class is the set of decision problems decidable in polynomial time on a deterministic Turing machine. Polynomial is a desirable, \emph{tractable} complexity that, as a rule, indicates algorithm run time does not increase \emph{too quickly} (think exponentially) with the size of the problem\footnote{A problem's size is usually expressed in the number of variables.}.

\subsubsection{NP class}

A common misconception is that NP stands for `non-polynomial' time. Ironically, that would be to beg the question of P versus NP\footnote{It is true that known algorithms for NP problems run in super-polynomial (exponential) time on deterministic Turing machines, but the whole point of P versus NP is that it is not yet known if faster algorithms exist.}. In actual fact, NP stands for \emph{non-deterministic polynomial time}, that is, \emph{NP is the class of algorithms that run in polynomial time on a non-deterministic Turing machine}. However, as noted above, algorithms that run polynomially on an NTM are algorithms whose solution may be verified in polynomial time on a deterministic Turing machine. Hence, the NP class can be alternatively stated as \emph{algorithms that can be verified in polynomial time by a deterministic Turing machine}. Note that all P time algorithms are in the NP class, as whatever can be done polynomially on a deterministic Turing machine can be done polynomially on a NTM, though not necessarily vice versa. This disjunction is the subject of one of the most famous unsolved problems in mathematics. A problem can be shown to be in the NP class if it can be shown that all candidate solutions can be verified in polynomial time.

\subsubsection{NP-hard class}

NP-hard (non-deterministic polynomial time \emph{hard}) problems are those problems that can are \emph{at least as hard as the hardest problems in NP}. In formal terms, this means an NP-hard problem can be reduced to any problem in NP in polynomial time. Note that a problem may be purely NP-hard (not in NP), whereas problems in the intersection between NP and NP-hard are known as NP-complete.

\subsubsection{NP-complete class}

The NP-complete class is the intersection of NP and NP-hard, that is, it contains problems that are reducible to all NP problems, and are additionally verifiable in P time. To prove the NP-completeness of a problem, it is necessary to,

\begin{enumerate}
\item Show it is in NP, by showing its solutions may be efficiently verified
\item Show it is NP-hard, by showing it can be efficiently reduced to another known NP-complete problem.
\end{enumerate}

The first decision problem proved to be NP-complete is the boolean satisfiability problem or SAT (see the Cook-Levin theorem, 1971), which aims to find a combination of values for literals in a boolean expression such that the overall expression evaluates \texttt{TRUE}. The expression is taken to be in conjunctive normal form (a conjunction of disjunctions or an \texttt{AND} of \texttt{OR}s), for example, $$(X_1 \lor X_2 \lor \dots \lor X_N) \land (\neg X_1 \lor \dots) \land \dots$$ There are some special cases of SAT where computation is easy, for example if each variable features in exactly one clause, but in general, no efficient algorithm is known to exist. From a purely intuitive perspective, the task lends itself to an exponential runtime, as there are $2^N$ possible interpretations of an expression with $N$ variables. Until P versus NP is resolved, it is not known whether there is a reliable shortcut. Richard Karp used this result to find and categorise 21 NP-complete problems, beginning with the 3SAT problem, a variant of SAT in which all clauses contain three literals. It is easy to transform a CNF expression to the 3SAT form, with the introduction of dummy variables. The expression loses logical equivalency, but retains equisatisfiability, meaning that solutions in the 3SAT form are necessary and sufficient for solutions in the original form. The length of the expression triples, hence the reduction step is polynomial. Karp's 21 NP-complete problems further include problems from graph theory (such as clique selection) and 0-1 integer programming. Note that if a problem in the NP-hard or NP-complete classes could be shown to have an efficient, polynomial-time solution algorithm, it would imply all problems in NP have efficient solutions.

\section{Discrete Optimisation}

\subsection{Integer Programming Problems}

Integer programming is a branch of discrete optimisation. It is closely related to combinatorial optimisation. When we talk about integer programming, we usually refer to \emph{integer linear programming} (ILP), a variant of linear programming with the restriction that decision variables must be integers. Zero-one integer linear programming is a special case where variables take on binary values, and there is no objective function to optimise, only constraints to satisfy. Zero-one ILP is NP-complete, on the other hand ILP in general is NP-hard\footnote{The decision version of ILP will also be NP-complete. For example, `is there a feasible value of $\mathbf{x}$ such that $z \geq k$?' for some value $k$. Clearly, this can be verified in polynomial time.}. An integer programming problem may be formulated as,

\begin{align}
\begin{array}{rl}
\text{maximise} & z = \mathbf{c}^T\mathbf{x} \\
\text{subject to} & \mathbf{A}\mathbf{x} = \mathbf{b}, x_i \geq 0, x_i \in \mathbb{Z}
\end{array}.
\label{eq:intprog}
\end{align}

There are many famous problems that may be modelled as an integer program.

\subsubsection{The assignment problem}

The assignment problem models the allocation of machines to tasks such that each machine performs exactly one task and each task is performed, minimising a function subject to costs incurred from job-machine pairings. Unlike most ILPs, the assignment problem is in the P class, the famous Hungarian algorithm solving it in cubic time.

\subsubsection{The knapsack problem}

The knapsack problem as an integer programming model for maximising the summed benefit of a range of items, subject to a weight constraint. Each item has an associated value and weight parameter. The decision variables represent the number of each item selected.

\subsubsection{The travelling salesman problem}

The travelling salesman problem (TSP) models a route plan between a set of destinations, each of which must be visited once and only once\footnote{Clearly, visiting any city more than once must be suboptimal anyway, as by simple geometry it cannot be faster to visit one destination via another than to visit that destination directly.}. The destinations may be represented as vertices on a fully connected graph whose arcs provide the costs of moving between each pair. As an integer programming problem, the objective function minimises the total route cost and the boolean decision variables $x_{ij}$ indicate if destination $j$ is visited from destination $i$. There are \emph{sum to one} constraints to ensure each vertex is arrived at and departed from exactly once.

\subsection{Solution Methods}

Of the exact solution methods for integer programming problems, the branch and bound algorithm is the most prevalent. However, as integer programming is NP-hard, computing exact solutions is often infeasible, and heuristic techniques are used to approximate optimal solutions efficiently. A variety of other approaches exist, including dynamic programming.

\subsubsection{Branch and bound}

The branch and bound method is an exact algorithm, improving upon an exhaustive approach. For example, in an assignment minimisation problem, it is easy to establish a rough lower bound on the optimal solution, simply by ignoring the constraints and taking the minimising value for each variable. Thus, an initial iteration will consider all possible assignments for a first variable, and calculate the (possibly infeasible) lower bound for solutions arising from that. The algorithm continues to consider all possible alternatives for a second variable, and a third, and so on. Once an exact feasible solution is found, it can be used to eliminate all branches with a lower bound exceeding that solution, as even in the best (possibly infeasible) case a solution will be inferior. There are some similarities between branch and bound and the alpha-beta pruning algorithm used in computer chess, and others.

\subsubsection{Simulated annealing}

Simulated annealing is a popular meta-heuristic algorithm. The aim is to reach a \emph{near-optimal} solution efficiently. Exact solutions may be acquired by chance, though this becomes increasingly unlikely for larger problems. Simulated annealing facilitates both incremental improvement and exploration, according to a schedule of \emph{temperatures} (annealing is a metallurgical term) that decrease linearly to zero round by round. In each round, update steps are taken on the objective function. Improvements are always retained, but updates that reduce the objective function are adopted according to some probability function (such as softmax), for which the probability of accepting inferior updates reduces as the schedule plays out. As a meta-heuristic, it describes an exploration strategy for non-convex objective functions without specifying the update step. It is therefore compatible with many problems.

\subsubsection{Tabu Search}

Tabu (taboo\footnote{From the Tongan word for \emph{prohibited}.}) search is a meta-heuristic algorithm for discrete optimisation problems. From an initial solution, the algorithm considers all possible updates in the solution's neighbourhood. It chooses the best amongst them, even if it is inferior (though it tracks the best overall solution). This update is memorised and from it, a new neighbourhood is generated. Any update that would undo the previous update is regarded as \emph{tabu}, except if it beats the best overall solution. The algorithm proceeds thus, with continued local aspiration until some stopping condition is met. Various stopping conditions are possible, for example if no improvement is made for a streak of iterations.

\section{Graph Theory}
Graphs are mathematical objects with many interesting properties, as well as a vast range of applications. A graph, G = (V, E), consists of a set of vertices (or nodes, or points), $V$, and a set of edges (or arcs, or lines), $E$, that connect them. Edges consist of distinct start and end vertices. In general, a graph's properties do not depend on the \emph{placement} of its vertices, nor on the curvature of its edges. As a result, it does not matter how a graph might be drawn. Its \emph{structure} does not vary as long as all edges and vertices are correctly connected. It can be of mathematical interest, however, to compare the ways a graph can be drawn, for example: given a graph, $G$, can it be drawn without intersecting edges?

There are many structural properties of graphs that may be of interest. A graph is \emph{connected} if every node is reachable from every other node via some path of one or more edges. A disconnected graph may have several \emph{islands} of vertices with no edges to bridge them. A graph of $N$ vertices is \emph{complete} when each of its nodes share an edge with every other node, thus amounting to $1 + 2 + 3 + \cdots + N = N(N - 1)/2$ edges\footnote{The same formula used in the legend of the schoolboy Gauss, who summed the arithmetic series $1 + 2 + 3 + \cdots + 100$.}.  The density of a graph reflects the degree to which it is complete. Density is therefore given by the number of edges divided by the above formula (the maximum). A graph is \emph{bipartite} if its vertices are in two disjoint sets, with every node of each set connected to at least one node in the other set, but with no arcs between nodes of the same set. A graph is \emph{complete} bipartite if each node connects to every node in the opposite set. A graph is \emph{planar} if it can be drawn in two dimensions such that its edges intersect only at vertices, that is, no criss-crossing. It may therefore be \emph{embedded} on a two-dimensional plane. Graphs are taxonomised in various other ways according to the number of their vertices and the manner by which they are connected.

The edges of a graph may embed information such as direction and weight. If a graph has directed edges (commonly depicted with an arrowhead), it is known as a \emph{directed} graph or \emph{digraph}, otherwise, undirected. If some arcs are bidirectional, the graph is known as a directed multigraph, and, for example, a weight may be stored in each direction (upstream and downstream). A directed acyclic graph (DAG) is a directed graph with no cycles. For example, a tree is always directed and acyclic. If a graph has edge weights, it is called a \emph{weighted} graph, and may model things such as route distances and network flows for which paths of accumulating length are of interest.

\subsection{The Seven Bridges of K\"onigsberg}

A celebrated early result in graph theory is by Euler in 1736, when he solved the \emph{Seven Bridges of K\"onigsberg} problem. He demonstrated no walk could be made over the seven bridges connecting the islands of the town, K\"onigsberg (then Prussia, now Kaliningrad, Russia), such that each bridge was crossed exactly once. Euler reduced the layout of the town from a detailed cartographical map to a graph, with four vertices to represent the four land masses and seven arcs between them for the bridges. He then made the simple observation that apart from the first and last land masses in a tour, each land mass must have an outbound arc for every inbound arc in order to meet the requirement of crossing every bridge exactly once. As every land mass had an odd number of bridges, the problem was logically unsolvable. Of course, Euler could have performed this same reasoning without this novel formalism, but a neat graph depiction brought about some mathematical rigour.

\subsection{The Three Cottages Problem}

The Three Cottages Problem is an old problem of unknown origin whose solution constitutes a fundamental result in graph theory. The problem statement reads:

\emph{Suppose we have three cottages in a small village, and three utility stations (for example, water, drainage, and electricity). Is it possible to connect each cottage with each utility with paths that do not cross?}

Thus, the cottages and stations form a complete bipartite graph, with three nodes in each set. In the Kuratowski\footnote{Kazimierz Kuratowski (1896-1980) was a Polish mathematician.} notation, this is $K_{3, 3}$, so the problem is asking whether $K_{3, 3}$ is planar. A relatively simple solution exists. Note first that because the graph is bipartite, any face has at least four edges, as we cannot draw arcs between nodes of the same set. Thus, we may write $E^{*} \geq 4F$, where $F$ is the number of faces and $E^{*}$ is an upper bound on $E$, the number of edges, noting that an edge may be shared between multiple faces. In fact, we may observe also that each edge is in exactly two faces, as it shares a vertex with two other edges. Thus, $E^{*} = 2E$, giving $E \geq F/2$. Now, by the Euler characteristic\footnote{The Euler characteristic states for any convex polyhedron (any shape in any dimensions that is outward-facing like a circle, triangle, cube, sphere, etc.), or planar graph, $V - E + F = 2$, where $V$ is the number of vertices, $E$ is the number of edges, and $F$ is the number of faces. This equation is regularly voted as one of the most beautiful in mathematics, usually right behind Euler's identity.}, $V - E + F = 2$, where finally $V$ is the number of vertices. By substitution, we obtain $E \leq 2V - 4$. Since in our problem $V = 6$ and $E = 9$, we have found a contradiction, hence the problem has no solution. Wagner's theorem is a fundamental result for planar graphs, stating that no planar graph may contain the \emph{utility graph}, $K_{3,3}$, or $K_5$ (the fully-connected graph in fives node) as a minor\footnote{A graph minor is, roughly speaking, a graph formed from a subset of the vertices in a graph. Specifically, a minor can be formed from an initial graph by \emph{deleting} some number of vertices and edges, or \emph{contracting} edges, that is merging two vertices such that the edge between them disappears.}.

\subsection{NP-complete Graph Problems}

Many graph-related problems are NP-complete. Here we briefly detail the clique problem and the minimum vertex cover problem, both members of Karp's 21 NP-complete problems.

\subsubsection{The Clique Problem}

A clique is a subset of the vertices of a graph whose subgraph is complete. In a social network, this could represent a group of people who all know each other.  Finding a clique of a certain size is an NP-complete problem, as demonstrated graphically in Figure \ref{fig:clique}. For the 3SAT expression $(y \lor x \lor x) \land (\neg x \lor y \lor y) \land (\neg x \lor \neg y \lor \neg y)$, form the tripartite graph corresponding to the groupings of the three clauses, connecting nodes whose corresponding literals are logically compatible. If three literals are logically compatible, they solve the 3SAT problem, and form a clique. This implies that the clique problem is NP-complete.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{Figures/clique.pdf}
\caption{Graphically showing the equivalence between 3SAT and the clique problem, implying the clique problem is NP-complete.}
\label{fig:clique}
\end{figure}

\subsubsection{Minimum Vertex Cover}

The goal of the minimum vertex cover problem is to find a subset, $V'$, of vertices of a graph such that every edge in the graph has at least one endpoint in $V'$, and such that the sum of the costs of the chosen vertices is minimised. This can be formulated as an integer linear program,

\begin{align}
\begin{array}{rl}
\text{minimise} & z = \sum_v c(v)x_v \\
\text{subject to} & x_u + x_v \geq 1, \forall \{u, v\} \in E, \\
& x_i \in \{0, 1\}
\end{array}
\label{eq:vertexcover}
\end{align}

where $c(v)$ denotes a cost function for a vertex $v$ and $x_v$ is set to one if the vertex $v$ is selected for the cover.

\subsection{Graph Traversal}

The act of traversing a graph is to visit each of its nodes exactly once, according to some traversal ordering. Two main strategies may be used: depth-first search (DFS) and breadth-first search (BFS). A DFS visits the descendants of a node before visiting its siblings, that is, traversing sub-branch by sub-branch. A BFS visits the siblings of a nodes before visiting its descendants, that is, traversing depth by depth. Pseudo-code for DFS and BFS is given in Algorithms \ref{alg:dfs} and \ref{alg:bfs} respectively. When the graph has a tree structure (only branching, no converging), there is a single path to each node from the root, hence marking nodes as visited is unnecessary. Traversal may, for example, be used to exhaustively locate a particular node in the graph. A BFS has the potential advantage of facilitating a depth cutoff, searching each depth exhaustively and one at a time. However, there is then a space requirement to queue descendants in some internal memory structure, and for a ballooning tree structure such as in a chess \emph{game tree}, storing more than a few levels or \emph{ply} is infeasible.

\begin{algorithm}
\caption{Pseudocode for a depth-first search (DFS).}\label{alg:dfs}
\begin{algorithmic}[1]
\Procedure{InitDFS}{graph}
\State DFS(graph$\rightarrow$root)
\EndProcedure
\Procedure{DFS}{node}
\State node$\rightarrow$MarkAsRead()
\For {child in node$\rightarrow$children}
\If{NOT child$\rightarrow$AlreadyRead()}
\State DFS(child)
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Pseudocode for a breadth-first search (BFS).}\label{alg:bfs}
\begin{algorithmic}[1]
\Procedure{BFS}{graph}
\State queue $\leftarrow$ [ ]  \//* initialise FIFO queue *\//
\For {child in graph$\to$root$\to$children}
\State queue$\rightarrow$Enqueue(child)
\EndFor
\While {NOT queue$\rightarrow$IsEmpty()}
\For {child in queue$\rightarrow$front$\rightarrow$children}
\If{NOT child$\rightarrow$AlreadyRead()}
\State queue$\rightarrow$Enqueue(child)
\EndIf
\EndFor
\State queue$\rightarrow$Dequeue(queue$\rightarrow$front)
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Shortest Path Problem}

A common problem to compute over a weighted graph is the shortest path between a source node and sink node within the graph. Perhaps the two most famous shortest path algorithms are Dijkstra's algorithm and the Bellman-Ford algorithm.

\subsubsection{Dijkstra's Algorithm}

Given a weighted graph, Dijkstra's algorithm, discovered by Dutch computer scientist,  Edsger W. Dijkstra (1930-2002), finds the shortest path from a source node to all other nodes in a graph. In practice, the goal is often to find the shortest path from a source node to a unique sink node, but Dijkstra's algorithm computes shortest paths to all nodes as a matter of course. The algorithm consists of visiting each node in order of closeness (initially the source node has zero distance and all others are infinite). Each neighbour of the current node is then inspected and its shortest distances updated, if it happens that a shorter path can reach it via the current node. After a node has been visited, it is discarded, and the algorithm terminates once all nodes have been visited. Aside from their shortest distance, nodes store the node from which its shortest path derives, such that at the end, the shortest path can be traced back from sink to source. The algorithm runs in $O(N\log N)$ time in the worst case, where $N$ is the number of vertices.

\section{Introduction to Machine Learning}

Machine learning makes up the stochastic wing of the artificial intelligence community. It is an amalgamation of techniques from across statistics, probability theory, and numerical optimisation.

\subsection{Fundamental Notions of Machine Learning}

Here we specify a glossary of important terms in machine learning.

\subsubsection{Learning Approaches}

A machine learning task will follow one of the following fundamental approaches, which depend on the nature of the data $\mathcal{D}$ to be learned from:

\begin{itemize}
\item supervised learning - in this case, $\mathcal{D} = \{\mathbf{X}, \mathbf{y}\}$ is \emph{labelled}, that is, each data sample, $\mathbf{x}_i$, has a corresponding output, $y_i$, which is real for a regression task, and discrete for classification. The task of supervised learning is therefore to learn the map that best approximates $\mathbf{y}$ from $\mathbf{X}$. This is by far the most common form of machine learning, which includes many well known techniques, such as regressions, decision trees, support vector machines, and neural networks.
\item unsupervised - in this case $\mathcal{D}$ is unlabelled and the learning task is to extract patterns from the data. It is sometimes also known as \emph{knowledge discovery}. For example, we might want to find how to reasonably group the observations in $\mathcal{D}$, a task known as clustering. This is arguably the part of machine learning that is most like human learning in that rules are inferred without external confirmation. Unlike supervised learning, which sports a profusion of techniques, unsupervised learning techniques are less extensive and understood. Unsupervised learning is a sort of holy grail of machine learning.
\item semi-supervised - in this case, the dataset is a mixture of both labelled and unlabelled data. In many practical problems, labelled data is hard to come by, but unlabelled data might be abundant. Semi-supervised techniques seek to combine the data in a reliable way. This approach to learning has shown surprisingly limited success so far.
\end{itemize}

The following schemes refer to how the data is \text{used}, rather than the nature of the data (as above):

\begin{itemize}
\item reinforcement learning - in this case, learning incorporates occasional feedback signals in a trial and error framework.
\item active learning - refers to a scheme whereby training data can be actively selected.
\item online learning - refers to where models are trained on data samples that arrive one by one rather than all at once, as if streamed.
\item ensemble learning - ensemble models train multiple `weak' models on subsets of the data, then aggregate their individual predictions. Random forests are an example of this, where many decorrelated decision trees are trained in an effort to reduce variance. Adaboost (adaptive boosting) is a famous boosting  meta-algorithm.
\end{itemize}

\subsubsection{Training and Prediction}

\textbf{Feature engineering} is the task of designing features, according to some ontology about the model. This is an opportunity for domain knowledge to be imparted into a model, in the view of making more indicative features. This is an inherently soft science, but constitutes a large part of applied machine learning in practice. \textbf{Feature extraction} is a separate concept, which uses unsupervised learning to derive features for modelling. \textbf{Training} or learning is the act of inferring model parameters from a dataset through some sort of optimisation algorithm. It is vitally important to (randomly) set aside some data from the start, because there must be some independent data available to validate the trained model. If we use all data on training, it is impossible to know how it will perform on unseen data. The usual scheme is to first split the data into training and test datasets. The test data is strictly separated from the training process. A part of the training set is extracted as a validation set. These split sizes follow some loose convention, leaving the majority of data for training, for example a $70-20-10$ split. Models are trained on a training set and its predictive performance is assessed on a validation set, providing a benchmark to compare choices of model and model hyperparameters. When data is scarce, we can simulate having a validation set in a procedure called cross validation (CV). \textbf{Cross validation} partitions the training data into $K$ validation \emph{folds}. A model is trained on the first $K - 1$ folds, and validated on the final fold. This process is repeated $K$ times, such that each fold is validated on, giving $K$ samples for estimating error. If the fold size is chosen to be a single sample (that is, $K = N$), we have leave-one-out cross validation (LOOCV). The chosen model will be the one that minimises the average CV error. A model is then trained on all the training data (including validation data), and the resulting model tested on the test data. The predictions made are compared with the true values, giving an estimation of generalisation error that can then be reported. \textbf{Generalisation error} refers to the expected test error averaged over all possible datasets. As such data is in practice unavailable, an estimate can be found using cross validation. As more data is added, the predictive power (generalisation error) of the model improves, as the approximation error (the discrepancy between the estimate and the best estimate given the choice of model) in the model parameters is minimised.

\begin{figure}
\centering
\begin{tabular}{cc}
\subfloat[$k = 1$]{\includegraphics[width=0.5\textwidth]{Figures/knn1.pdf}} & 
\subfloat[$k = 7$]{\includegraphics[width=0.5\textwidth]{Figures/knn7.pdf}}
\end{tabular}
\caption{Visualisation of overfitting with k-nearest neighbours (kNN) (non-parametric model). A data point is classified as the majority class of the $k$ geometrically closest data points. Note the steadier decision boundary formed for $k = 7$. Created with \texttt{knnDemo.m}.}
\label{fig:knn}
\end{figure}

\textbf{Overfitting} is a constant danger when training models. This is the effect of \emph{fitting the noise instead of the signal}. All data contains noise, and when the dataset is sufficiently small and the model sufficiently complex (e.g. a high degree polynomial), the model can fit the training data perfectly, only to be useless on test data. Take for example a k-nearest neighbours classifier (kNN) (Figure \ref{fig:knn}). For small $k$, outliers have unreasonable influence on local test data, creating discontinuous decision boundaries. There are various strategies to attenuate overfitting. The most ideal is to increase the amount of data available for training, but this is rarely an option. A more typical approach is \emph{regularisation}. This usually takes the form of a penalty term, but really corresponds to imposing a prior distribution on the parameter set. A prior distribution assigns low probabilities to large parameter values. Complex models that overfit usually require large parameter values to make a fit, so regularisation curtails this tendency. Another, more drastic approach, is to change the form of the likelihood (cost) function of the model to insulate it from outliers in the data. An example of this is robust regression, where a Laplace likelihood replaces a Gaussian. A variant of overfitting is the zero count (sparse data) problem, where specific data that ought to be modelled has not appeared in the dataset and leads to a zero probability prediction. \emph{Underfitting} occurs when the complexity of the model is insufficient to capture the trends of the data. This can be addressed by modifying the choice of basis function or kernel, otherwise by choosing a different technique.

\subsubsection{Philosophical Ideas}

\textbf{Ground truth} is a somewhat nebulous term referring variably both to the \emph{correctness} of the data being used (with respect to the real world), and, more intuitively, to the real world data itself. It differs from data that is derived or inferred. The term comes from the field of earth sciences where it refers to the reality of conditions `on the ground' rather than collected data samples. The \textbf{gold standard} is a related term from statistics that refers to the convention for the best possible statistical test one can perform. What qualifies as the gold standard varies according to the context. In practice, it is shorthand for indicating that a statistical study conforms to the highest possible standard. The \textbf{curse of dimensionality} refers to the difficulty of generalising over a small amount of high-dimensional data. This is because the observation state space grows exponentially as features are added. We therefore need to increase our data exponentially to maintain good coverage. Automatic techniques such as principal components analysis (PCA) exist for \emph{dimensionality reduction} of data, in which the data is mapped to a lower-dimensional space representing the axes of highest variance (think \emph{trends}) of the data. Another option is to encourage model sparsity using $l_1$ regularisation. This tends to build models that ignore unimportant features. Dimensionality reduction can also be crudely achieved by removing model features. The \textbf{bias-variance tradeoff} refers to a result illustrating the expected behaviour of a parameter estimate, with respect to the `true' parameters. It may be shown that the MSE between an estimate and the true parameters, averaged over all possible data, depends both on the variance of the parameters around its own mean, and the bias (the gap between this estimate mean and the true parameters\footnote{The flaw here is that we require the structural assumptions of the model to be correct before we can talk about `true' parameters, which is rarely the case. This discrepancy is known as `structural error'. Even if the model structure is correct, there is an inescapable error of noise that all models share called the \emph{noise floor}. The noise floor is the limit of the learning curve (the trend of improvement in generalisation error as data is added) for an unbiased estimate.}. An unbiased estimate is a parameter model (for example MLE) that converges (in probability) to a hypothetical true parameter set as data increases, but it may entail a higher variance, making it more likely to draw a bad estimate some of the time. Hence, if we wish to be more certain about the optimality of our estimate, it may be prudent to allow some bias (for example through regularisation) if it sufficiently reduces the variance. In conclusion, the bias-variance tradeoff illustrates that on average it may benefit to use a ridge regression rather than OLS regression, even though this invokes a small bias.

\subsection{Bayes and the Beta-Binomial Model}

We begin by examining the primordial Beta-Binomial model--that which was studied by English mathematician, Thomas Bayes (1701-1761), in \emph{An Essay towards solving a Problem in the Doctrine of Chances}\footnote{This early term for probability theory comes from de Moivre's seminal textbook (1718).}. Here we consider the inference of data produced from a sequence of coin tosses, such as, $$0101001010111,$$ where $0$ represents `tails' and 1 `heads'. In fact, it is sufficient to know the number of heads and tails, which we denote $N_1$ and $N_0$ respectively. These are known as `sufficient statistics'. Note that the beta-binomial model is not a classifier, as we are not engaging in \emph{classification}, but rather we are predicting a single next outcome. The starting point is to define the likelihood. The probability distribution for such a random sequence is the discrete binomial distribution. As we are maximising for the rate parameter, we can drop the normalisation constant and we have,

$$p(\mathcal{D}|\theta) = \theta^{N_1}\cdot(1 - \theta)^{N_0}.$$

Clearly, we can maximise this expression by taking the derivative to give,

\begin{align}
N_1\theta^{N_1 - 1}(1 - \theta)^{N_0} - N_0\theta^{N_1}(1 - \theta)^{N_0 - 1} &= 0 \notag \\
&\implies \hat\theta_{MLE} = \frac{N_1}{N_0 + N_1} = \frac{N_1}{N}, \notag
\end{align}

where $N = N_0 + N_1$. The prior distribution is chosen to be \emph{conjugate} to the likelihood, that is, of like form. The continuous beta distribution is therefore a suitable choice, hence,

$$p(\theta) = \text{Beta}(\theta|a, b) \propto \theta^{a - 1}(1 - \theta)^{b - 1},$$ where we drop the normalising beta function for the proportionality. The values of $a$ and $b$ are hyper-parameters that control the skew. When $a = b = 1$, we have a uniform prior and the posterior is just the same as the likelihood. We multiply the prior and likelihood to obtain the posterior,

$$p(\theta|\mathcal{D}) \propto \theta^{N_1 + a}\cdot(1 - \theta)^{N_0 + b}.$$

Notice how the hyper-parameters behave as additional observations. For this reason they are known as \emph{pseudo-counts}--it is as though we had some additional data other than $\mathcal{D}$ that we can use to influence or stabilise the behaviour of the posterior. This is the power of a prior distribution. Now, the beta distribution distributes the continuous $\theta$, so this time we can simply look up the mode of a beta distribution to obtain the MAP estimate,

$$\hat\theta_{MAP} = \frac{N_1 + a - 1}{N + a + b - 2}.$$ Notice that as $N \to \infty$, the MAP estimate approaches the ML estimate. This shows that a prior serves its role of stabiliser only for smaller data sets. As data is added, it is overwhelmed by the volume of data fitted by the likelihood. Now, the posterior predictive distribution for unseen data, $\hat{x}$, is either given directly by the ML or MAP estimates, or else from Bayes model averaging,

\begin{align}
p(\hat{x} = 1|\mathcal{D}) &= \int_0^1 p(x = 1 |\theta)p(\theta|\mathcal{D})\mathop{dx} \notag \\
&= \int_0^1 \theta p(\theta|\mathcal{D})\mathop{dx} = \mathbb{E}[\theta|\mathcal{D}] = \frac{N_0 + a}{N + a + b}. \notag
\end{align}

The mean of a beta distribution differs from its mode, as it is asymmetric.

\subsection{Generative Classifiers}

Here we describe \emph{generative classifiers}, by illustrating how their components are derived with the naive Bayes classifier (NBC). A classifier is simply a model that predicts the membership of an observation, $x$, in one of a discrete number of classes. Generative classifiers `generate' the posterior distribution through inference, in contrast to \emph{discriminative} classifiers, which model the posterior distribution directly. The parameters of a NBC are found by considering the likelihood for $N$ data training samples, $\mathcal{D}$, of dimension $D$, which may be written,

\begin{align}
\textbf{likelihood} = p(\mathcal{D}|\theta) = p(\mathbf{X}, \mathbf{y}|\boldsymbol\theta) &= p(\mathbf{y}|\boldsymbol\theta)p(\mathbf{X}|\mathbf{y},\boldsymbol\theta) \notag \\
&= \textbf{class prior} \times \textbf{class conditional density} \notag \\
&= \prod_{i=1}^N p(y_i|\boldsymbol\pi)p(\mathbf{x}_i|y_i,\boldsymbol\theta), \label{eq:iid} \\
&= \prod_{i=1}^N p(y_i|\boldsymbol\pi)\prod_{j = 1}^D p(x_{ij}|y_i,\boldsymbol\theta_j), \label{eq:nbc}
\end{align}

where we denote the set of parameters for both input and output variables, $\boldsymbol\theta = \{\boldsymbol\pi, \boldsymbol\theta\}$. Step (\ref{eq:iid}) follows from the data being independent and identically distributed (a standard assumption), and step (\ref{eq:nbc}) from the naive Bayes assumption of feature independence when the class is given\footnote{This is called a naive assumption because it precludes covariance between model features, which in practice is usually present.} We take logarithms to derive the log-likelihood, a common trick that makes optimisation easier without changing the optimal parameters,

\begin{align}
\log p(\mathcal{D}|\boldsymbol\theta) &= \sum_{i=1}^N \log p(y_i|\boldsymbol\pi) + \sum_{i=1}^N\sum_{j = 1}^D\log p(x_{ij}|y,\boldsymbol\theta_j) \notag \\
&= \sum_{c} N_c\log \boldsymbol\pi_c + \sum_{c}\sum_{i:y_i=c}\sum_{j = 1}^D\log p(x_{ij}|y=c,\boldsymbol\theta_{jc}), \notag
\end{align}

where $N_c$ is the number of observations having class $c$. Now we have an expression that is easy to optimise, we can obtain the \textbf{maximum likelihood estimate} (MLE) by optimising each of the parts of the sum. This is equivalent to calculating the \emph{mode} of each of the probability functions. For the multinomial class variable, the task is simple, $\hat\pi_c = N_c/N.$ For the input parameters, the MLE depends upon the choice of distribution. One of the benefits of the naive independence assumption is it allows us to model any combination of distributions with ease--all we need do is substitute their probability functions. For illustration, we may simply opt for Bernoulli distributions on each of the features. Hence, the mean for feature $j$ given class $c$ is $\hat\theta_{jc} = N_{jc}/N_c$, where $N_{jc} = \sum_{i:y_i=c} x_{ij}$, noting the $x_{ij}$ are binary. The maximum likelihood estimate is therefore, $\hat{\boldsymbol\theta}_{MLE} = \{\hat\pi_c, \hat\theta_{jc} : 1\leq c\leq C, 1\leq j\leq D\}$. This is now enough to make a prediction. The posterior predictive distribution derives from,

\begin{align}
\textbf{posterior predictive} = p(y = c|\mathbf{x},\boldsymbol\theta) 
&= \frac{p(y = c|\boldsymbol\theta)p(\mathbf{x}|y=c,\boldsymbol\theta)}{\sum_{c'}p(y = c'|\boldsymbol\theta)p(\mathbf{x}|y=c',\boldsymbol\theta)} \notag \\
&\propto p(y = c|\boldsymbol\theta)p(\mathbf{x}|y=c,\boldsymbol\theta), \label{eq:postpred}
\end{align}

having the same form as (\ref{eq:nbc}). For a test observation, $\hat{\mathbf{x}}$, we can simply cycle through each of the classes, plug the appropriate parameters into (\ref{eq:postpred}), and compute a probability. We can then predict the classification of $\mathbf{x}$ to be the class $c$ having maximum probability. This is called a \textbf{plug-in approximation}, and it is the first of two alternatives for prediction. We will come to the other shortly, however, there are several more things we can do with plug-in approximation. To begin with, we may wish to include \emph{prior} information on the parameters. Thus, we form the prior distribution,

\begin{align}
\textbf{prior} = p(\boldsymbol\theta) = p(\boldsymbol\pi)\prod_c\prod_{j=1}^Dp(\theta_{jc}). \notag
\end{align}

The prior imposes a distribution on the parameter set, which has a stabilising effect when the training data is limited, mitigating overfitting. We choose the prior distributions on the parameters to be \emph{conjugate} to the likelihood distributions, that is, having a similar form. This is by no means compulsory, but it makes the derivations easier. Since we have a multinomial distribution on $y$ and Bernoulli distributions on $x|y=c$, we choose Dirichlet and beta distributions respectively for the prior parameters. Thus, $\boldsymbol\pi \sim \text{Dir}(\boldsymbol\alpha)$ and $\boldsymbol\theta_j \sim \text{Beta}(\beta_0, \beta_1)$. Choosing $\boldsymbol\alpha = \mathbf{1}$ and $\boldsymbol\beta = \mathbf{1}$ corresponds to a common practice known as \emph{add one} smoothing.

If we combine the prior with the likelihood, we get the posterior distribution, which strikes a balance between what the data tells us and what we expect in advance,

\begin{align}
\textbf{posterior} = p(\boldsymbol\theta|\mathcal{D}) &\propto \textbf{likelihood} \times \textbf{prior} \notag \\
&= \prod_{n=1}^N\text{Cat}(y_i|\boldsymbol\pi)\text{Dir}(\boldsymbol\pi;\boldsymbol\alpha)\prod_{c}\prod_{i:y_i = c}\prod_{j = 1}^D \text{Ber}(x_{ij}|\theta_{jc})\text{Beta}(\theta_{jc};\beta_0, \beta_1), \notag \\
&= p(\boldsymbol\pi|\mathcal{D})\prod_{j = 1}^D p(\boldsymbol\theta_{j}|\mathcal{D}), \notag
\end{align}

where $p(\boldsymbol\pi|\mathcal{D}) = \text{Dir}(N_1 + \alpha_1, \dots, N_C + \alpha_C)$ is the posterior on $\boldsymbol\pi$, and $p(\boldsymbol\theta_{j}|\mathcal{D}) = \text{Beta}((N_c - N_{jc}) + \beta_0, N_{jc} + \beta_1)$ is the posterior on $\boldsymbol\theta_{j}$. Note that because we are optimising for $\boldsymbol\theta$, it does not matter that the expression is only proportional to the posterior. Also note that if we have a uniform prior, the probabilities are constant, and the posterior is equivalent to the likelihood. Now we have two options: firstly, we can compute the mode of this posterior distribution giving us the \textbf{maximum a posteriori estimate}, $\hat{\boldsymbol\theta}_{MAP}$. This can then be plugged into the posterior predictive, just as with the MLE. The alternative is to use \textbf{Bayes model averaging}. This calculates the posterior as a weighted average of the distribution. In this scheme, neither the ML or MAP estimates are computed directly, rather, we sum or integrate over the unknown parameters, marginalising them,

\begin{align}
p(y = c|\hat{\mathbf{x}}, \mathcal{D}) &= \frac{p(\hat{\mathbf{x}}|y = c, \mathcal{D})p(y=c|\mathcal{D})}{\sum_{c'}p(\hat{\mathbf{x}}|y = c', \mathcal{D})p(y=c'|\mathcal{D})} \notag \\ &\propto p(\hat{\mathbf{x}}|y = c, \mathcal{D})p(y=c|\mathcal{D}) \notag \\
&= \int p(\hat{\mathbf{x}}, \boldsymbol\theta|y = c, \mathcal{D})\mathop{d\boldsymbol\theta}\int p(y=c, \boldsymbol\pi|\mathcal{D})\mathop{d\boldsymbol\pi} \label{eq:ltp} \\
&= \int p(\hat{\mathbf{x}}|y = c, \boldsymbol\theta, \cancelto{}{\mathcal{D}})p(\boldsymbol\theta|\cancelto{}{y = c}, \mathcal{D})\mathop{d\boldsymbol\theta}\int p(y=c|\boldsymbol\pi, \cancelto{}{\mathcal{D}})p(\boldsymbol\pi|\cancelto{}{y = c}, \mathcal{D})\mathop{d\boldsymbol\pi}, \label{eq:cancel}
\end{align}

where step (\ref{eq:ltp}) comes form the law of total probability and the cancellations in step (\ref{eq:cancel}) occur because the prediction is independent of the data when conditioned on the parameters. Thus we are averaging by the posterior distribution. This may be solved analytically for the choice of distributions we have made. Now, a retrospective connection to plug-in approximation may be seen. Plug-in approximation works on the assumption that the distribution on the parameters, $p(\theta|\mathcal{D}) \to \delta_{\hat{\theta}_{MAP}}(\theta)$ as $|\mathcal{D}| \to \infty$. This follows the intuition that increasing data increases our certainty about the parameters. If we replace the posteriors in the integral with the limit, then by the sifting property of the Dirac delta function, we obtain the class conditional and class prior distributions, conditioned on the MAP estimate--the plug-in approximation. A simple implementation of the naive Bayes classifier is given in \texttt{nbcDemo.m}. 

\subsection{Multivariate Normal Distribution}
The multivariate Normal (MVN) or multivariate Gaussian distribution is the multidimensional generalisation of the univariate Gaussian distribution. The form of the probability density function for a $D$-dimensional Gaussian is defined to be,

$$\mathcal{N}(\mathbf{x} ; \mu, \Sigma) \triangleq \frac{1}{(2\pi)^{D/2}|\boldsymbol\Sigma|^{1/2}}
\exp\bigg[-\frac{1}{2}(\mathbf{x} - \boldsymbol\mu)^T\boldsymbol\Sigma^{-1}(\mathbf{x} - \boldsymbol\mu)\bigg]
$$

where the vector $\boldsymbol\mu$ is the vector of means and $\boldsymbol\Sigma$ is the covariance matrix. The exponent expresses the Mahalanobis distance (named after Indian statistician Prasanta Chandra Mahalanobis (1893-1972)), which captures the behaviour of mean square error in $D$ dimensions. The covariance matrix is by definition symmetric positive semi-definite, and may be diagonalised with a diagonal matrix of eigenvalues, $\boldsymbol\Lambda$, and orthonormal eigenvectors, $\mathbf{U}$, as $\boldsymbol\Sigma = \mathbf{U}\Lambda\mathbf{U}^T$ where orthonormality implies $\mathbf{U}^{-1} = \mathbf{U}^T$. Consequently,

$$\boldsymbol\Sigma^{-1} = \mathbf{U}^T\Lambda^{-1}\mathbf{U} = \sum_{d=1}^D\frac{1}{\lambda_d}\mathbf{u}_d\mathbf{u}_d^T,$$

where $\mathbf{u}_d$ are the columns of $\mathbf{U}$. Therefore, the Mahalanobis distance can be expressed as,

$$(\mathbf{x} - \boldsymbol\mu)^T\Sigma^{-1}(\mathbf{x} - \boldsymbol\mu) = \sum_{d=1}^D\frac{y_d^2}{\lambda_d^2},$$

where $y_i = \mathbf{u}_i^T(\mathbf{x} - \boldsymbol\mu)$, from which it may be seen that events of equal probability lie along elliptical contours, for which the eigenvectors form the axes and eigenvalues control the distortion. Fitting a MVN from data $\mathcal{D}$ of size $N$ can be done with maximum log-likelihood estimation. With a bit of vector calculus, the MLE is,

$$\hat{\boldsymbol\mu}_{MLE} = \frac{1}{N}\sum_{n=1}^N\mathbf{x}_n,$$

that is, the sample mean and,

$$\hat{\boldsymbol\Sigma}_{MLE} = \frac{1}{N}\sum_{n=1}^N(\mathbf{x}_i - \boldsymbol\mu)(\mathbf{x}_i - \boldsymbol\mu)^T,$$

the sample covariance matrix.

\subsection{Gaussian Discriminant Analysis}

Gaussian discriminant analysis is a family of classifiers that fit a Gaussian distribution to each of $K$ classes. That is, the class-conditional densities are defined to be,

$$p(\mathbf{x} | y = c) = \mathcal{N}(\mathbf{x} ; \hat{\mu}_c, \hat\Sigma_c),$$

where the parameters come from the maximum likelihood for multivariate normal distributions (MVN). Where the covariance matrix is diagonal, the features are independent, and this is equivalent to the naive Bayes classifier. This sort of modelling, where we learn $K$ Gaussian densities and predict membership of a test observation is effectively a generalisation of a nearest centroids classifier\footnote{A nearest centroids classifier is one of the simplest classifiers imaginable. Training is performed by computing centroids for points in each class of training data, $\boldsymbol\mu_c = 1/|c|\sum_{i:y_i=c}\mathbf{x}_i$. A prediction for unseen data $\hat{\mathbf{x}}$ is then made as $\hat{y} = \min_c ||\hat{\mathbf{x}} - \boldsymbol\mu_c||$, that is, the class of the nearest centroid.}.

\subsubsection{Quadratic Discriminant Analysis}

The posterior distribution can then be defined as,

$$p(y = c|\mathbf{x}, \theta) = \frac{\pi_c(2\pi)^{-D/2}|\boldsymbol\Sigma|^{-1/2}\exp\big[-\frac{1}{2}(\mathbf{x} - \mathbf{\mu}_c)^T\boldsymbol\Sigma_c^{-1}(\mathbf{x} - \boldsymbol\mu_c)\big]}{\sum_{c'}\pi_{c'}(2\pi)^{-D/2}|\boldsymbol\Sigma|^{-1/2}\exp\big[-\frac{1}{2}(\mathbf{x} - \boldsymbol\mu_{c'})^T\boldsymbol\Sigma_{c'}^{-1}(\mathbf{x} - \boldsymbol\mu_{c'})\big]},$$

where $\pi_c$ is the class prior. It may be shown (though we will not do so here) that a decision rule\footnote{A decision rule is a probability threshold for classification, usually equal to 0.5 in binary classification.} creates a quadratic boundary between the centroids in Euclidean space. Notice finally that we are effectively doing a plug-in approximation, in the absence of Bayes model averaging.

\subsubsection{Linear Discriminant Analysis}

Linear discriminant analysis (LDA) arises from QDA when the simplifying assumption is made that the covariance matrices are all equal. In this instance we have,

$$p(y = c|\mathbf{x}, \theta) \propto \exp\Big[\boldsymbol\mu_c^T\Sigma^{-1}\mathbf{x} - \frac{1}{2}\boldsymbol\mu_c^T\boldsymbol\Sigma^{-1}\boldsymbol\mu_c + \log\pi_c\Big]\exp\Big[-\frac{1}{2}\mathbf{x}^T\boldsymbol\Sigma^{-1}\mathbf{x}\Big],$$

and the quadratic term cancels out over the sum, leaving an expression that is linear in $\mathbf{x}$. It can be shown easily that this produces linear decision boundaries. There are some interesting connections between LDA and other parts of machine learning. If we define $\gamma_c = - \frac{1}{2}\mu_c^T\Sigma^{-1}\mu_c + \log\pi_c$ and $\beta_c = \Sigma^{-1}\mu_c$, we can write,

$$p(y = c|\mathbf{x}, \theta) = \frac{e^{\beta_c^T\mathbf{x} + \gamma_c}}{\sum_{c'}e^{\beta_{c'}^T\mathbf{x} + \gamma_{c'}}} = \mathcal{S}({\eta})_c,$$

where $\eta_c = [\beta_1^T\mathbf{x} + \gamma_1, \beta_2^T\mathbf{x} \gamma_2 , \dots, \beta_C^T\mathbf{x} + \gamma_C]$, and $\mathcal{S}$ is the \emph{softmax} function\footnote{The softmax function is so called because at low `temperatures', that is, if we divide all the exponents by $T$, the probability of the most likely class goes to $1$ as $T \to 0$. This terminology comes from statistical physics--in fact the softmax function has the same form as the Boltzmann distribution (Ludwig Boltzmann (1844-1906) was a German physicist credited with the development of statistical mechanics).} The marginal likelihood is known as the partition function, denoted $Z$ for `Zustandssumme'--the German expression for `sum over states'. This form of LDA is very similar to logistic regression, differing only in the fact that LDA is generative and logistic regression is discriminative. If we normalise a naive Bayes classifier with Gaussian features, it is equivalent to LDA--hence naive Bayes and logistic regression form what is known as a generative-discriminative pair.

\subsection{Linear Regression}

A linear regression is used to make predictions for a continuous variable.

\subsubsection{Constructing a Loss Function}
The starting point for building a regression model is a dataset, $\mathcal{D}$, where,

$$\mathcal{D} = \{(\mathbf{x}_n, y_n)\}_{n=1}^{N},$$

that is, a set of $N$ data points, where $\mathbf{x}_i \in \mathbb{R}^D$ is a $D$-dimensional \emph{input} vector, and $y_i \in \mathbb{R}$ are the corresponding scalar outputs. The dataset, $\mathcal{D}$, is known as our \emph{training set}. The objective of the regression algorithm is to infer the linear function that best fits the data. More precisely, we want to find the vector of coefficients, $\boldsymbol\beta = [\beta_1, \beta_2, \dots, \beta_D]$, that gives the best approximation for,

\begin{align}
y_n &\approx \beta_1x_{n1} + \beta_2x_{n2} + \cdots + \beta_Dx_{nD} \notag \\
&= \mathbf{x}_n^T\boldsymbol\beta, \notag
\end{align}

for all $n = 1, \dots, N$. Each of the $D$ dimensions of $\mathbf{x_n}$ is known as an indicator. The greater the corresponding coefficient in the parameter vector, $\boldsymbol\beta$, the more weight this dimension has in determining the output, $y_n$. We can formulate the above as an optimisation problem,

$$\min_{\boldsymbol\beta}\mathcal{L}(\boldsymbol\beta) = \frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta) = \frac{1}{2}\sum_{n=1}^{N}(y_n - \mathbf{x_n}^T\boldsymbol\beta)^2,$$

where $\mathcal{L}(\boldsymbol\beta)$ is the `loss' function, expressing a \emph{mean square error} (MSE) for choosing parameter vector $\boldsymbol\beta$, that is, the average squared distance of the approximation from the true value. Note the similarity to the variance statistic. It is the job of the learning algorithm to minimise this function. It is also typical to write $\text{MSE} = \text{RSS}/N$, where RSS stands for \emph{residual sum of squares}, the non-weighted sum. Note we could have chosen any sort of loss function, but our reasons for choosing mean square error will be revealed in the following section.

\subsubsection{Method of Least Squares}

The method of least squares (MLS) is a closed-form solution to a linear regression. As previously stated, it was first published in the same paper by Gauss that also first formalised the Normal distribution. We will discover the link in the following section. We may write the derivative of the loss function,

$$\frac{\delta\mathcal{L}}{\delta\boldsymbol\beta} = -\frac{1}{N}X^T(\mathbf{y} - X\boldsymbol\beta),$$

where $\mathbf{X} \in \mathbb{R}^{N\times D}$ is the matrix whose $i$th row is the $i$th $D$-dimensional vector, $\mathbf{x}_i$, and $\mathbf{y} \in \mathbb{R}^D$ is the vector whose $i$th element is the $i$th scalar, $y_i$. We can therefore minimise $\mathcal{L}(\boldsymbol\beta)$ by setting its derivative to 0, giving the normal equation,

$$\mathbf{X}^T(\mathbf{X}\boldsymbol\beta - \mathbf{y}) = 0,$$

and finally, the optimal parameter vector,

$$\boldsymbol\beta^* = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.$$

MLS also has a nice geometric interpretation. According to the normal equation, the error, $\mathbf{X}\boldsymbol\beta - \mathbf{y}$, is minimised when it is perpendicular to the hyperplane defined by $\mathbf{X}^T$. This makes sense, because it means the choice of prediction, $\mathbf{X}\boldsymbol\beta$, that minimises the distance to the true value, $y$, is the one directly `below' it on the hyperplane, $\mathbf{X}^T$. This solution is known as the ordinary least squares (OLS) solution.

\subsubsection{The Gram Matrix}

For the closed-form method of least squares, we require the \emph{gram} matrix $\mathbf{X}^T\mathbf{X}$ to be invertible. If $\mathbf{X}$ is full rank\footnote{A matrix is full rank if its columns are linearly independent. By the rank-nullity theorem, full rank implies a zero-dimensional null space or kernel.} then $\mathbf{X}\mathbf{a} = \mathbf{0}$ iff $\mathbf{a} = \mathbf{0}$. Therefore, $\mathbf{a}^T\mathbf{X}^T\mathbf{X}\mathbf{a} \geq 0$, implying the gram matrix is positive-definite, and therefore invertible. Note that when $N < D$ and we have a `fat' $\mathbf{X}$, $\mathbf{X}$ is often rank deficient. Even when the gram matrix is invertible, it may still be ill-conditioned. This arises when a high degree of \emph{multilinearity} is present in the data. This refers to high degrees of correlation between features. Ill-conditioning is a concept common in numerical analysis, and in linear algebra it is captured by the \emph{condition number} of a matrix. This provides an upper bound on the relative error between solutions for changes in the data. A high condition number indicates parameters may vary significantly for small changes in the data. This can lead to problems in practice, when floating point arithmetic introduces small inaccuracies. It can be shown that the condition number, $\kappa({\mathbf{A}}) = ||\mathbf{A}||\cdot||\mathbf{A}^{-1}|| = \boldsymbol\sigma_{max}/\boldsymbol\sigma_{min}$, that is, the ratio of the largest and smallest eigenvalues of $\mathbf{A}$. Geometrically, this makes sense. Consider the way a matrix transforms a unit circle of vectors in two dimensions. When one eigenvalue dominates the other, vectors are mostly `stretched' in the direction of the first eigenvector. We would find that a small change in the direction of the second eigenvector corresponds to a much larger change in the direction of the first. `Lifting' the eigenvalues (thereby reducing the condition number) is a convenient outcome of ridge regression. This is easily seen by considering the eigenvalue decomposition of the gram matrix, then $\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I} = \mathbf{U}\boldsymbol\Sigma\mathbf{U}^T + \lambda\mathbf{I} = \mathbf{U}(\boldsymbol\Sigma + \lambda\mathbf{I})\mathbf{U}^T$. It can now be seen that a ridge regression improving its condition number through regularisation is the means by which it reduces variance in the bias-variance tradeoff.

\subsubsection{Gradient Descent}

As an alternative to closed-form solutions, there are sure-fire numerical methods. The most fundamental of these is the method of gradient descent. The convenient form of our loss function makes it a differentiable function that is furthermore \emph{convex}. It therefore has a unique global minimum. We can approach this optimal point iteratively from an arbitrary starting point by taking steps in the direction of the gradient (Figure \ref{fig:gradientdescent}). The algorithm is defined as,

$$\boldsymbol\beta^{k+1} = \boldsymbol\beta^{k} - \alpha \nabla\mathcal{L}(\boldsymbol\beta^k),$$

where $\alpha$ is the step size parameter and $\nabla\mathcal{L}(\boldsymbol\beta^k) = \frac{\delta\mathcal{L}(\boldsymbol\beta_k)}{\delta\boldsymbol\beta}$. Gradient descent can be enhanced with a line search to find the optimal step size for each search direction. When used, the algorithm exhibits a zig-zag path to the optimal point, as each step is perpendicular to the last. Gradient descent also has more sophisticated variants such as Newton's method and quasi-Newton methods, whose application may be warranted for larger data sets. One alternative to gradient descent is the conjugate gradient method, an iterative technique related to direct method Cholesky decomposition for symmetric positive definite matrices, in which the solution vector is built up iteratively as a linear combination of a set of mutually conjugate vectors. This approach may be advantageous in sparse linear systems.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{Figures/gradientdescent.png}
\caption{The method of gradient descent iteratively approaches a global minimum on a convex function.\cite{gradientdescent}}
\label{fig:gradientdescent}
\end{figure}

\subsubsection{Model Prediction}

Once we have computed our optimal model parameters, $\boldsymbol\beta^*$, we can start making predictions for unseen data, $\hat{\mathbf{x}}$. Our prediction is then,

$$\hat{y} = \hat{\mathbf{x}}^T\boldsymbol\beta^*.$$

\subsubsection{Maximum Likelihood Estimation}

Now we will derive our regression model from a probabilistic angle. Probabilistic constructions are preferred in machine learning because they make the models more comprehensible\footnote{For some techniques, however, such as neural networks, things are not so straightforward.}. We start our derivation with the following assumptions: firstly, that the data points, $(\mathbf{x}_n, y_n)$ are independent and identically distributed (i.i.d); second, that the output variable is modelled by a linear function with Gaussian noise, that is, $\mathbf{y} = X\boldsymbol\beta + \boldsymbol\epsilon$, where $\boldsymbol\epsilon$ is a vector of normally distributed random variables, $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$, for which $\sigma^2$ is the variance representing the random noise. We are thereby making an assumption about how the output variable is distributed around our line of best fit, $\mathbf{x}_n^T\boldsymbol\beta$. Now we may write a probability distribution for $y_n$,

$$p(y_n|\mathbf{x}_n, \boldsymbol\beta) = \mathcal{N}(\mathbf{x}_n^T\boldsymbol\beta, \sigma^2),$$

and since the data points are independent, the distribution over all $y_n$ is,

$$p(\mathbf{y}|X, \boldsymbol\beta) = \prod_{n=1}^N p(y_n|\mathbf{x}_n, \boldsymbol\beta).$$

As we will soon see, maximising this probability is equivalent to minimising the square loss function! Because of our assumption of Gaussian noise, the optimal parameters will be those that construct a hyperplane such that the data points around it are distributed according to a Normal distribution, that is, maximising the likelihood of the data around the plane. This is equivalent to minimising the square loss. As we shall see, it is convenient to work with the log of the likelihood. Since a $\log$ function is a monotonically increasing function, maximising the log-likelihood is no different to maximising the likelihood itself. Therefore we have,

\begin{align}
\max_{\boldsymbol\beta}\mathcal{L}_{lik}(\boldsymbol\beta) &= \max_{\boldsymbol\beta}\Big\{\log \prod_{n=1}^N \mathcal{N}(\mathbf{x}_n^T\boldsymbol\beta, \sigma^2)\Big\} \notag \\
&=\max_{\boldsymbol\beta}\Big\{\sum_{n=1}^N \log \frac{1}{\sigma\sqrt{2\pi}}e^{-(y - \mathbf{x}_n^T\boldsymbol\beta)^2/2\sigma^2}\Big\} \notag \\
&=\max_{\boldsymbol\beta}\Big\{-\frac{1}{2\sigma^2}\sum_{n=1}^N (y - \mathbf{x}_n^T\boldsymbol\beta)^2 + \log\frac{N}{\sigma\sqrt{2\pi}}\Big\} \notag \\
&=\max_{\boldsymbol\beta}\Big\{-k_1\mathcal{L}(\boldsymbol\beta) + k_2\Big\} \notag \\
&=\min_{\boldsymbol\beta}\mathcal{L}(\boldsymbol\beta)\notag
\end{align}

where $k_1$ and $k_2$ are positive constants, and so they have no bearing on the optimal $\boldsymbol\beta^*$. Thus, we see that maximising our likelihood function is equivalent to minimising our original mean square error loss function. This is equivalently known as the negative log-likelihood (NLL).

\subsubsection{Non-linear Fits}

If we wish to fit a non-linear function to our data, we can simply transform the data. For example, if we want to fit a quadratic function to our data (Figure \ref{fig:quadraticfit}), we need only square the values in our dataset and introduce it as a new dimension in our data. It is important to note that this is still a linear regression, as we still have a linear combination of parameters, albeit with a non-linear basis function. These more complex models are best created in the framework of a \emph{ridge regression}, where we introduce a penalty term in the loss function.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{Figures/quadraticfit.pdf}
\caption{A non-linear fit may be found by transforming the data.}
\label{fig:quadraticfit}
\end{figure}

\subsubsection{Bayesian Linear Regression}

Bayes model averaging may be performed for linear regression to obtain a more robust posterior predictive distribution, just as we saw for generative classifiers. Such a result can for example be analysed for its variance, unlike a mere point estimate such as ML or MAP estimates. The derivation involves multiplying and integrating Gaussians and belongs in a textbook, rather than in Last Chance Stats.

\subsubsection{Robust Linear Regression}

One drawback of using a Gaussian likelihood is that it is sensitive to outliers. This makes it likely to overfit in practice. There are several ways of mitigating this effect. The most crude is known as `early stopping', where training is terminated before convergence, and before the model has a chance to overfit. The most ideal option is to train on more data, taking density away from outliers, but data is usually hard to come by. The most common approach is to \emph{regularise} the cost function by introducing a prior distribution on the model parameters. This gives us the posterior distribution, just as it was with the beta-binomial model, and the optimal parameters are the MAP estimate, $\boldsymbol\theta_{MAP} = \argmax_{\boldsymbol\theta} \text{NLL}(\boldsymbol\theta)$, where NLL is the negative log-likelihood. If we choose another Gaussian for the prior, we are using $l_2$ regularisation, and we get what is known as a \emph{ridge regression},

$$\text{NLL} = \frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^T(\mathbf{y} - \mathbf{X}\boldsymbol\beta) + \frac{\lambda}{2}\boldsymbol\beta^T\boldsymbol\beta = \frac{1}{2}\sum_{n=1}^N(y_n - \boldsymbol\beta^T\mathbf{x}_n)^2 + \frac{\lambda}{2}\sum_{d=1}^D\beta_d^2.$$

Clearly, this is a linear regression with an extra cost over the parameter values. This is known as a penalty term in the cost function. High parameter values, which correspond to more complex fits (overfitting), are punished with a quadratic cost. The parameter $\lambda$ is a scaling constant which may be optimised through cross validation. Ridge regression is very popular, and has the added bonus of producing a more numerically stable normal equation, because the regularisation term increases the eigenvalues of the gram matrix, $\mathbf{X^T}\mathbf{X}$, in the normal equations, thereby reducing the ratio between the highest and lowest eigenvalues, and thereby improving its condition number. Another option is a Laplace prior. This is known as $l_1$ regularisation, and when applied to regression is known as a LASSO\footnote{Least absolute shrinkage and selection operator} regression. One of the effects of $l_1$ regularisation is to encourage \emph{sparse} models. Sparsity refers to models whose parameters are non-zero for only a small subset of the dimensions. Encouraging sparsity is a form of \emph{feature selection}. This is useful for high-dimensional data with many noisy indicators, otherwise known as the `small $N$, large $D$' problem. The intuition behind this phenomenon of encouraging sparsity is that the contours of the prior distribution are linear, forming a diamond shape in the parameter space, whose vertices lie along each of the axes. In contrast, the Gaussian $l_2$ prior has ellipsoidal contours. Thus, in $l_1$ regularisation sparse parameters on the axes have greater magnitude than parameters along the edges of the diamond. These sparse parameters with greater magnitude (allowing for better fits) may be chosen at no extra penalty. Unfortunately, the cost function is no longer differentiable everywhere, due to the absolute value function, and our solution algorithm changes--the most common approaches are coordinate descent algorithms\footnote{Coordinate descent algorithms are similar to gradient descent, but where at each step the function is optimised with respect to one variable.} such as the `shooting' algorithm.

A final, more drastic option for robust regression is to modify the form of the likelihood to a distribution with heavier tails, making it more robust to outliers, thereby creating to \emph{robust linear regression}. Typical choices are the Student or Laplace distributions. In the latter case, the function is no longer differentiable, but with a reformulation trick can be made into a linear programming problem. For example, given a Laplace likelihood,

$$\text{NLL}(\mathbf{w}) \propto -\log\prod_n\exp\big({-|y_n- \mathbf{w}^T\mathbf{x}_n|}\big) = \sum_n |y_n- \mathbf{w}^T\mathbf{x}_n|.$$

We can remove the absolute value sign using the \emph{split variable trick}, that is, by introducing artificial positive and negative variables, $r_n^+$ and $r_n^-$, such that,

$$\text{NLL}(\mathbf{w}) =  \sum_n r_n^+ + r_n^-,$$

where $r_n^+ - r_n^- = y_n- \mathbf{w}^T\mathbf{x}_n$ with the \emph{type} constraint that $r_n^+r_n^- = 0$ (ensuring only one variable is activated at a time), and $r_n^+, r_n^- \geq 0$. The type constraint can be omitted, as at optimality it will be satisfied as a matter of course. Now we have a constrained convex optimisation problem, which can be optimised with any linear solver, for example, the simplex algorithm.

\subsubsection{The Simplex Algorithm}

The simplex algorithm was designed by American mathematician George Dantzig\footnote{There is famous anecdote from Dantzig's time as a doctoral student at Berkley. Arriving late to class, Dantzig copied down two important unsolved statistics problems the professor had earlier written on the blackboard, taking them to be set for homework. With some effort, he managed to solve both problems, and submitted them to the professor for review. It was not until some weeks later that the professor looked at the solutions, to great excitement. Dantzig then had the rare privilege of submitting his homework as his doctoral thesis.} (1914-2005). It is an optimisation algorithm for linear programming problems, that is, a problem of the form,

\begin{align}
\begin{array}{rl}
\text{maximise} & z = \mathbf{c}^T\mathbf{x} \\
\text{subject to} & \mathbf{A}\mathbf{x} = \mathbf{b}, x_i \geq 0
\end{array}
\label{eq:linprog}
\end{align}

where $z$ is known as the objective function, and the linear system specifies a set of linear constraints on the values of the input variables, $x_i$, which are further non-negative. The program specifies an N-dimensional convex solid\footnote{If we take a spherical fruit such as an apple to be our unconstrained convex function, adding a linear constraint corresponds to slicing it at some position at a fixed angle. Intuitively, the fruit remains convex after the cut, despite losing its smoothness (roundness).}, and the algorithm works by moving along the edges of this shape from corner to corner. As any optimal solution must be on the surface, we can discard all points interior to the solid. This reduces the feasible solution space to a finite set. Furthermore, the convexity of the solid guarantees that as long as we repeatedly take upward steps along the surface edges, we will arrive at a maximum in a finite number of steps. In practice the algorithm is very efficient, though there are a special class of problems for which the algorithm will run in exponential time. Now, take for example the simple program,

$$
\begin{array}{rl}
\text{maximise} & z = 2x_1 + x_2 \\
\text{subject to} & x_1 + x_2 \leq 5 \\
& 5x_1 + 2x_2 \leq 10 \\
& x_1, x_2 \geq 0
\end{array},
$$

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{Figures/simplex.pdf}
\caption{The convex surface of a simple linear programming problem.}
\label{fig:simplex}
\end{figure}


depicted in Figure \ref{fig:simplex}. The first phase of optimisation involves reformulating the linear program into the \emph{standard} form expressed in (\ref{eq:linprog}). First, any unrestricted variables are eliminated from the program. Then, any variable with a non-zero lower bound is replaced with a variable specifying an offset. Finally, \emph{slack} variables are introduced to change any inequality constraint to an equality constraint. In our problem, two slack variables are required giving,

$$
\begin{array}{rl}
\text{maximise} & z = 2x_1 + x_2 \\
\text{subject to} & x_1 + x_2 + s_1 = 5 \\
& 5x_1 + 2x_2 + s_2 = 10 \\
& x_1, x_2, s_1, s_2 \geq 0
\end{array}.
$$

The convenient way to apply the algorithm is to write the problem into a \emph{tableau}. This can be written,

$$
\begin{array}{rrrrr|r}
x_1&x_2&s_2&s_2&z&c \\
\hline
1&1&1&0&0&5 \\
\mathbf{5}&2&0&1&0&10 \\
\hline
-2&-1&0&0&1&0 \\
\end{array}
$$

The tableau form permits the execution of the algorithm to follow a sequence of simple row reductions. We begin by considering the final line of the tableau. This indicates the rate of increase of the objective function with respect to a change in each variable. As a simple heuristic, we select the variable yielding the greatest increase--$x_1$. We then select the constraint which most restricts increasing this variable--here, the second. The choice of row and column defines the \emph{pivot} variable for the current iteration. The task now is to render the $x_1$ column an elementary vector with the unit in the pivot position. This can be achieved first by scaling the second row by $1/5$ ($R_2^* \leftarrow \frac{1}{5}R_1 $), then subtracting it from the first row ($R_1^* \leftarrow R_1 - R_2^*$), and adding twice that to the final row ($R_3^* \leftarrow R_3 + 2R_2^*$), yielding,

$$
\begin{array}{rrrrr|r}
x_1&x_2&s_2&s_2&z&c \\
\hline
0&\mathbf{3/5}&1&-1/5&0&3 \\
1&2/5&0&1/5&0&2 \\
\hline
0&-1/5&0&2/5&0&4 \\
\end{array}
$$

Now we are at a point on the surface $(2, 0, 4)$ at which increases in $x_1$ yield $0$ improvement to $z$. Hence, we are at a corner on the surface. The presence of negative coefficients in the final row indicates further improvement can be made. Identifying the next pivot element and repeating the process gives,

$$
\begin{array}{rrrrr|r}
x_1&x_2&s_2&s_2&z&c \\
\hline
0&1&5/3&-1/3&0&5\\
1&0&-2/3&1/3&0&0\\
\hline
0&0&1/3&-1/15&0&5\\
\end{array}
$$

Now we see the final row contains non-negatives for $x_1$ and $x_2$, therefore no improvement can be made by varying either variable, and hence we have arrived at an optimal solution. Because the state of $R_3$ (the objective function) has been arrived at as a combination of its previous state and the new first constraint, $R_1$, any solution which satisfies the under-determined $R_1$ satisfies its contribution to $R_3$. Hence, we choose the trivial solution, $(x_1, 5, 0, 0, 0)$. This may not always be the only optimal solution (though in our problem it is), but it is indeed optimal. Because of the linearly independent basis vectors, this leaves the trivial solution available in the second constraint, $R_2$, that is, $(0, x_2, 0, 0, 0)$. Combining these yields the optimal solution, $x_1 = 0, x_2 = 5, s_1 = 0, s_2 = 0$, as seen in Figure \ref{fig:simplex}. Thus, in general, our method brings us to a state wherein the optimal solution can be read directly from the final tableau as the basic columns, with other variables equal to 0. As noted, this method verifiably gives us an optimal solution, though others may exist.

\subsection{Logistic Regression}

Logistic regression is a class of classification models. Learning a logistic regression can be thought of as fitting a multivariate Bernoulli distribution. There is also a strong connection between logistic regression and linear discriminant analysis. In fact, logistic regression models are the \emph{discriminative} form of the same model. Discriminative models fit a posterior distribution directly, unlike generative models, which derive the posterior by combining likelihood and prior distributions\footnote{This is often hard to do, especially when the input data is vector-valued, and is a limitation on generative models. In contrast, discriminative models allow for arbitrary basis function expansion and feature engineering. On the other hand, generative models are usually easier to fit, more robust to changes in the dataset (discriminative models must be retrained), and better facilitate semi-supervised learning.}. Logistic regressions are a core part of machine learning with ties to more sophisticated techniques such as neural networks. Studying logistic regressions notably presents the student with the quintessential optimisation techniques of machine learning. Linear and logistic regressions are part of a wider family of models called \emph{generalised linear models}. This is a family of models modelling exponential densities over output variables, and where the mean is a linear combination of the parameters, in some cases passed through a non-linear function.

\subsubsection{Binary Logistic Regression}

A binary logistic regression models,

$$p(y|\mathbf{x}, \boldsymbol\beta) = \text{Ber}(y|\sigma(\boldsymbol\beta^T\mathbf{x})),$$

where,

$$\sigma(\boldsymbol\beta^T\mathbf{x}) = \frac{\exp(\boldsymbol\beta^T\mathbf{x})}{1 + \exp(\boldsymbol\beta^T\mathbf{x})},$$

is the sigmoid or logistic function\footnote{This may be substituted by any function mapping $(-\infty, +\infty) \to [0, 1]$, for example the Gaussian CDF, giving what is known as a \emph{probit} regression. Recall these functions have roughly the same shape, but a Gaussian CDF may also be tuned by its variance.}. Thus, in training a logistic regression model we are fitting a multivariate Bernoulli distribution. In this binary case, we encode the class variable as a binary variable, that is, $y_i \in \{0, 1\}$, and we have $p(y = 1) = \sigma(\boldsymbol\beta^T\mathbf{x})$ and $p(y = 0) = 1 - \sigma(\boldsymbol\beta^T\mathbf{x})$. The negative log likelihood for our model is therefore,

\begin{align}
\text{NLL}(\boldsymbol\beta) &= -\log\prod_i \sigma(\boldsymbol\beta^T\mathbf{x}_i)^{y_i}(1 - \sigma(\boldsymbol\beta^T\mathbf{x}_i))^{(1 - y_i)} \notag \\
&= \sum_i \log(1 + \exp(\boldsymbol\beta^T\mathbf{x}_i)) - y_i\boldsymbol\beta^T\mathbf{x}_i.\notag
\end{align}

Due to the logistic function, it is no longer possible to obtain a least-squares solution. We therefore have recourse to numerical techniques. Because the function is convex, vanilla gradient descent will do the trick. However, more powerful techniques exist. Newton's method is a similar algorithm incorporating second order information into the descent step. It comes from considering the second order Taylor polynomial for the NLL function, around a point $\boldsymbol\beta_k$,

$$NLL(\boldsymbol\beta) \approx NLL(\boldsymbol\beta_k) + g_k^T(\boldsymbol\beta - \boldsymbol\beta_k) + \frac{1}{2}(\boldsymbol\beta - \boldsymbol\beta_k)\mathbf{H}_k(\boldsymbol\beta - \boldsymbol\beta_k).$$

This quadratic expression is minimised for $\boldsymbol\beta = \boldsymbol\beta_k - \mathbf{H}^{-1}g_k$. This constitutes the Newton step. Now we will derive expressions for the gradient and Hessian of our logistic regression model. The gradient is the vector of partial derivatives,

$$g(\boldsymbol\beta) = \begin{bmatrix}
\frac{\partial}{\partial \beta_0} \text{NLL}(\boldsymbol\beta) \\
\frac{\partial}{\partial \beta_1} \text{NLL}(\boldsymbol\beta) \\
\vdots \\
\frac{\partial}{\partial \beta_D} \text{NLL}(\boldsymbol\beta) \\
\end{bmatrix} =
\sum_i \begin{bmatrix}
(\sigma(\boldsymbol\beta\mathbf{x}_i) - y_i)x_{i1} \\
(\sigma(\boldsymbol\beta\mathbf{x}_i) - y_i)x_{i2} \\
\vdots \\
(\sigma(\boldsymbol\beta\mathbf{x}_i) - y_i)x_{iD} \\
\end{bmatrix}
= \sum_i (\sigma_i - y_i)\mathbf{x}_i = \mathbf{X}^T(\boldsymbol\sigma - \mathbf{y}),
$$

The Hessian is defined as $\frac{\partial}{\partial\mathbf{\boldsymbol\beta}}g(\boldsymbol\beta)^T$,

$$\mathbf{H} = \begin{bmatrix}
\frac{\partial}{\partial \beta_0} g(\boldsymbol\beta) \\
\frac{\partial}{\partial \beta_1} g(\boldsymbol\beta) \\
\vdots \\
\frac{\partial}{\partial \beta_D} g(\boldsymbol\beta) \\
\end{bmatrix} =
\sum_i\begin{bmatrix}
\sigma(\boldsymbol\beta\mathbf{x}_i)(1 - \sigma(\boldsymbol\beta\mathbf{x}_i))x_{i1}\mathbf{x}_i^T \\
\sigma(\boldsymbol\beta\mathbf{x}_i)(1 - \sigma(\boldsymbol\beta\mathbf{x}_i))x_{i2}\mathbf{x}_i^T \\
\vdots \\
\sigma(\boldsymbol\beta\mathbf{x}_i)(1 - \sigma(\boldsymbol\beta\mathbf{x}_i))x_{iD}\mathbf{x}_i^T
\end{bmatrix}
= \sum_i \sigma_i(1 - \sigma_i)\mathbf{x}_i\mathbf{x}_i^T = \mathbf{X}^T\mathbf{S}\mathbf{X},
$$

where $S = \text{diag}([\sigma(\boldsymbol\beta^T\mathbf{x}_1), \sigma(\boldsymbol\beta^T\mathbf{x}_2), \dots, \sigma(\boldsymbol\beta^T\mathbf{x}_N)])$. The Newton step may be rewritten as a least squares problem. In such a formulation, the optimisation algorithm is known as iterative reweighted least squares (IRLS). Due to the computational complexity of computing the Hessian, quasi-Newton techniques exist that create an approximation iteratively. The most common quasi-Newton method is the BFGS algorithm (Broyden, Fletcher, Goldfarb and Shanno). The $\mathcal{O}(D^2)$ space complexity of the Hessian gives rise to a further approximation in the limited memory or L-BFGS algorithm. This is the usual weapon of choice for modelling high-dimensional data. Just as with linear regression, $l_2$ regularisation may be used. Unlike linear regression, however, the full posterior distribution is not directly computable due to the absence of a conjugate prior. Approximation techniques, such as Markov chain Monte Carlo (MCMC), are therefore used. An object-oriented logistic regression model is solved with both gradient descent and Newton's method in \texttt{logRegDemo.m}.

\subsubsection{Multinomial Logistic Regression}

Going beyond binary logistic regression requires some adjustments--the sigma function is replaced by the generalised softmax function,

$$p(y_i = c | \mathbf{x}_i, \mathbf{W}) = \frac{\exp(\mathbf{w}_c^T\mathbf{x})}{\sum_{c' = 1}^C \exp(\mathbf{w}_{c'}^T\mathbf{x})},$$

just as with linear discriminant analysis. We now have the generalised multinomial or softmax regression. The parameters now make up a matrix $\mathbf{W}$ whose $C$ columns correspond to each of the $C$ classes\footnote{This, incidentally leads to an identifiability issue. Identifiability is a property of statistical models whereby a model (the distribution) is defined by a unique parameter set, that is, there is a one-to-one mapping between parameters and models. It is important for precise statistical inference. In this instance the model is clearly not identifiable, as adding any constant vector to each of the parameter vectors gives the same model probabilities. To address this, the parameters are usually offset to eliminate the ambiguity.}. Note that the matrix structure is only for notational convenience, the parameters still effectively constitute a vector of unknowns, and the gradient still has a vector structure. The binary encoding on the classes is dropped in favour of a `one-of-C encoding' binary vector of length $C$. Thus, the negative log likelihood is,

$$\text{NLL}(\mathbf{W}) = -\sum_i \bigg[\bigg(\sum_{c = 1}^Cy_{ic}\mathbf{w}_c^T\mathbf{x}_i\bigg) - \log\bigg(\sum_{c'=1}^C\exp(\mathbf{w}_{c'}^T\mathbf{x}_i)\bigg)\bigg].$$

The gradient and Hessian may be derived as before, but it useful to introduce a specialised \emph{Kronecker\footnote{Leopold Kronecker (1823-1891) was a German mathematician, well known for his Kronecker delta--a shorthand for the indicator function for identity. This is not to be confused with the Dirac delta function, a Gaussian distribution with infinitesimal variance.} tensor product} notation for specifying block matrices,

$$\mathbf{A} \otimes \mathbf{B} = \begin{bmatrix}
a_{11}\mathbf{B}&a_{12}\mathbf{B}&\cdots&a_{1m}\mathbf{B}\\
a_{21}\mathbf{B}&a_{22}\mathbf{B}&\cdots&a_{2m}\mathbf{B}\\
\vdots & \vdots & \ddots & \vdots \\
a_{n1}\mathbf{B}&a_{n2}\mathbf{B}&\cdots&a_{nm}\mathbf{B}\\
\end{bmatrix}.$$

Now, we may derive the gradient as before, adhering to the first principle of forming the vector of partial derivatives,

$$g(\mathbf{W}) = \begin{bmatrix}
\frac{\partial}{\partial \mathbf{w}_{11}} \text{NLL}(\mathbf{W}) \\
\vdots \\
\frac{\partial}{\partial \mathbf{w}_{D, C}} \text{NLL}(\mathbf{W}) \\
\end{bmatrix} =
\sum_i \begin{bmatrix}
(\sigma(\mathbf{w}_1\mathbf{x}_i) - y_{i1})x_{i1} \\
\vdots \\
(\sigma(\mathbf{w}_c\mathbf{x}_i) - y_{ic})x_{iD} \\
\end{bmatrix}
= \sum_i (\boldsymbol\sigma_i - \mathbf{y}_i)\otimes\mathbf{x}_i.
$$

The Hessian is seemingly messy to derive, but it can be done on paper without too much trouble\footnote{It is simplest to try the 2-dimensional case with two classes to spot the pattern.}, yielding,

$$\mathbf{H}(\mathbf{W}) =  \begin{bmatrix}
\frac{\partial}{\partial \mathbf{w}_{11}} g(\mathbf{W}) \\
\vdots \\
\frac{\partial}{\partial \mathbf{w}_{D, C}} g(\mathbf{W}) \\
\end{bmatrix} = \sum_i (\text{diag}(\boldsymbol\sigma_i) - \boldsymbol\sigma_i\boldsymbol\sigma_i^T)\otimes(\mathbf{x}_i\mathbf{x}_i^T),$$

giving a $DC \times DC$ matrix. The model can likewise incorporate a regularisation term. An example of multinomial logistic regression is given in \texttt{logRegDemo.m}.

\subsection{Online Learning and Perceptrons}
\subsubsection{Online learning}
Online learning is an alternative to the more customary offline learning. In online learning, data is streamed rather than processed in a batch. That is, data samples arrive one at a time, and the parameters are adjusted at each iteration. The loss is compared to the batch performance in a function known as the \emph{regret}. This leads to a technique known as online gradient descent. It is further possible to simulate online learning as an alternative to batch gradient descent in an algorithm known as \emph{stochastic} gradient descent (SGD). Here, the gradient at a point is constructed on some subset of the available data, possibly a single data sample alone, rather than the full dataset. The algorithm cycles through the dataset in these smaller chunks, according to some random permutation. A complete cycle is known as an \emph{epoch}, and multiple epochs may be required before convergence is reached. There are various problems where it may be beneficial to do this, for example when fitting neural networks whose cost function is not convex. In this case, the randomness of stochastic gradient descent can help escape local minima during the descent. Moreover, the algorithm may simply outperform batch gradient descent, as calculating the full gradient is $\mathcal{O}(N^2)$, but only $\mathcal{O}(N)$ when a single sample is used. It is likely a small subset of the data can give a good approximation to the true gradient, in particular when duplication is present in the data. When applied to least squares, the algorithm is known as least mean squares (LMS). A comparison of batch and stochastic gradient descent is given in \texttt{regressionDemo.m}.

\subsubsection{The Perceptron}
The perceptron is an historically important algorithm. It is one of the earliest machine learning algorithms, invented by American psychologist, Frank Rosenblatt (1928-1971), in 1956. The perceptron learns a binary classifier in an online manner from supervised data, in a process that closely resembles stochastic gradient descent\footnote{The perceptron algorithm replaces the probability, $\mu_i = p(y_i = 1 | \mathbf{x}_i)$, with the prediction, $\hat{y}_i$ (effectively rounding the probability), but is otherwise identical to SGD.}. The algorithm is guaranteed to converge if the data is linearly separable, and there is a nice proof of this. The algorithm is furthermore guaranteed \emph{not} to converge when the data is not linearly separable, and various enhancements exist to mitigate this problem. Notable examples of the perceptron's limitations are in learning logical functions. A perceptron can learn the \texttt{OR}, \texttt{AND}, and \texttt{NAND} functions, but is unable to learn \texttt{XOR}\footnote{Consider the four points arising from \texttt{XOR} in two variables--they are clearly not linearly separable.}. The multi-layer perceptron, also known as an artificial feed-forward neural network, arises from stacking several perceptrons in layers. These neural networks are much more flexible than the basic perceptron. The perceptron is also the origin of logistic regression, and maximum margin methods such as support vector machines (SVM). An example of a perceptron fitting the logical \texttt{NAND} function is given in \texttt{perceptronDemo.m}.

\subsection{Neural Networks}

Several types of neural networks exist\footnote{See for instance auto-encoders, which reconstruct the input from a reduced number of hidden states--a form of unsupervised learning; or convolutional neural networks, a neural network with a sliding window designed for pattern recognition within high resolution images.}, but we will consider the most common--the feed-forward neural network. Neural networks extend the structure of the simple perceptron by stacking a number of perceptrons into $K$ layers, such that the output of each layer propagates into the next. For this reason, a feed-forward neural network is also known as a \emph{multi-layer perceptron}. Each layer, $k$, has a number of `hidden' units, $\mathbf{z}^{(k)}$, and is a linear combination of the last layer of variables, $\mathbf{z}^{(k-1)}$, and a set of weights, $\mathbf{B}^{(k-1)}$, finally passed through a non-linear `activation' function, $h$ (usually a sigmoid or hyperbolic tangent). That is, $\mathbf{z}^{(k+1)} = h(\mathbf{B}^{(k)}\mathbf{z}^{(k)})$ In this way, the states of each of the hidden units are a culmination of the previous layers. Thus, a neural network makes a prediction as,

$$\hat{y} = h(\mathbf{B}^{(K-1)}h(\mathbf{B}^{(K-2)}(\cdots(h(\mathbf{B}^{(1)})\hat{\mathbf{x}})))).$$

A neural network may be used for both regression and classification, the output passed through the logistic function in the latter case. Despite the complexity, this model can be trained with standard gradient descent techniques. There are some complications however, as the loss function is not convex. The gradients, $\frac{\partial\mathcal{L}}{\partial\mathbf{B}^{(k)}}$, are obtained by application of the chain rule, in an algorithm called \emph{back propagation}. Feed-forward neural networks, and other variants, have seen a great amount of success, especially with the advent of deep learning in recent years. Neural networks are very powerful non-linear models, which are in fact capable of fitting any smooth function given enough layers, making them \emph{universal approximaters}. A simple example of a neural network fitting an absolute value function is given in Figure \ref{fig:universalapproximater}.

\begin{figure}
\centering
\begin{tabular}{cc}
\subfloat[]{\includegraphics[width=0.49\textwidth]{Figures/universalapproximater.pdf}} & 
\subfloat[]{\includegraphics[width=0.5\textwidth]{Figures/sgd.pdf}}
\end{tabular}
\caption{A simple example of a neural network fitting an absolute value function (A) and the corresponding learning curve for stochastic gradient descent (B). Generated with \texttt{mlpDemo.m}.}
\label{fig:universalapproximater}
\end{figure}

\subsection{Support Vector Machines}

Russian computer scientist, Vladimir Vapnik (1936-), invented the maximum-margin classifier in 1963, which later evolved into the kernelised support vector machine (SVM).

\subsubsection{Kernels}

Kernel functions are used to describe a similarity measure between objects. It is often convenient to work with a kernel of the data rather than the data itself. An example of this is tf-idf cosine similarity of a bag of words representation for document classification. Another example are Gaussian kernels, which give a multivariate Gaussian probability for the difference between two observations--the smaller the difference, the greater the probability. In linear models, it is possible to reformulate a model in terms of a kernel matrix $\mathbf{K} = \boldsymbol\Phi^T\boldsymbol\Phi$, where $\boldsymbol\Phi$ is some transformation of data $\mathbf{X}$ using basis function $\phi$. This is known as the \emph{kernel trick}, and it is particularly useful, for example, when we may apply a kernel such as the \emph{radial basis function} whose corresponding basis function is infinite-dimensional, without having to compute the infinite-dimensional basis function itself. Models can be `kernelised' according to some reformulation, and replacing linear kernels (dot products) with another kernel function. In the case of kNN, this can be done quite readily. Kernelising ridge regression requires a reformulation trick known as the matrix inversion lemma. Support vector machines are kernelised by reformulating the problem into its dual problem, by application of the minimax theorem. The kernel trick requires a \emph{Mercer} kernel be used, that is, such that the Gram matrix be symmetric positive-definite.

\subsubsection{Kernelised kNN}

The $k$ nearest neighbours algorithm classifies a data point as the majority vote of the classes of the $k$ closest training data points. By default, the distance metric is Euclidean distance, $||\mathbf{x} - \mathbf{x}'||_2^2 = \mathbf{x}^T\mathbf{x} + \mathbf{x}'^T\mathbf{x}' - 2\mathbf{x}^T\mathbf{x}'$. It is therefore clear we can replace the inner products with any kernel function of our choosing.

%\subsubsection{Duality}
%
%Optimisation problems in general have two forms, a primal and a dual. In general, the solution to the primal gives an upper bound to the dual solution, and the dual solution gives a lower bound to the solution of the primal. This is known as the duality gap,
%
%$$\sup_{y\in\mathcal{Y}}-F^*(0, y) \leq \inf_{x\in\mathcal{X}}F(x, 0),$$
%
%where $F^*$ is the convex conjugate of $F$, $\sup$ is the supremum (least upper bound) and $\inf$ is the infimum (greatest lower bound). For certain convex problems, this bound goes to $0$, and the primal and dual solutions correspond. It is often useful to consider the dual to a primal problem. For one thing, it may yield a more efficient (kernelised) representation. Furthermore, however, it may change the nature of the problem to one more easily solvable with numerical techniques, as we will see for support vector machines (SVMs). Note there are various forms of duality, for example Wolfe duality, named after American mathematician Philip Wolfe (1927-), a pioneer in the fields of convex optimisation theory and mathematical programming.

\subsubsection{Matrix Inversion Lemma}

We will here derive a result that we will use below, known as the Woodbury matrix identity, or the matrix inversion lemma. Given matrices $\mathbf{X} \in \mathbb{R}^{M\times N}$ and $\mathbf{Y} \in \mathbb{R}^{N\times M}$, consider,

\begin{align}
\mathbf{X}\mathbf{Y}\mathbf{X} + \mathbf{X} &= \mathbf{X}(\mathbf{Y}\mathbf{X} + \mathbf{I}_N) \label{eq:mli1} \\
&= (\mathbf{Y}\mathbf{X} + \mathbf{I}_M)\mathbf{X} \label{eq:mli2}
\end{align}

Equating (\ref{eq:mli1}) and (\ref{eq:mli2}),

\begin{align}
(\mathbf{Y}\mathbf{X} + \mathbf{I}_M)\mathbf{X} = \mathbf{X}(\mathbf{Y}\mathbf{X} + \mathbf{I}_N) &\implies \mathbf{X}^{-1}(\mathbf{Y}\mathbf{X} + \mathbf{I}_M)^{-1} = (\mathbf{Y}\mathbf{X} + \mathbf{I}_N)^{-1}\mathbf{X}^{-1}\notag\\
&\implies (\mathbf{Y}\mathbf{X} + \mathbf{I}_M)^{-1}\mathbf{X} =  \mathbf{X}(\mathbf{Y}\mathbf{X} + \mathbf{I}_N)^{-1},\notag
\end{align}

where the final step is achieved by multiplying both sides by $\mathbf{X}$ to the left and right.

\subsubsection{Reformulating Ridge Regression}

If we apply the matrix inversion lemma to the optimal parameters of a ridge regression, we obtain,

$$\boldsymbol{\beta}^* = (\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I}_D)^{-1}\mathbf{X}^T\mathbf{y} = \mathbf{X}^T(\mathbf{X}\mathbf{X}^T + \lambda\mathbf{I}_N)^{-1}\mathbf{y} = \mathbf{X}^T\boldsymbol\alpha,$$

where $\boldsymbol\alpha = (\mathbf{X}\mathbf{X}^T + \lambda\mathbf{I}_N)^{-1}\mathbf{y}$ is a vector of \emph{dual} variables. This demonstrates that the optimal primal parameters $\boldsymbol\beta^*$ lie in the \emph{row} space of $\mathbf{X}$. By contrast, the output $\mathbf{y} = \mathbf{X}\boldsymbol\beta$ lies in the \emph{column} space of $\mathbf{X}$. There is a useful result called the \emph{representer theorem} which states that the primal and dual solutions are always related in this way for models of like form, that is, models with a loss function and penalty term. This is useful, as it is easier to solve the dual problem for support vector machines.

\subsubsection{Decision Boundaries}

Consider a simple binary classifier,

\begin{equation}
f(\mathbf{x}) = \text{sgn}\big(f(\mathbf{x})\big),
\label{eq:svmprimal}
\end{equation}

where $f(\mathbf{x}) = \mathbf{w}^T\mathbf{x} + b$, that is, returning $-1$ if $f(\mathbf{x}) < 0$ and $1$ if $f(\mathbf{x}) > 0$. The function is linear and has a linear decision boundary. It is useful to consider how the model parameters relate to the decision boundary. The decision boundary are those points $\mathbf{x}$ such that $f(\mathbf{x}) = 0$, thus, on the boundary between the two classes. Consider the two-dimensional case where $f(\mathbf{x}) = w_1x_1 + w_2x_2 + b = 0$ (Figure \ref{fig:decisionboundary}A). For $\mathbf{x} = [x_1, x_2]$ on the decision boundary, consider $x_1 = 0 \implies x_2 = -b/w_2$, and $x_2 = 0 \implies x_1 = -b/w_1$. Then, the vector $\mathbf{x} = [0, -b/w_2] - [-b/w_1, 0] = [b/w_1, -b/w_2]$, runs parallel to the decision boundary. Clearly, $\mathbf{w}$ is orthogonal to $\mathbf{x}$, and this result can be shown to hold in higher dimensions also. A vector, $\mathbf{x}$, on the decision boundary and running parallel to $\mathbf{w}$, can be expressed as $\mathbf{x} = |\mathbf{x}|\cdot\mathbf{w}/|\mathbf{w}|$. Now, $f(\mathbf{x}) = \mathbf{w}^T|\mathbf{x}|\cdot\mathbf{w}/|\mathbf{w}| + b = 0$, hence $|\mathbf{x}| = -b/|\mathbf{w}|$. We thereby see that the bias term scales the distance of the decision boundary from the origin.

\begin{figure}
\centering
\begin{tabular}{cc}
\subfloat[]{\includegraphics[width=0.49\textwidth]{Figures/decision.pdf}} & 
\subfloat[]{\includegraphics[width=0.5\textwidth]{Figures/maximummargin.pdf}}
\end{tabular}
\caption{The relationship between a linear decision boundary and model parameters (A) in two dimensions, and comparing a maximum margin, $f_1$, with an alternative, $f_2$ (B).}
\label{fig:decisionboundary}
\end{figure}

Among the infinite possible decision boundaries we could create, a promising heuristic for separating data would be to create a decision boundary such that the perpendicular distance between the nearest data points of each class is maximised\footnote{Note that we are assuming temporarily that the data is linearly separable. This will be addressed shortly.}. Unlike the alternatives, this maximises the \emph{space} left for unseen test cases that fall close to the boundary, likely increasing the performance of the classifier (Figure \ref{fig:decisionboundary}B). This principle is embodied in support vector machines (SVM). Therefore, SVMs are not based on statistical assumptions, rather on the rule of thumb that maximum margins perform better.

Now, any point $\mathbf{x}$ can be decomposed into the sum of its projection onto the decision boundary, $\hat{\mathbf{x}}$, and the residual, perpendicular to the boundary (and parallel to $\mathbf{w}$). Thus, $\mathbf{x} = \hat{\mathbf{x}} + r\cdot\mathbf{w}/|\mathbf{w}|$, where $r$ is the length of the residual, hence the size of the margin of $\mathbf{x}$. Thus, $f(\mathbf{x}) = \mathbf{w}(\hat{\mathbf{x}} + r\cdot\mathbf{w}/|\mathbf{w}|) + b =  |\mathbf{w}|\cdot r$, since $f(\hat{\mathbf{x}}) = 0$. Thus, we have $r = f(\mathbf{x})/|\mathbf{w}|$. Maximising the margin between the nearest points can therefore be expressed as the quadratic programming (QP) problem,

$$
\begin{array}{rl}
\min_{\mathbf{w}, b} & \frac{1}{2}\mathbf{w}^T\mathbf{w} \\
\text{subject to} & y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1, \forall i
\end{array}
$$

where the constraints ensure each data point is correctly classified. Of course, the assumption of linear separability is in general untrue, so we introduced slack variables, $\xi_i$, to allow for data on the wrong side of the decision boundary, that is, misclassifications. Thus, we replace each constraint with, $y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1 - \xi_i$, leaving,

$$
\begin{array}{rl}
\min_{\boldsymbol\xi, \mathbf{w}, b} & \frac{1}{2}\mathbf{w}^T\mathbf{w} + C\sum_{i = 1}^{N}\xi_i \\
\text{subject to} & y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1 - \xi_i, \forall i \\
& \xi_i \geq 0, \forall i
\end{array}
$$

%$$
%\begin{array}{rl}
%\min_{\boldsymbol\alpha} & \inf_{\boldsymbol\xi, \mathbf{w}, b}\bigg(\frac{1}{2}\mathbf{w}^T\mathbf{w} + C\sum_{i = 1}^{N}\xi_i + \sum_{i=1}^N \alpha_i(1 - \xi_i - y_i(\mathbf{w}^T\mathbf{x}_i + b))\bigg) \\
%\text{subject to} & \alpha_i \geq 0, \xi_i \geq 0, \forall i
%\end{array}
%$$

where $C$ is a weight for classification error. This problem is often expressed in terms of a hinge loss function, following the notation of other regression models. By the principle of convex duality, we can convert this problem into its dual form where the dual variables $\alpha_i$ are the Lagrange multipliers of the earlier constraints. The dual form is then the maximum of the \emph{infimum}\footnote{The infimum of a set is its greatest lower-bound, the counterpart of the \emph{supremum}, the least upper-bound. Some sets have these bounds without having a definable minimum or maximum. For example, the set $\{x : 0 < x < 1\}$ has no minimum or maximum, but its infimum is 0 and supremum is 1.}  of our current problem. We can find the infimum of this expression by taking derivatives of each of the parameters, thus $\frac{\partial f}{\partial \mathbf{w}} = \mathbf{w} - \sum_{i=1}^N \alpha_iy_i\mathbf{x}_i = 0 \implies \mathbf{w}^* = \sum_{i=1}^N \alpha_iy_i\mathbf{x}_i$ at optimality, which may be substituted into the objective function. For the bias term, $\frac{\partial f}{\partial b} = \sum_{i=1}^N \alpha_iy_i = 0$ at optimality. For the slack variables, 
$\frac{\partial f}{\partial\xi_i} = C - \lambda_i \implies \lambda_i \leq C$ at optimality, where the inequality arises from the positivity constraint on $\xi_i$. Thus, the parameters are eliminated by substitution, and the bias and slack variables correspond with constraints on the dual variables, giving the dual form,

\begin{equation}
\begin{array}{rl}
\max_{\boldsymbol\alpha} & \sum_{i = 1}^{N}\alpha_i + \frac{1}{2}\sum_{i}^N\sum_{j}^N \alpha_i\alpha_jy_iy_j\mathbf{x}_i^T\mathbf{x}_j \\
\text{subject to} & \sum_{i=1}^N \alpha_iy_i = 0 \\
& 0 \leq \alpha_i \leq C, \forall i
\end{array}
\label{eq:dual}
\end{equation}

thus a quadratic programming (QP) problem in the dual variables. Noting the relation between primal and dual parameters, we can rewrite the classifier (\ref{eq:svmprimal}) in terms of the dual variables as,

\begin{equation}
f(\mathbf{x}) = \text{sgn}\Bigg(\sum_{i=1}^N \alpha_iy_i\kappa(\mathbf{x}_i, \mathbf{x}) + b\Bigg),
\label{eq:svmdual}
\end{equation}

where $\kappa(\cdot, \cdot)$ is a Mercer kernel of our choosing, replacing the linear inner product from before. Apart from the dual form being conveniently kernelised, it affords an insightful alternative interpretation of the model. From (\ref{eq:svmdual}), we can see how an observation, $\mathbf{x}$ is classified according to its similarity with the training vectors. Each training vector, $\mathbf{x}_i$, casts its vote, $y_i$, be it $-1$ or $1$, with the influence regulated by a similarity measure provided by the kernel function, and the weight provided by the dual variable. Only the support vectors have a (non-zero) weighted vote. It is easy to see why this is such a powerful framework--we can choose between many similarity measures (kernels), though the mathematics restricts us to choosing Mercer kernels.

\subsubsection{Sequential Minimal Optimisation}

Our model (\ref{eq:svmdual}) may be solved with any QP solver, which was formerly the standard approach. However, in 1998, American computer scientist, John Platt (1963-), invented sequential minimal optimisation (SMO), an algorithm for analytically optimising the dual variables two at a time. The superiority and simplicity of this algorithm helped bring support vector machines into the forefront of machine learning research. The algorithm works by iteratively optimising pairs of variables that violate the optimality conditions. These are,

\begin{equation}
\begin{array}{rcl}
\alpha_i = 0 & \implies & y_if(\mathbf{x}_i) \geq 1 \\
0 < \alpha_i < C  & \implies & y_if(\mathbf{x}_i) = 1 \\
\alpha_i = C & \implies & y_if(\mathbf{x}_i) \leq 1
\end{array}
\label{eq:svmkkt}
\end{equation}

 Optimisation is done in pairs in order to maintain the equality constraint of the problem (\ref{eq:dual}). Thus, for pair $\alpha_i$ and $\alpha_j$, we make updates such that $y_i\Delta\alpha_i + y_j\Delta\alpha_j = 0$. Rearranging gives $\Delta\alpha_j = -\Delta\alpha_iy^{(i)}/y^{(j)} = -\Delta\alpha_iy^{(i)}y^{(j)}$, as since $y_i \in \{-1, 1\}$, division is the same as multiplication, and the cost function becomes,

\begin{align}
W(\boldsymbol\alpha) &= \sum_{k=1}^N\alpha_k + \Delta\alpha_i -\Delta\alpha_iy_iy_j - \frac{1}{2}\sum_{k=1}^Ny_k\alpha_k(f(\mathbf{x}_k) - b) \notag \\ 
&- \Delta\alpha_i(f(\mathbf{x}_i) - f(\mathbf{x}_j)) - \Delta\alpha_iy_i(f(\mathbf{x}_i) - f(\mathbf{x}_j)) - \frac{1}{2}\Delta\alpha_i^2(\mathbf{x}_i^T\mathbf{x}_i - 2\mathbf{x}_i^T\mathbf{x}_j + \mathbf{x}_j^T\mathbf{x}_j). \notag 
\end{align}

Differentiating and solving gives,

$$\Delta\alpha_i^* = \frac{y_i\big(f(\mathbf{x}_j) - y_j - f(\mathbf{x}_i - y_i)\big)}{\mathbf{x}_i^T\mathbf{x}_i - 2\mathbf{x}_i^T\mathbf{x}_j + \mathbf{x}_j^T\mathbf{x}_j},$$

giving us our update step. The value of $\Delta\alpha_j^*$ can then be determined from the initial equality. Note that both variables remain constrained, hence their values must be clipped if they are less than $0$ or greater than $C$. The bias term is then updated to account for the changes. For example, to ensure the classifier now emits $y_i$ for $f(\mathbf{x}_i)$, we rearrange,

$$y_i = \sum_{k=1}^N \alpha_iy_i\mathbf{x}_k^T\mathbf{x}_i + b + \Delta b + \Delta\alpha y_i\mathbf{x}_i^T\mathbf{x}_i + \Delta\alpha y_j\mathbf{x}_i^T\mathbf{x}_j,$$

for $\Delta b^*$. The same is done for $f(\mathbf{x}_j)$, and the expressions are combined according to the convergence conditions (\ref{eq:svmkkt}). Thus, the optimisation steps are done analytically, rather than with linear algebra. The rest of the algorithm is mostly concerned with heuristics for choosing the best $\alpha_i$, $\alpha_j$ pair. An implementation of support vector machines solved with the SMO algorithm is given in \texttt{supportVectorMachine.m}.

\subsubsection{Support Vector Machine Regression}

The ideas of support vector machines can be extended to regression models as well. This is achieved using a modification of the Huber loss function, creating a constrained quadratic programming (QP) problem. One of the shortcomings of SVMs is the difficulty in extending them to multi-class classification. This stems from the fact that they are not probabilistic models. Schemes exist, however, for example, by training multiple classifiers in a hierarchy.

\subsection{Decision Trees}

Decision trees are distinct from the previously discussed classifiers in that they are \emph{non-parametric}. This means the model is not simply a mathematical function with a fixed form, and that the size of the model changes depending on the size of the data.

\subsubsection{Learning Decision Trees}

Just as with regression, we begin with a dataset, $\mathcal{D} = \{(\mathbf{x}_n, y_n)\}_{n=1}^{N}$. Now, however, the output variable, $y \in \{0, 1, \dots, K-1\}$, is a discrete variable taking a value in one of $K$ categories. For simplicity, we will consider the case that $K = 2$, that is, binary classification. We therefore search for a way of splitting the data into two groups. We repeat this procedure for each of the subgroups iteratively, until we satisfy some stopping condition. In so doing, we create a binary tree. Prediction can therefore be done by tracing a data point through the splitting conditions of the tree.

The only question then is \emph{how} we should split the data. For example, if we are classifying vehicles, and $y \in \{\text{car}, \text{motorbike}\}$, and our data consisted of categories, $\mathbf{x} = [\text{number wheels}, \text{colour}, \text{milage}]$, the number of wheels is likely be a very effective indicator for sorting the cars from the bikes. In fact, it may be decisive, and rarely are indicators so informative in practice. The colour would likely tell us very little, but the milage might tell us something, as longer journeys are usually done by cars, so there may be a weak correlation. We therefore use an impurity measure to quantify the reduction in uncertainty upon making a given split. The best split is the one that minimises the uncertainty,

\begin{align}
( k^*, \tau^*) &= \min_{k, \tau} I_{split}(X, k, \tau) \notag \\
&= N_LI_{split}(p_L) + N_RI_{split}(p_R), \notag
\end{align}

where $L$ and $R$ denote the left and right splits of the data, $N_L$ and $N_R$ are the respective population sizes of the split data, and $p_L = \frac{\#(y = 0)}{N_L}$ and $p_R = \frac{\#(y = 0)}{N_R}$ are the sample probabilities of belonging to a given class, where in our fictitious data, $y=0$ could indicate a car, and $y=1$ a bike. Of course, when these probabilities go to 1, we are \emph{certain} of the class of the remaining data. Note the split consists of two components: the variable of split $x_k$, and the decision rule $\tau$. For example, with our fictitious data, the optimal split might be on $x_{k^*} = (\text{number wheels})$ and $\tau^* = (\text{number wheels} \leq 2)$. Clearly, this would more or less perfectly divide the bikes and the cars into distinct groups. In practice, we must systematically try all possible splits for each variable. If the variable is continuous, we must then discretise it in some way.

Several impurity measures are possible, but we know from information theory that uncertainty can be modelled with the \emph{entropy} statistic. For binary classification, we have only two probabilities, $p(y = 0)$ and $p(y=1) = 1 - p(y=0)$. We therefore have the special case of binary entropy, $h_2(p)$, and our impurity measure becomes,

$$I_{split}(p) = h_2(p) = -p\log p - (1-p)\log (1-p).$$

\subsubsection{More Advanced Techniques}

An extension of decision trees is the technique of \emph{random forests}. Random forests construct multiple decision trees from subsets of the training data. Prediction is then done by aggregating the predictions of individual trees. This is an example of ensemble learning and \emph{bagging} (bootstrap aggregating). The advantage of random forests is that it reduces the variance of the prediction.

\subsection{Dimensionality Reduction and PCA}

The curse of dimensionality motivates techniques to transform high-dimensional data, given by an $N \times D$ matrix $\mathbf{X}$, to a lower rank approximation. This can be achieved by choosing lower dimensionality, $M  < D$, and finding $D \times M$ matrix $\mathbf{W}$ and $N \times M$ matrix $\mathbf{Z}$ such that,

$$\mathbf{X}^T \approx \mathbf{W}\mathbf{Z}^T,$$

and such that the \emph{reconstruction error} is minimised. This leads to the objective function,

$$\min_{\mathbf{W}, \mathbf{Z}} J(\mathbf{W}, \mathbf{Z}) = \frac{1}{N}\sum_{i=1}^N||\mathbf{x}_i - \hat{\mathbf{x}}_i||^2,$$

where $\hat{\mathbf{x}}_i = \mathbf{W}\mathbf{z}_i$ is the reconstruction of the $i$th row. The reconstruction error function can be minimised (and the optimal decomposition found) with a technique known as alternating least squares (ALS). It can be shown that the optimal orthogonal choice occurs when $\mathbf{W}$ is the matrix of the $M$ most significant eigenvectors (highest eigenvalues) of the sample covariance matrix, $\hat{\boldsymbol\Sigma} = \frac{1}{N}\sum_i\mathbf{x}_i\mathbf{x}_i^T = \mathbf{X}^T\mathbf{X}$. These are otherwise known as the \emph{principal components} of $\mathbf{X}$, which brings us to principal component analysis (PCA), a highly popular technique for dimensionality reduction. If we can find $\mathbf{X}^T \approx \mathbf{W}\mathbf{Z}^T,$ for orthonormal $\mathbf{W}$, then $\mathbf{W}^T\mathbf{X}^T \approx \mathbf{W}^T\mathbf{W}\mathbf{Z}^T \implies \mathbf{X}\mathbf{W}  \approx \mathbf{Z}$, hence $\mathbf{W}$ maps our data to a lower-dimensional form. When the transformation is made, a model may be trained, perhaps more successfully than with the full data.

\subsubsection{Singular Value Decomposition}

Singular Value Decomposition (SVD) is to rectangular matrices what eigenvalue decomposition is to square matrices. This factorisation of matrix $\mathbf{X}$ can be written,

$$\mathbf{X} = \mathbf{U}\mathbf{S}\mathbf{V}^T,$$

where matrices $\mathbf{U}$ and $\mathbf{V}$ are orthonormal, and $\mathbf{S}$ is the diagonal matrix of \emph{singular values}. This decomposition may be arrived at by various techniques. Since $\mathbf{U}\mathbf{S}\mathbf{V}^T = \sum_i s_{ii} \mathbf{u}_{:,i}\mathbf{v}_{:,i}^T$, it is possible to obtain a lower-rank approximation to $\mathbf{X}$ just be dropping the less significant (lower singular value) terms in the sum. This is called truncated SVD, and is the basis of information retrieval techniques such as latent semantic indexing (LSI). To relate SVD back to PCA, note that the empirical covariance matrix,

$$\hat{\boldsymbol\Sigma} = \mathbf{X}^T\mathbf{X} = \mathbf{V}\mathbf{S}\mathbf{U}^T\mathbf{U}\mathbf{S}\mathbf{V}^T =  \mathbf{V}\mathbf{S}^2\mathbf{V}^T,$$

which is the eigenvalue decomposition. This implies that $\mathbf{W} = \mathbf{V}$ and $\mathbf{Z} = \mathbf{X}\mathbf{W} = \mathbf{U}\mathbf{S}\mathbf{V}^T\mathbf{V} = \mathbf{U}\mathbf{S}$. Thus, SVD (which may be done with various techniques) is an alternative to ALS for acquiring the reconstruction matrices for PCA. We can choose the dimensionality by taking the $M$ most significant singular values.

\subsection{Unsupervised Machine Learning}

Whereas supervised machine learning can be said to embody learning by example, \emph{unsupervised} machine learning is concerned with finding inherent patterns in a dataset. In this sense, its aim is \emph{description}, rather than prediction. The two most common applications of unsupervised machine learning are feature extraction and \emph{clustering}. Clustering is of particular interest to data mining applications. Clustering is the act of grouping data observations according to some grouping measure. The choice of clustering algorithm is usually problem-specific. Various approaches exist, for example density-based clustering algorithm DBSCAN (density-based spatial clustering with additive noise), or agglomerative clustering \emph{dendrogram} models\footnote{Dendrograms cluster data by pairing closest data samples into groups repeatedly, creating a binary tree hierarchy in a bottom-up approach until all data belongs to a single super group.}. These techniques are popular, but are more heuristic algorithms than machine learning. The techniques we will discuss are more sophisticated and are based on fitting probability densities to data.

\subsubsection{K-means}

One of the most popular clustering techniques, K-means, organises data into $K$ clusters. The algorithm begins by initialising $K$ centre points in $D$-dimensional space and alternates between two steps. The first step assigns observations to centres based on a distance measure (usually Euclidean distance). Observations assigned to a common centre can then be said to be in the same cluster. For each cluster, a new centre point is calculated as the average of all observations in the cluster. This is repeated until the centre points converge (which will occur when observations cease to change clusters). Note that $K$ must be pre-selected--the algorithm does not tell us what value of $K$ is most suitable. To formalise beyond this intuitive algorithm, we denote a set of parameters, $r_{nk}$, whose binary value indicates the membership (or not) of observation $x_n$ in cluster $k$. A further set of parameters, $u_k$, denote the centre point of each of the $K$ clusters. Thus, the K-means algorithm can be formulated as the optimisation problem,

$$\min_{\mu, \mathbf{r}} \mathcal{L}(\mu, \mathbf{r}) = \sum_k\sum_n r_{nk}||\mathbf{x_n} - \mathbf{\mu}_k||^2_2,$$

where $\sum_k r_{nk} = 1$, and the distance measure is the sum of squares, $(x_n - \mu_k)^T\cdot(x_n - \mu_k)$. Now, we may write step 1:

\[r_{nk} = \begin{cases}
    1 & \text{if} \ k = \arg \min_k \ ||\mathbf{x}_n - \mu_k||_2^2 \\
    0 & \text{otherwise}
\end{cases}\]

Step 2 can then be seen as an optimisation of $\mathcal{L}$ with respect to each $u_k$. Clearly,

\begin{align}\frac{\partial\mathcal{L}}{\partial\mu_k} = \frac{\partial}{\partial\mu_k}\sum_n r_{nk} (\mathbf{x}_n^T\mathbf{x}_n - 2\mathbf{x}_n^Tu_k + u_k^Tu_k) &= 0 \notag \\
&\implies \hat{\mu}_k = \frac{\sum_nr_{nk}x_n}{\sum_nr_{nk}}, \notag
\end{align}

this is clearly the arithmetic mean of the points in the cluster. With this in mind, we may see that our loss function can be written as a maximum likelihood,

\begin{align}\min_{\mu, \mathbf{r}} \mathcal{L}(\mu, \mathbf{r}) = \max_{\mu, \mathbf{r}} \log p(\mathcal{D} | \mu, \mathbf{r}) &= \log\prod_k\prod_n \mathcal{N}(\mathbf{x}_n ; u_k, \mathbf{I})^{r_{nk}} \notag \\
&= -\sum_k\sum_nr_{nk}(\mathbf{x}_n - \mu_k)^T\mathbf{I}(\mathbf{x}_n - \mu_k), \notag
\end{align}

where $\mathbf{I}$ is the identity matrix. This shows that K-means is fitting $K$ Gaussian densities with unit variance. So, despite the initially simple algorithm, we see the deeper probabilistic meaning behind K-means. Because the clusters are all based on spherical Gaussians of fixed size and shape, the technique can alternatively be viewed as clustering points by nearness in Euclidean space. It is useful, however, to take note of the probabilistic notions, as these are expanded upon in the more powerful Gaussian mixture models.

\subsubsection{Gaussian Mixture Models}

\begin{figure}
\centering
\begin{tabular}{cc}
%\subfloat[1 iteration]{\includegraphics[width=0.5\textwidth]{Figures/gmm0.png}} & 
%\subfloat[10 iterations]{\includegraphics[width=0.5\textwidth]{Figures/gmm10.png}}\\
%\subfloat[20 iterations]{\includegraphics[width=0.5\textwidth]{Figures/gmm15.png}}&
%\subfloat[30 iterations]{\includegraphics[width=0.5\textwidth]{Figures/gmm30.png}} \\
\subfloat[1 iteration]{\includegraphics[width=0.5\textwidth]{Figures/gmm0.pdf}} & 
\subfloat[3 iterations]{\includegraphics[width=0.5\textwidth]{Figures/gmm2.pdf}}\\
\subfloat[5 iterations]{\includegraphics[width=0.5\textwidth]{Figures/gmm4.pdf}}&
\subfloat[10 iterations]{\includegraphics[width=0.5\textwidth]{Figures/gmm10.pdf}} \\
\end{tabular}
\caption{Expectation maximisation (EM) algorithm in action over 30 iterations for Gaussian mixture models with $K = 3$. Colour coding reflects the ratio of probabilities for each cluster--note that points in overlapping positions are more ambiguous. Contour plot of sample Gaussians given in final iteration. Created with \texttt{gmmDemo.m}.}
\label{fig:gmm}
\end{figure}

A more powerful technique, Gaussian mixture models (GMM) improve on two major shortcomings of K-means. Firstly, GMMs fit elliptical Gaussian densities, optimising over the choice of $\boldsymbol\Sigma$, the covariance matrix. Secondly, the parameters, $r_{nk}$, are replaced by random variables, $r_n$, of a special kind called \emph{latent} random variables. These act as a bridge between each observation $\mathbf{x}_n$ (also a random variable), and its class, $k$. Thus, rather than hard binary values, the values $r_{nk} = p(r_n = k)$ indicate the probabilities of $\mathbf{x_n}$ belonging to cluster $k$. This is useful both for the fact that the parameters no longer grow with the size of the data, and also to quantify an uncertainty about $\mathbf{x}_n$, which can be used for analysis of outliers, and for example the selection of $K$. However, the complexity of the algorithm is far greater than K-means. Again, problem-specific considerations must be made when choosing the most suitable algorithm. Note that unlike spatial clustering algorithm DBSCAN, GMMs are capable of clustering densities that overlap, but are less useful when the data is not linearly separable. Thus, GMMs are good at clustering populations that are likely to be normally distributed in their features, but not, for example, geospatial data that exhibit irregular shapes. The likelihood function for GMMs is maximised using an iterative algorithm called expectation maximisation (EM) (Figure \ref{fig:gmm}), which, similar to K-means, consists of two alternating steps, the $E$ and $M$ steps. In general this is,

$$\arg \max_\theta \mathbb{E}_{p(z_n)}[\log p(x_n, r_n | \theta)].$$

\section{Computer Architecture}

The von Neumann architecture, named after Hungarian polymath John von Neumann (1903-1957), is the basic template for assembling a computer. It centres around a central processing unit (CPU) and primary memory source. Input and output devices may be connected to this processing core. In a modern personal computer, the components are organised across a printed circuit board (PCB)\footnote{A printed circuit board (PCB) is a plate of layered material. The top layer is made of a conductive copper and sits atop a non-conductive fibreglass layer. The circuit pattern is created by pressing a mask of the printed design to the copper plate and soaking it in an acid solution to remove the surrounding copper. Finally, a solder mask in the familiar blue or green material is placed on top of the copper traces to protect them from dust and debris, while leaving holes at the connection points. Components may then be connected to the copper circuit using melted solder (a lead-tin alloy) to fuse components to the exposed connection points. Thus, the PCB replaces traditional wiring with the copper traces, functioning as the circuitry as well as a platform to which integrated circuits and other components are attached.}, known as a \emph{motherboard}. A chip is a standalone circuit with a specific function, such as as memory or processor. These components are soldered onto the motherboard to connect them to the rest of the computer hardware. The wiring groups between components are known as \emph{buses}, and are used to transfer information throughout the system. However fine the motherboard circuitry is, the internal circuitry of an integrated circuit is far finer, having of the order of billions of transistors per square centimetre. When a processor is contained in a single integrated circuit, it is referred to as a \emph{microprocessor}. This is the defining characteristic of a microcomputer or personal computer (PC). Personal computers have long supplanted the large, mainframe computers of earlier decades.

The distinction between hardware and software is at times not well defined. Thus, it is difficult to talk about one without the other. Ultimately, everything exists in the physical layer, software just being structured (low entropy) configurations of electrical signals buried in the hardware. Software has varying levels of abstraction, right up to the user-friendly graphical interfaces, but at least in some sense, this is all just an illusion created by organising lower-level parts.

\subsection{Hardware}

As it is in the von Neumann architecture, the centrepiece of computer activity is the interaction between the central processing unit (CPU) and the (primary) memory--all other components are arguably peripheral to this. When a computer is turned on, a component within the CPU begins to oscillate. This is like the heartbeat of the computer, and provides a periodic \emph{clock} bit to each computation. In a Turing machine, the clock would be the invisible force driving the transitions along the tape. With some additional circuitry, the clock signal is accelerated to a \emph{clock rate} of the order of billions of hertz (Hz). This largely determines the speed of the computer, though the practical rate of computation, or floating point operations per second (FLOPS), will depend on other things, such as the efficiency of the circuitry.

The CPU first runs a primitive piece of firmware that solves the bootstrap problem\footnote{The bootstrap is conceptually the necessary prime mover for all further software on the system.}. On a PC, this is called the BIOS (Basic Input Output Software) on PC, and UEFI (Unified Extensible Firmware Interface) on a Mac. The BIOS performs checks to ensure all the hardware is functional. It also provides a user interface for making low-level changes to the system\footnote{For example, a CPU can be \emph{overclocked} to increase performance. This can be controlled from the BIOS. As a general rule, the more active the CPU is, the more voltage in its circuits per unit time. More voltage creates more heat, and thermal sensors in the motherboard cause the fans to work harder to cool the chip. Overclocking can have detrimental effects if the circuitry gets too hot.}. The BIOS is usually located on a special chip of read-only memory (ROM), installed by the manufacturer. The BIOS finds the boot loader stored in secondary memory--typically a hard disk drive (HDD) or solid-state drive (SSD), but other secondary memory such as a USB drive, CD-ROM, or the former floppy disk, are sometimes used\footnote{A user can usually interrupt the BIOS and choose where to boot from.}. The boot loader is then loaded into memory. Its task is to load the operating system (OS) into memory in turn. Again, the user may interrupt this procedure in order to choose between operating systems installed on the drive. Once the operating system is in memory it has control of the system, giving the user the ability to control the computer and load and execute other software.

\subsection{Software}

Software exists in memory as a sequence of precompiled instructions, usually loaded from secondary memory. The control unit (CU) within the CPU is hardwired to take each instruction in turn and run it through the arithmetic/logic unit (ALU) circuitry. \emph{Registers} are small, localised pieces of memory for storing inputs for CPU operations. At this level, the code is not interpreted, but rather hardwired. Each instruction has an operator code which channels execution to the appropriate circuit. The form of the instructions are defined in an assembly language, which reflects the hardwired functions provided by the processor. An example of this is the x86 and x64 Intel resp. 32-bit and 64-bit instruction sets. Assembly code is usually automatically generated from high-level language code by a compiler. The assembly code is then transformed into a binary encoding called machine code. There is more or less a one-to-one mapping between assembly and machine code. In this way, machine code is more of an encoding than a language. As an example, take the simple high-level operation \texttt{a + b}, as it would be in \texttt{C}, or another programming language. When translated to assembly code, this will look something like Algorithm \ref{alg:assembly}.

\begin{algorithm}
\caption{Assembly pseudocode for a \texttt{a + b} operation.}\label{alg:assembly}
\begin{algorithmic}[1]
\State LDR R1 a // load of 1 with variable a
\State LDR R2 b // load register 2 with variable b
\State ADD R1, R2, R3 // add register 1 and 2 and put the answer in register 3
\end{algorithmic}
\end{algorithm}

This follows what is called \emph{reverse Polish notation} (RPN), a convenient way to encode such instructions. This might then be translated to machine code as \texttt{01001100 00000010 00000011 00000100}, that is, perform operation 76 (addition) on registers 1 and 2 and put answer in register 3. Binary code is directly translatable to a system of high and low voltages. Thus, the program can exist in secondary storage as a series of electrical signals, ready to be loaded into memory by the operating system instructions for processing. To better understand how this processing works, it is necessary to descend to the electronics level.

\subsection{Digital Circuitry}

The fundamental component of electrical computers is the transistor. Conceptualised in 1926 by Hungarian physicist, Julius Edgar Lilienfeld (1882-1963), it was first implemented by a trio of Bell labs researchers in 1947, work for which they received the Nobel prize in Physics in 1956. A transistor is made of semiconductor material such as silicon (Si). It functions as a bridge between a source and sink wire, providing a third, base wire that controls the conductivity of the silicon material. If current is passed through the base wire, the bridge is opened and current flows from source to sink. In this way, the current on the main wire can be switched on or off very quickly and with no moving parts. The size of a computer overall therefore depends crucially on how small we can make transistors. Moore's law\footnote{Named after Gordon Moore (1929-), one of the founders of Intel Corporation.} predicts that the number of transistors per integrated circuit doubles every two years. This allows for building computers that are twice as powerful while retaining the same approximate size, cost, and power consumption. However, we are rapidly approaching the physical limit on transistor size. At $1.4 \times 10^{-10}\text{m}$, quantum effects are starting to interfere with accuracy.

\subsubsection{Logic Gates}

Using combinations of transistors, we can create logic gates. As it is in boolean normal forms, the only gates we need are conjunction (\texttt{AND}), disjunction (\texttt{OR}), and negation (\texttt{NOT}), and we can wire any logical circuit. An \texttt{AND} gate can be created by placing two transistors in sequence. If both transistors are activated, current will flow overall. If either or both transistors are not activated, current will not flow. Thus, it may be seen that submitting some combination of on/off (\texttt{TRUE}/\texttt{FALSE}) currents to the transistors will result in an overall on/off current through the wire, in accordance with the truth table of a logical \texttt{AND} operator. Thus, we have a circuit capable of computing a logical \texttt{AND}. An \texttt{OR} gate may be created by placing two transistors in \emph{parallel}, and a \texttt{NOT} gate or \emph{inverter} by combining positive and negative transistors.

\subsubsection{Adders}

From logic gates we can construct adders. The exclusive-or (\texttt{XOR}) operation handles the first part of binary addition: $0 \oplus 0 = 1 \oplus 1 = 0$ and $1 \oplus 0 = 0 \oplus 1 = 1$. The only thing that is not accounted for is the \emph{carry bit}. For this reason, \texttt{XOR} is called a \emph{half-adder}. It can be created in a number of ways, for example, as \texttt{OR AND NOT AND}. To create a \emph{full-adder}, it is combined with an \texttt{AND} gate to represent the carry bit. To add an 8-bit number, eight full-adders can be arranged in sequence, with each carry bit feeding into the next. More complex operations such as multiplication can be created by combining multiple full-adders. Examples of other CPU operations are the \texttt{COMPARE} and \texttt{JUMPIF} instructions. These are used in sequence to dynamically move among instructions in memory based on some logical test. This is what enables loops and if statements to occur.

\subsubsection{Memory}

The type of memory usually as primary memory is \emph{random access memory} (RAM). Random access refers to the fact that any memory address can be accessed equally quickly. This is in contrast to a hard disk drive used for \emph{secondary} memory, where a latency of a few milliseconds is incurred to move the requested memory sector under the drive's read-write head. Increased affordability has brought about the advent of the much faster \emph{solid state}\footnote{That is, no moving parts.} (SSD) secondary memory. Note that SSD memory is still orders of magnitude slower than RAM. Despite being fast, RAM is \emph{volatile}. Volatility is a property referring to the loss of main memory when the power is switched off. This contrasts with the \emph{persistent} memory provided by secondary memory sources. There are two main types of RAM: static RAM (SRAM) and dynamic RAM (DRAM). Usually SRAM is used in a CPU cache. A cache is used to store recently used data in fast memory near to the CPU. This is an heuristic strategy that helps to minimise the delay incurred on frequently used data by the much slower main memory access, the CPU being far faster. Typically, only a small cache is sufficient to speed up processing dramatically. Like logic gates, a unit of static RAM is constructed by wiring transistors in a special way. We start by wiring two \texttt{NOR} gates together such that one of the inputs of each is the output of the other. The remaining input wires are denoted the \emph{set} and \emph{reset} wires. With this configuration, it is possible to send current through the set and reset wires to switch the signals between the \texttt{NOR} gates, and such that this signal persists when the current stops. This mechanism is called a RS latch, and provides a way to store a binary signal. When combined with a wire for write data, and the CPU clock bit, we can create what is called a \emph{flip-flop}. Each flip-flop is used to store a single bit of information, and these are arranged into an array that can be addressed by row and column number. In contrast, DRAM is cheaper and more suited to primary memory storage. This consists of an array of capacitors to store bits. The capacitors only hold the bit signal for a small amount of time (milliseconds), so a mechanism is used to quickly read and rewrite memory periodically (every 100 nanoseconds or so). This is why DRAM is known as \emph{dynamic}, though it might better be referred to as \emph{extra-volatile} memory.

\subsection{Operating Systems}

An operating system (OS) is the primary software running on a computer. As well as generally providing the user a means of manipulating the computer, it facilitates the running of other programs on the hardware. A program is loaded into memory as a \emph{process}, with its own \emph{heap} of memory. A heap is a contiguous block of main memory. When memory is allocated dynamically in a language like \texttt{C} it takes it from the process heap. Segmentation faults occur when memory on the heap is abused. A process consists of one or more \emph{threads}, which represent streams of execution. Each thread has a \emph{stack} which is a FIFO queue for instructions and data. The stack is pushed and popped in accordance with the requirements of the program. The stack grows with the initialisation of local variables, but also with each new call to a nested function. Therefore, a maximum recursion depth is imposed by many languages to prevent the stack memory allowance being exceeded, an event known as a \emph{stack overflow}. A process scheduler in the operating system controls the access to hardware resources for each process. This is essential to a multiprogramming operating system. Input and output devices are connected to the bus with addresses. IN/OUT instructions can send or retrieve data from these devices via a data bus. The operating system provides \emph{driver} software for interfacing with hardware components. The heart of an operating system is the kernel. This provides various services for memory allocation of processes, organisation of the OS file system, handling interrupts from peripheral devices, and so on.

\subsubsection{The Unix Story}

Unix is an important operating system that was developed at Bell Labs by American computer scientists Ken Thompson (1943-) and Dennis Ritchie (1941-2011). It is the ancestor of many of the most widely used operating systems used today. At the time, Unix was favoured due to its speed, stability, modularity, and security. Unix was the first operating system written in a high-level language (\texttt{C}, itself created by the same people), and was thus portable to any hardware with a \texttt{C} compiler. It also had just the right set of innovations to make it the ideal operating system, for example \emph{piping}, used to chain software together via their outputs. However, Unix was proprietary software owned by AT\&T\footnote{Bell Labs, NJ, was owned by AT\&T for most of the 20th century. It is credited with inventing radio astronomy, the transistor, information theory, \texttt{C}, \texttt{C++}, Unix, and the one time pad (Vernam cipher). It is now owned by Nokia.}, and interest developed to create free equivalents. In particular, MIT AI lab programmer Richard Matthew Stallman (1953-), started the Free Software Foundation (FSF) and the GNU (GNU's Not Unix!) project to create a free operating system. This was a pioneering initiative in the history of open-source software.

Apart from being free to use, the idea of a GNU operating system was to expand \emph{user land}, that is, the part of the OS flexible to a user's preferences, having only a "micro-kernel" and a set of replaceable daemons\footnote{In Unix, daemons are named with a trailing `d'. This is why the Apache HTTP Server, \texttt{httpd}, (the most commonly used web server) is so named.}. The sense of \emph{free} software is really to do with free control over the software--gratuity is only one part of that. The GNU micro-kernel is in contrast to a complete, \emph{monolithic} kernel such as the Linux kernel. The micro-kernel was to be known as Hurd (a gnu is a type of antelope). Due to some technical difficulties, progress stalled and the project was supplanted by the Linux project, developed by Finnish programmer, Linus Torvalds (1969-), taking with it much of the support and enthusiasm for GNU. Later, divisions arose in the GNU community due to the divisive political vision of GNU. The GNU project did, however, create a large amount of very important software, for example the GNU compiler collection (GCC), the GNU Core Utilities (reimplementations of \texttt{cat}, \texttt{ls}, \texttt{rm}, etc.) the BASH shell\footnote{The Bourne Again Shell was an open-source replacement for the Unix Bourne Shell, an improvement to the Thompson Shell (no scripting).}, \texttt{glibc} (GNU's implementation of the C standard library), the GNOME desktop environment (which runs on the X Windows system for bitmap displays), and Emacs. These tools are used on many other operating systems, including Windows, OS X, and Linux. It is for this reason that Richard Stallman refers to Linux as a \emph{GNU} operating system with Linux kernel. It is unclear whether the pure GNU operating system will ever be finished, but its legacy endures. Another notable output of the movement was the GNU Public License (GPL), a software license supporting \emph{copyleft} practices, that is, free reuse of intellectual property, provided the same copyleft open-source license is retained, and therefore the ability to reuse that software too. Such an arrangement can be described as \emph{share-alike}, and is adopted by other open source licenses\footnote{There are more permissive licenses such as Berkeley Software Distribution (BSD) and the MIT license, arguably even more permissive than BSD. This comes only with the disclaimer that the author shall not be responsible for adverse effects caused by the free use of their software. The GNU Lesser General Public License (LGPL) was created as a compromise between MIT/BSD and GNU GPL. Note also that these licenses are free to use, the copyright is applied without the need to register the software. The license text is usually put in a project file marked \texttt{LICENSE}.}.

At approximately the same time, the systems lab at the University of California, Berkeley, were extending their licensed version of Unix, in particular, extending its TCP/IP functionalities. This came to be known as the Berkeley Software Distribution (BSD) of Unix, but it remained subject to AT\&T licensing restrictions. The Unix parts were gradually rewritten, and BSD and its variants (FreeBSD, OpenBSD, and others) came under the BSD license, a highly permissive license, allowing the full reuse of software, even in proprietary products, provided inclusion of the license text. Many of the operating systems of the present day are descended from Unix via the free alternatives of BSD. For example, BSD forms a large part of Apple's OS X operating system. BSD was the subject of a law suit by the Unix proprietors in the early 90s, during a time nicknamed the ``Unix wars'', with BSD ultimately winning the case. Unlike Linux, BSD does not use GNU, it has its own OS software including, for example, the famous vi editor. Linux is more user-friendly, however, with BSD having a command line interface (CLI) orientation. Thus, whereas GNU reverse engineered Unix, BSD evolved from it directly like the ship of Theseus. Both extended it far beyond its initial capabilities. Because of them, we now have whole families of software that are deemed ``Unix-like''.

\section{Useless Maths Facts}

\begin{itemize}
\item The length : width ratio of all standard paper sizes (A4, A5, etc.) is $\sqrt{2}:1$. It is easy to show folding a sheet of paper in half width-ways retains this same ratio. 
\item There are $10!$ seconds in 6 weeks
\item Mersenne primes are primes of the form $2^n - 1$ for some integer $n$. The Great Internet Mersenne Prime Search (GIMPS) is an ongoing collaborative experiment to find Mersenne primes. The largest Mersenne prime (and largest known prime) found to date is $2^{74207281} - 1$. Only 49 Mersenne primes have ever been found.
\item $1/e \approx 37\%$
\item Pairs of primes of difference two (e.g. 5, 7) are known as twin primes. Pairs of primes of difference four (e.g. 7, 11) are known as cousin primes. Pairs of primes of difference six (e.g. 11, 17) are known as sexy primes.
\item According to legend, the Pythagorean mathematician, Hippasus, was killed for proving the existence of irrational numbers. This conflicted with the dogmatic worldview of the other Pythagoreans.
\item For any map (for example, one of a geographic region), there is always a way to colour the map with at most four colours such that no two sections of the map sharing a border have the same colour. This comes from the four colour theorem, the first theorem in the world to be proved (partially) by computer (brute force).
\item The mirror reflection of 3.14 closely resembles the word `pie'.
\item Perfect numbers are those whose factors (including 1) sum to itself. For example, $6 = 1 + 2 + 3$. Amicable numbers are pairs of numbers each of whose factors sum to the other. For example 220 and 284.
\item 23 is the first number at which the Birthday paradox gives a greater than $50\%$ chance of a collision
\item $2^{20} \approx 10^6$
\item One googol is $10^{100}$, that is, 1 followed by 100 zeroes. A googolplex is $10^{\text{googol}} = 10^{10^{100}}$. A \emph{googolplexian} is $10^{\text{googolplex}} = 10^{10^{10^{100}}}$.
\item Graham's number is a famous named number larger by incomprehensible magnitudes. It provides an upper bound to the solution of a problem in graph theory. It is so large, it can only practically be written in terms of amalgamations of hyper-operations, exponentiation being absurdly insufficient. Using the specialised Knuth's arrow notation, Graham's number can be written as the last in a sequence 64 numbers, the first being $g_1 = 3\uparrow\uparrow\uparrow\uparrow3 = g\uparrow^4$, that is, 3 hexation 3 (already unimaginably big), the next being $g_2 = 3\uparrow^{g_1}3$, where $\uparrow^{g_1}$ is a hyperoperation whose order is $g_1$ (recall exponentiation is order 3 and hexation is order 6). It is necessary to pause and try to think what this means (you can't). Thus, the sequence continues, with $g_n = 3\uparrow^{g_{n-1}}$, all the way to $g_{64} = 3\uparrow^{g_{63}}$, and $g_{64}$ is Graham's number. There are simply no conceivable analogies to describe or relativise this number, or even the first number in the sequence. If, for example, you wrote 1 digit on every Planck length volume in the universe, once every Planck time unit, from the start to the end of the universe, you would still have completed $0.000000...\%$ (with another (slightly smaller) inconceivably large number of zeroes) of even $g_1$.
\item With 42 folds of a single sheet of paper, you would have a tower high enough to reach the moon. Given the thickness of paper is $1 \times 10^{-4}$ meters (0.1 mm) and the moon is $3.8 \times 10^{9}$ meters (380,000 km) away, the number of folds required solves the equation $10^{-4}\times 2^{\text{folds}} = 3.8\times10^9 \implies \text{folds} \approx 42$. Unfortunately, it is not possible to fold paper more than six or seven times! 103 folds would span the size of the universe.
\item $1729 = 10^3 + 9^3 = 12^3 + 1^3$ is the smallest number expressible by two positive cubes in two different ways. According to legend, this observation was first made by Ramanujan to G. H. Hardy when Hardy informed him he had travelled in taxi number 1729 to visit him. The number makes a number of other famous appearances in number theory.
\item The `number of the beast', 666, is likely to have originated from an encoding of the Hebrew name of the Roman emperor, `Nero Caesar'.
\item $0.999... = 1$. A paradox arises if one does not appreciate what `forever recurring' means. To see this, let $x = 0.999...$. Then $10x = 9.999...$ and so $10x - x = 9.999 - 0.999 = 9 \implies x = 1$.
\item There is a fairly compelling argument for replacing $\pi$ with tau, $\tau = 2\pi$. Many formulas from the normal pdf, to the Fourier transform, to the period of a sine wave use a superfluous $2\pi$. It is also more difficult to teach with half circles than full circles.
\item There is a good argument for replacing the decimal system (which is rooted in the metric system that came from revolutionary France) with the \emph{dozenal} system (base-12). Common fractions are more easily expressible in base-12, since it has more factors than 10, so measurements and arithmetic become easier to learn and work with. Though humans have 10 fingers in total, they have 3 segments on each of their 4 fingers on each hand, making base-12 equally convenient.
\item Most integers contain the digit 3. Consider the numbers before the first power of 10, namely, $0, 1, 2, 3, 4, 5, 6, 7, 8, 9$. Clearly there is only one number containing a 3 (3 itself), and we write $N(1) = 1$, and the proportion is $P(1) = 1/10$. For the first hundred numbers, the pattern is the same, with each group of tens containing one number with a 3, which are $3, 13, 23, \dots, 93$, except for the thirties, which have $30, 31, 32, 33, \dots, 39$. Thus we have $N(2) = 9 \times 1 + 10 = 19$, and $P(2) = 19/100$. For the thousands, the same is true for each hundred, except for the three hundreds, all of which contain a 3. Hence, $N(3) = 9\times(9 \times 1 + 10) + 100 = 199$, and $P(3) = 199/1000$. It is clear that in general, for the $k$th power of 10 we have $N(k) = 10^{k-1} + 9\cdot N(k-1) = 10^{k-1} + 9(10^{k-2} + 9(10^{k-3} \dots 9\cdot N(1))) = \sum_{i=1}^{k}9^{i-1}\cdot10^{k-i} = (9/10^k)\cdot\sum_{i=1}^{k}(9/10)^i$. Thus, we have a geometric series displaced by one term, and so $N(k) =  (9/10^k)\cdot((1-(9/10)^k)/(1-(9/10)) - 1)$, and $P(k) = N(k)/10^k$. As $k \to \infty$, $P(\infty) = \lim_{k \to \infty} P(k) = (1/9)\cdot(10 - 1) = 1$, hence the proportion of numbers containing at least one digit 3 goes to 1. Of course, the same can be shown for the other 9 digits, and the general finding is that almost all numbers contain every digit.
\item Mill's constant is a number that, conditioned on the (unproven) Riemann hypothesis, is the base of a double exponential function that always yields an integer part that is prime. That is, Mill's number is $A \approx 1.306$ and $\lfloor A^{3^n}\rfloor$ is always prime.
\item Fractals have very interesting self-similar properties. For example, Koch's snowflake is a geometric shape with infinite perimeter but finite area. By inspection the area is finite, as a circle of finite diameter can be drawn around it provided it is sufficiently large. The perimeter is the result of repeating infinitely many times a procedure that reduces side-length by a factor of three, but nevertheless quadruples the number of sides each time. Repeating ad infinitum therefore results in a perimeter that is $\lim_{N \to \infty}(4/3)^N = \infty$ times the length of the initial perimeter.
\item A M\"obius strip is a three dimensional shape with a single surface and one edge. It can be emulated by twisting a ribbon by a half-turn and connecting its ends. A Klein bottle is a \emph{four}-dimensional solid with a single surface and \emph{no} edges! It is formed by fusing two M\"obius strips along their edges.
\item Skewes' number, $10^{10^{10^{34}}}$ is another famous large number, and the old record holder before the discovery of Graham's number. Named after one of John Littlewood's students, Stanley Skewes, it gives an upper bound on the smallest number for which Gauss' prime-counting function, $\pi(n)$ is greater than the logarithmic integral function, $\text{li}(n)$ (another prime-counting function initially thought to always be greater than Gauss').
\item The harmonic series, $h = \sum_{n=1}^{\infty} 1/n$ is (surprisingly) divergent. Clearly, $h = 1 + 1/2 + (1/3 + 1/4) + (1/5 + 1/6 + 1/7 + 1/8) + \cdots > 1 + 1/2 + (1/4 + 1/4) + (1/8 + 1/8 + 1/8 + 1/8) \cdots = 1 + 1/2 + 1/2 + 1/2 + \cdots$, which is clearly divergent.
\end{itemize}

\begin{thebibliography}{9}

\bibitem{eulerformula}
``Euler's formula'' by Originally created by gunther using xfig, recreated in Inkscape by Wereon, italics fixed by lasindi. - Drawn by en User:Gunther, modified by others.. Licensed under CC BY-SA 3.0 via Commons - https://commons.wikimedia.org/wiki/\\File:Euler\%27s\_formula.svg\#/media/File:Euler\%27s\_formula.svg

\bibitem{inclusionexclusion}
``Inclusion-exclusion'' Licensed under CC BY-SA 3.0 via Commons - \\https://commons.wikimedia.org/wiki/File:Inclusion-exclusion.svg\#/media/File:Inclusion-exclusion.svg

\bibitem{gradientdescent}
``Gradient descent'' by Gradient\_descent.png: The original uploader was Olegalexandrov at English Wikipediaderivative work: Zerodamage - This file was derived from Gradient descent.png:. Licensed under Public Domain via Commons - \\https://commons.wikimedia.org/wiki/File:Gradient\_descent.svg\#/media/File:Gradient\_descent.svg

\end{thebibliography}

\end{document}
